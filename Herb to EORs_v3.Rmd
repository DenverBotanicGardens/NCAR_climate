---
title: "Herbarium Bias CNHP"
author: "Michelle DePrenger-Levin"
date: "December 4, 2017"
output: html_document
---
Use Jessie's cool mapping code: "Q:\Research\All_Herbaria\KHD\Projects\exploring_herbarium_with_maps" 

Rick says don't need quotes for table or column names

MySQL query

#There are only rows in the synonym_links table when the names are synonyms, if it's not in synonym_links, it's the accepted name; taxonomic_units is the main table, if it's a synonym it's linked by synonym_links
SELECT `synonym_links`.*, `taxonomic_units`.* , `taxon_authors_lkp`.*, `geographic_div`.*
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn` 
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
LEFT JOIN `geographic_div` ON `geographic_div`.`tsn` = `taxonomic_units`.`tsn`
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 AND `geographic_div`.`geographic_value` IN ('North America')



#The authorities are giving me trouble right now; can do NOT IN (0,3,9999) or whatever
SELECT `synonym_links`.*, `taxonomic_units`.* , `taxon_authors_lkp`.*
FROM `taxonomic_units` 
JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn` 
JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 AND `taxonomic_units`.`unaccept_reason` IS NULL AND `taxonomic_units`.`taxon_author_id` NOT IN (0) AND `taxonomic_units`.`name_usage` NOT IN ('invalid') AND `taxonomic_units`.`n_usage` IN ('accepted')



#the taxon_authors_lkp.taxon_author_id isn't the tsn, it's the taxonomic_units.taxon_author_id!!!
SELECT `synonym_links`.*, `taxon_authors_lkp`.*, `taxonomic_units`.* 
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn`
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 



SELECT `synonym_links`.*, `taxon_authors_lkp`.*, `taxonomic_units`.* 
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn`
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`tsn` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 



cd /p/hackathon/Simulations/

```{r}
rm(list=ls())

# install.packages("NicheMapR") based on animal needs anyway, mechanistic
library(ggplot2)
library(rgeos)
library(sp)
library(spdep)
library(rgdal)
library(maptools)
library(raster)
library(Taxonstand)
library(RCurl)
library(taxize)
library(ggmap)

library(gganimate)

library(dismo)
library(ENMeval)
library(biomod2)
library(spThin)
```
#Resample elevation (and others) to the bioclim sized grid, 30km grid is way to small, computer can't handle 
```{r}

coElev <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/CO_Mosaic_Elevation_VT/co_elev_VT_WGS84.tif")

coElev_biosize <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/co_elev_WGS84_30sec.tif")

plot(coElev)

plot(coElev_biosize, xlim = c(-108, -103), ylim=c(37,41))
```



```{r}
l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")

colocounties <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties_wgs84")
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")


plot(colocounties)

#Each polygon might be made of multiple disconnected polygons. Need to separate and label
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))

#How many names in 
length(unique(l1G1G2$GNAME)) #60
#write.table(unique(l1G1G2$GNAME), "clipboard", sep="\t", row.names=FALSE)


# Could take all the synonyms and names of l1G1G2 to make the list of names to pull from SEINet big list
namesg1g2 <- table(l1G1G2$GNAME)
length(namesg1g2[namesg1g2 > 0]) #60 species
g1g2names <- TPL(names(namesg1g2[namesg1g2 > 0]))
table(g1g2names$Taxonomic.status) #2 not in there, "Gutierrezia elegans" "Oonopsis sp. 1", 10 synonyms, 3 unresolved




#Are both synonyms and accepted in as GNAME?
notaccepted <- l1G1G2$GNAME[!is.na(match(l1G1G2$GNAME,
      g1g2names$Taxon[g1g2names$Taxonomic.status != "Accepted"]))]
unique(notaccepted)
g1g2names[g1g2names$Taxonomic.status != "Accepted",]

unique(l1G1G2$GNAME[grep("Boechera",l1G1G2$GNAME)])

#Are any accepted names also in the list?
l1G1G2$GNAME[l1G1G2$GNAME %in% 
               g1g2names$New.Genus[g1g2names$Taxonomic.status
                                   != "Accepted"]]

g1g2names[g1g2names$Taxonomic.status == "",] #Gutierrezia elegans; Arabis crandallii --> Boechera crandallii

g1g2names$Taxon
g1g2names$Taxon[g1g2names$Taxonomic.status == ""]
g1g2names$Taxon[g1g2names$Taxonomic.status == "Synonym"]

# All names to look for in SEINet, both the stated and the corrected by TPL
allnames <- c(g1g2names$Taxon, 
              paste(g1g2names$New.Genus,g1g2names$New.Species,sep=" "))
length(unique(allnames)) #71

```


Mo Ewing:    
     1) split all polygons (from CNHP) with ArcGIS 'explosion' tool    
     2) Copy the Feature_id field into the Parcel-num field and annotate each parcel: "a", "b" etc    
     3) Import G1G2 Seinet Vouchers into ArcMap      
     4) Create linking field: "Parcel_num"          
     5) Go through each parcel, linking the Eos by duplicating the Parcel_num.    
     6) determin the closest 'parcel' to each herabarium point using ArcMap 'near' tool Analyst Tools/Proximity/Near   *I assume to the nearest line of the polygon, nearest spot of the polygon, not to the center of the polygon          
     7) Add Map Method informaiton     
            i. "GPS" = indication that GPS was used, just GPS coordinates present    
            ii. "tr" = township/range info only recorded to township and range     
            iii. "trs" = township/Range info recorded to section, or indication that tr conversion tool used       
            iv. "trqs" = Township/Range info recorded to quarter-section       
            v. "Locality" = no verbatim data recorded, only written description recorded    
                     or "Terrain nav" (terrain navigator used)    
                     or "Nat. Geog. TOPO"    
                     or "Mapstedi"     
                     or "digital mape"     
                     or "geoLocate"     
                     or "GoogleEarth"      





#I must have made this one sometime, need to check again that matched all names   
I think I decided that 54 names was not all the G1/G2 so I went to get the rest one by one from SEINet instead of all Colorado specimens which then has name issues:    
coloradosps <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/occurrences.csv")
```{r}
seinet <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/seinet_colorado_with_coordinates_ALLG1G2.csv")

seinet <- seinet[seinet$genus != "",]

table(seinet$basisOfRec) #PreservedSpecimen: 1871 and HumanObservation: 4
seinet <- seinet[seinet$basisOfRecord != "HumanObservation",]
namesSEINet <- names(table(seinet$scientificName)) #93 names
SEINet_names <- TPL(namesSEINet)
table(SEINet_names$Taxonomic.status) #
#             Accepted    Synonym Unresolved 
#         2         75          9          7 
SEINet_names$scientificNameAccpt <-paste(SEINet_names$New.Genus, SEINet_names$New.Species)
#write.table(SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)], "clipboard", sep="\t", row.names=FALSE)
SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)]

seinet_TPL <- merge(seinet, SEINet_names, by.x = "scientificName", by.y = "Taxon")
head(seinet_TPL) # all the information from herbarium collections and if they're accepted names or not

seinet_TPL$family <- toupper(seinet_TPL$family)

seinet_TPL[seinet_TPL$institutionCode=="USU",]
seinet_TPL$institutionCode[seinet_TPL$institutionCode=="USU"] <- "USUUB"

```

# Check when names are good or not from herbarium specimens
```{r}
namesbyyear <- table(seinet_TPL$Taxonomic.status,seinet_TPL$year)
plot(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[2,],col="blue", type="l",
     xlab="Year",ylab="Specimens", ylim=c(0,max(namesbyyear))) #accepted
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[3,],col="green") #synonym
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[4,],col="orange") #Unresolved
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[1,],col="pink") #not in TPL
```
#Families collected over time
```{r}
familyxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$family)) #now changed toupper lowercase $family has 44 levels, inconsistent capitals and lowercase..
familyxyear$Var1 <- as.numeric(as.character(familyxyear$Var1))

familyxyear[familyxyear$Freq>0,]


library(lattice)
xyplot(Freq ~ Var1|Var2, familyxyear[familyxyear$Freq>0,],
       type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Year of Collection",
       ylab = "Number of Collections", aspect = "xy")


herbariumxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$institutionCode)) 
ggplot(herbariumxyear[herbariumxyear$Freq>0,],aes(Var1, Freq, colour=Var2 ))+
  geom_point()+
  theme_bw()


unique(seinet_TPL$geodeticDatum)
seinet_TPL[seinet_TPL$geodeticDatum == "27",]
```

Cite MODIS data: <https://lpdaac.usgs.gov/citing_our_data>   
<https://lpdaac.usgs.gov/data_access/data_pool>  - need to get data but weeky mainenance is every wednesday until noon   
Got snow data https://search.earthdata.nasa.gov/data/retrieve/5396545932 can compare to NCAR's data.   
got some veg data or getting https://search.earthdata.nasa.gov/data/retrieve/2536564325  
```{r}
library(ncdf4)
#library(maptools)
#library(extRemes)
library(fields)
library(parallel)
library(doParallel)
library(foreach)
library(abind)
library(prism)

library(raster)
library(rasterVis)
#library(measurements)
library(ENMeval)
library(RCurl)
library(dismo)

#install.packages("digest")
library(devtools)
#install_github('hadley/rvest')
library(rvest)

# add the command to bind along the 3rd dimension
abind3 <- function(...) { abind(along = 3, ...) }

```

#Can't get the layers again... skip for now. 
```{r}
NetCDF2Raster <- function(simulationmin, simulationmax, Yearmin, Yearmax, SimulationStartName,
                          SimulationEndName, SimulationEndName2, 
                          variablename,variablename2, precip = TRUE){
  sims <- sprintf('%0.3d', simulationmin:simulationmax)
  
  varyears <- lapply(sims, function(sims){
    nc_open(paste(SimulationStartName,sims,SimulationEndName,sep=""))
    })
   #Precip needs to add together CC and CL
  if(precip){
  varyears2 <- lapply(sims, function(sims){
    nc_open(paste(SimulationStartName,sims,SimulationEndName2,sep=""))
  })
  }
  
  #Sum 12 months for annual 
  varannual <- lapply(1:length(varyears), function(model){
    precc1 <- ncvar_get(varyears[[model]], variablename)
    cl <- makeCluster(6)
    registerDoParallel(cl)
    precc <- foreach(timeindex = seq(1,(Yearmax-Yearmin)*12,12),
                     .combine = "abind3") %dopar% 
      {
        apply(precc1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), 
              if(precip){
                sum
                } else {
                  mean
                })
        }
    stopCluster(cl)
    gc()
    rm(precc1)
    precc
    })
  
  if(precip){
  #Sum 12 months for annual 
  varannual2 <- lapply(1:length(varyears2), function(model){
    precc1 <- ncvar_get(varyears2[[model]], variablename2)
    cl <- makeCluster(6)
    registerDoParallel(cl)
    precc <- foreach(timeindex = seq(1,(Yearmax-Yearmin)*12,12),
                     .combine = "abind3") %dopar% 
      {
        apply(precc1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
        }
    stopCluster(cl)
    gc()
    rm(precc1)
    precc
    })
  
  varannual_sum <- lapply(1:length(varyears), function(model){
    Ycc <- varannual[[model]]
    Ycl <- varannual2[[model]]
    cl <- makeCluster(6)
    registerDoParallel(cl)
    total <- foreach(timeindex = 1:(Yearmax-Yearmin),
                     .combine = "abind3") %dopar% 
      {
        Ycc[,,timeindex]+Ycl[,,timeindex]
        }
    stopCluster(cl)
    gc()
    rm(Ycc,Ycl)
    total
  })
  }
  
  # Avreage over the models over years
  cl <- makeCluster(6)
  registerDoParallel(cl)
  avg_annual <- foreach(timeindex = 1:(Yearmax-Yearmin),
                        .packages = "abind") %dopar%
    {
      byyearmodel <- lapply(if(precip){
        varannual_sum
        } else {
          varannual
        }, function(model1){
      model1[,,timeindex]
    })
    oneyear <- abind(byyearmodel, along=3)
    abind(apply(oneyear, MARGIN = c(1,2), mean), along=3)
    }
  stopCluster(cl)
  
  save(list="avg_annual", file=paste("P:/hackathon/Simulations/",variablename,"modelavg_annual.RData",
                                     sep=""))
  dims <- dim(avg_annual [[1]])
  
  #make rasters
  var.raster <- lapply(avg_annual, function(model1){
  e <- extent(c(0,360,-90,90))
  mean.raster <- flip(raster(t(as.matrix(model1[,,1], nrow=dims[2]))),
                      direction = "y")
  extent(mean.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  mean.raster
  })
  
  years <- c(Yearmin:Yearmax)
  save(list="var.raster", file=paste("P:/hackathon/Simulations/",variablename,Yearmin,Yearmax,
                                     "modelavg.raster.RData",sep=""))
  
  #write rasters
  for(i in 1:(Yearmax-Yearmin)){
    writeRaster(rotate(var.raster[[i]]), 
                paste("P:/hackathon/Simulations/",variablename,years[i],sep=""),
                format = "GTiff",
                overwrite=TRUE)
    }
  
  #Average over 20 years
  annual20 <- lapply(seq(1,(Yearmax-Yearmin) - 19,20),
                     function(year){
                       model1 <- abind(avg_annual[year:year+19], along=3)
                       mn <- apply(model1, MARGIN = c(1,2), mean)
                       mn
                       })
  
  save(list="annual20", file=paste("P:/hackathon/Simulations/",variablename,
                                   Yearmin,Yearmax,"20_annual.RData",sep=""))
  
  var.raster <- lapply(annual20, function(model1){
    e <- extent(c(0,360,-90,90))
    mean.raster <- flip(raster(t(as.matrix(model1, nrow=dims[2]))),
                        direction="y")
    extent(mean.raster) <- e
    proj4string(mean.raster) <- CRS("+init=epsg:4326")
    mean.raster
    })
  
  save(list="var.raster", file=paste("P:/hackathon/Simulations/",
                                     variablename,Yearmin,Yearmax,
                                     "_raster.RData", sep=""))
  
  yrs <- paste(Yearmin,Yearmin+20,sep="-")
  for(i in 1:(trunc((Yearmax-Yearmin)/20)-1)){
  yrs <- c(yrs,paste(Yearmin+((20*i)+1),Yearmin+(20*(i+1)),sep=""))
  } 
  
  
  #write rasters
  for(k in 1:trunc((Yearmax-Yearmin)/20)){
    writeRaster(rotate(var.raster[[k]]),
                paste("P:/hackathon/Simulations/",variablename,yrs[k],sep=""),
                format = "GTiff",
                overwrite = TRUE)
  }
}


```

```{r}


```




#Michelle DePrenger-Levin      
In ArcMap do this: <https://blogs.esri.com/esri/arcgis/2010/09/16/nearbygroup/>
repeat distance analysis in R          
"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet        
Only G1G2 EORs (to limit range of species)
"colonames" == TPL synonomy for all coloradosps names
```{r}
#coloradosps <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/occurrences.csv")

#save(coloradosps, file= "P:/hackathon/Simulations/coloradosps.Rda")
load("P:/hackathon/Simulations/coloradosps.Rda")

howlongrecordedby <- nchar(as.character(coloradosps$recordedBy))
head(howlongrecordedby<20)
recordedby <- unique(coloradosps$recordedBy[howlongrecordedby < 100])


table(coloradosps$basisOfRecord) # PreservedSpecimen: 566067; preservedspecimen: 55; Preserved specimen: 143; Photograph: 1; Observation: 124; Native and Naturalized Flora of; HumanObservation; many just wrong column info in this

# both of these from OBI: California Polytechnic State University
coloradosps[coloradosps$basisOfRecord == "Plants of Jefferson County Open",] #68 plus 1 more with some other typo
coloradosps[coloradosps$basisOfRecord == "Plants of Colorado",] #68 plus 1 more with some other typo

#Subset by just preserved specimens  
coloradosps$basisOfRecord <- as.character(coloradosps$basisOfRecord)
coloradosps.specimens <- coloradosps[grep("specimen",coloradosps$basisOfRecord,
                                          ignore.case=TRUE),]
table(coloradosps.specimens$basisOfRecord)

coloradosps.specimens <- coloradosps.specimens[nchar(coloradosps.specimens$basisOfRecord)<50,]
table(coloradosps.specimens$basisOfRecord)

#Now only preserved specimens
coloradosps <- coloradosps.specimens
nrow(coloradosps)


coloradosps[grep("bluffs above tributary",coloradosps$basisOfRecord) ,] #there are all messed up

g1g2names$AcceptedName <- paste(g1g2names$New.Genus,g1g2names$New.Species)
#match SEINet names to either the given or accepted names from G1/G2s
length(unique(g1g2names$Taxon))
length(unique(g1g2names$AcceptedName))
length(unique(c(g1g2names$Taxon,g1g2names$AcceptedName))) #71 


rowstomatch <- lapply(c("Taxon","AcceptedName"), function(x){
   coloradosps[,"scientificName"] %in%  g1g2names[,x]
})

#######do I need this or just take matches for coloradosps? 

#The rows to keep that match either an accepted or synonym or unresolved name for a G1G2 species
#coloradosps.specimens was intermediate to get rid of none specimen records
coloradosps.g1g2 <- unique(rbind(coloradosps[rowstomatch[[1]],],coloradosps[rowstomatch[[2]],]))
nrow(coloradosps.g1g2)
hist(as.numeric(as.character(coloradosps.g1g2$decimalLatitude)))
#write.table(sort(unique(coloradosps.g1g2$scientificName)), "clipboard", sep="\t", row.names=FALSE)

setdiff(sort(unique(coloradosps.g1g2$scientificName)),sort(unique(l1G1G2$GNAME))) #species from the SEINet specimens that isn't in the names given for G1G2
setdiff(sort(unique(l1G1G2$GNAME)),sort(unique(coloradosps.g1g2$scientificName))) #the first list that's not in the second

head(g1g2names)
unique(l1G1G2$PolyID$GNAME)
plot(colocounties.UTM) #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, add=TRUE)
plot(l1G1G2[grep( "Astragalus",l1G1G2$GNAME),], border = "red", add=TRUE)
plot(l1G1G2[grep( "Penstemon",l1G1G2$GNAME),], border = "blue", add=TRUE)


```
       [1] WGS84                     WGS 84                    WGS 1984                      
       [4] WGS96                     WGS 72                    WGS-84                     
       [7] WGS 1984.                 WGS83                     WGS84 (UTM Datum: NAD83)   
      [10] WGS84  (UTM Datum: NAD83) WGS84, Google Earth       WGS 1983                   
      [13] WGS 1984,                 WGS98                     WGS85                       
      [16] WGS86                     WGS89                     WGS95                       
      [19] WGS87                     WGS88                     WGS97                       
      [22] WGS92                     WGS 83                    WGS93                       
      [25] WGS94                     WGS84/NAD83               WGS90                        
      [28] WGS 19                    WGS99                     WGS91      
      
      "The latest revision is WGS 84 (also known as WGS 1984, EPSG:4326), established in 1984 and last revised in 2004.[1] Earlier schemes included WGS 72, WGS 66, and WGS 60. WGS 84 is the reference coordinate system used by the Global Positioning System."

Scale bar attempts
```{r}
myScalebar = function(units_label, yadj=1.5) {

  # Get plot coordinates
  pc = par("usr") 

  # Position scale line between last two major x-axis tick marks
  # and 1/10th of the total y-range above the lower y-axis coordinate
  lines(c(floor(pc[2]-100),floor(pc[2])),     
        rep(pc[3] + 0.1*(pc[4] - pc[3]), 2))

  # Place the units label at the midpoint of and just below the scale line
  text(x=mean(c(floor(pc[2]-100), floor(pc[2]))), 
       y=pc[3] + 0.1*(pc[4] - pc[3]),
       label=units_label, adj=c(0.5, yadj))
}

ScaleBar <- function(reference_raster_utm, round_to_nearest_km, width_percent, y_percent_from_bottom, x_percent_from_left, y_text_percent_from_bottom, ...) {
    # Round by max to nearest... e.g. 5 km 
    mround <- function(x,base){ 
        base*round(x/base) 
    }   
    # scale bar size adjustment to avoid decimals
        scale_size <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*width_percent)/1000
        scale_size_adj <- mround(scale_size, round_to_nearest_km)
        scale_size_adj_plot <- (scale_size_adj*1000)/2
    # Horizontal percent position (x) for scale bar
        x_position <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*x_percent_from_left)+xmin(reference_raster_utm)
    # Vertical percent position y for scale bar
        y_position <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_percent_from_bottom)+ymin(reference_raster_utm)
        y_position_text <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_text_percent_from_bottom)+ymin(reference_raster_utm)
    # Draw line on plot
        library(sp)
        x_ends <- c((x_position-scale_size_adj_plot), (x_position+scale_size_adj_plot))
        y_ends <- c((y_position), (y_position))
        scale_bar_line <- SpatialLines(list(Lines(Line(cbind(x_ends, y_ends)), ID="length")))
        projection(scale_bar_line) <- projection(reference_raster_utm)
        plot(scale_bar_line, add=TRUE, ...)
        text(x_position, y_position_text, paste0(scale_size_adj, "km"))
}

```


```{r}
us <- getData('GADM', country = 'US', level = 1)
colorado <- us[us$NAME_1 == "Colorado",]
projLL <- proj4string(colorado)

colorado<- spTransform(colorado, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
i<-1
 polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),] 
 
grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
grid <- SpatialPoints(grid, proj4string = CRS(proj4string(polys)))

length(grid[polys,]) #106 points
```

#map each species, map each EOR, check distance from points to nearest EORs
```{r}
#l1G1G2 are in UTM zone 13
#l1G1G2@proj4string <- CRS("+proj=utm +zone=13 ellps=NAD83") 
# says  projargs: chr "+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84
#transform to UTM to match l1G1G2
#60 names with the listed name and the accepted name acording to TPL

proj4string(l1G1G2) <- CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
coloradosps.dist <- coloradosps.g1g2
coloradosps.dist$distNearEOR <- NA

distXsp <- lapply(1:nrow(g1g2names), function(i){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),] 
     grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
     grid <- SpatialPoints(grid, proj4string = CRS(proj4string(polys)))
     EORpnts <- data.frame(grid[polys,]@coords)
  if(nrow(EORpnts)>0){
    coordinates(EORpnts) <- ~ x1+x2
    proj4string(EORpnts) <- CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
    EORpnts <- spTransform(EORpnts, CRS("+proj=longlat +datum=WGS84"))
  } else {
    EORpnts <- NA
  }
     
  g1g2now <- coloradosps.g1g2[coloradosps.g1g2$scientificName %in%
                                c(g1g2names$Taxon[i],g1g2names$AcceptedName[i]),]
  
  areanow <- sapply(slot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),], "polygons"), 
         slot, "area")

  g1g2now$decimalLatitude <- as.numeric(as.character(g1g2now$decimalLatitude))
  g1g2now$decimalLongitude <- as.numeric(as.character(g1g2now$decimalLongitude))
  g1g2now <- g1g2now[!is.na(g1g2now$decimalLatitude),]
  g1g2now <- g1g2now[!is.na(g1g2now$decimalLongitude),]
  if(nrow(g1g2now)>0){
    #turn into spatialpointsdataframe for individual species
    coordinates(g1g2now) <- ~decimalLongitude+decimalLatitude
    proj4string(g1g2now) <- CRS("+proj=longlat +datum=WGS84") 
    g1g2now <- spTransform(g1g2now, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
    distnow <- apply(gDistance(g1g2now,
                               l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                                          g1g2names$Taxon[i]),], byid=TRUE),
                     2,min)
    
    
    out <- list(g1g2now, distnow, areanow, EORpnts)
  } else {
      out <- list(NA,NA, areanow, EORpnts)
    }
  out
  })

distXspbind <- do.call(c,mapply('[[', distXsp, 2))
 
length(distXspbind[is.na(distXspbind)]) #14 don't have any data
distXspbind <- distXspbind[!is.na(distXspbind)]
coloradosps.dist$distNearEOR[which(rownames(coloradosps.dist) %in%
                                         names(distXspbind))] <-unname(distXspbind)
  

```

```{r}

plot(distNearEOR~as.factor(scientificName), coloradosps.dist[coloradosps.dist$distNearEOR<500000,], las=2)

nrow(coloradosps.dist) #2451
colo.merged <- merge(coloradosps.dist, g1g2names, by.x = "scientificName", by.y="AcceptedName")
nrow(colo.merged) #2251
colo.merged.taxon <- merge(coloradosps.dist, g1g2names, by.x = "scientificName", by.y="Taxon")
nrow(colo.merged.taxon) #2121
## Want the ones that are only in the first and only the second and one copy of the middle ones... 
colo.all <- unique(rbind(colo.merged[,-grep("^Taxon$", names(colo.merged))], 
                         colo.merged.taxon[,-grep("^AcceptedName$",names(colo.merged.taxon))]))
nrow(colo.all) #2451
#Names that aren't in TPL
unique(colo.all$scientificName[colo.all$Family==""]) #"Gutierrezia elegans"
colo.all$Family[colo.all$Family==""] <- "Asteraceae"


plot(distNearEOR~as.factor(Family), colo.all[colo.all$distNearEOR<500000,], las=2,
     xlab="",ylab="")
require(lme4)
require(lattice)


length(table(colo.merged$Taxon)) #58 species with some data


#Write table for VisTrails field data
nrow(seinet_TPL)
seinet_TPL.cc <- seinet_TPL[seinet_TPL$decimalLongitude > -110,]
seinet_TPL.cc <- seinet_TPL.cc[seinet_TPL.cc$decimalLatitude > 35, ]

plot(seinet_TPL.cc$decimalLongitude,seinet_TPL.cc$decimalLatitude)

write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2.csv")

write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Alpine_Phenology_2/Species_SEINet/FieldData_EORG1G2.csv")

herbspecimennumxsp<-table(seinet_TPL.cc$scientificNameAccpt)


whoswho <- data.frame(table(seinet_TPL.cc$scientificNameAccpt,seinet_TPL.cc$scientificName))
whoswho[whoswho$Freq>0,]  



```


#Animate maps over each species with points and polygons
```{r}
all.g1g2now <- do.call(c,mapply('[[', distXsp, 1)) #all the transformed to UTM species points
all.g1g2now <- all.g1g2now[!is.na(all.g1g2now)]

plot(colocounties.UTM, border="grey80")
plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[1],g1g2names$Taxon[1]),],
     border = "red", add=TRUE, lwd=4)
points(distXsp[[1]][[1]], pch=16, cex=0.25)
ScaleBar(raster(colocounties.UTM), 100, .20, .10, .75, .07, lwd=2)
```

Get the coordiates from each saved to use as csvs in VisTrails. 
```{r}
distXsp[[1]][[2]] #The distances from each herbarium point to the nearest EOR
distXsp[[1]][[3]] #The areas of each EOR polygon for that species

#distXsp[[i]][[1]] are the herbarium points, SpatialPointsDataFrame
speciesNoHerb <- do.call(c,mapply('[[', distXsp, 1))
want <- which(do.call(c,lapply(speciesNoHerb, function(x) !is.na(x))))
wantEOR <- which(do.call(c,lapply(mapply('[[', distXsp, 4),
                         function(x) !is.na(x))))

bboxes <- lapply(want, function(i){
  speciesNoHerb[[i]]@bbox
  })

#If there are multiple names/synonyms
unique(speciesNoHerb[[1]]@data$scientificName)

#All the points
speciesNoHerb[[1]]@coords

#points in the EORs
EORsxsp <- mapply('[[', distXsp, 4)

for(i in wantEOR){
  write.csv((EORsxsp[[i]]@coords),
            paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2",g1g2names$AcceptedName[i],".csv", sep=""))
}


# make them match the number of points that the herbarium specimens have
for(i in wantEOR){
  if(nrow(EORsxsp[[i]]@coords)>herbspecimennumxsp[i]){
  write.csv(EORsxsp[[i]]@coords[sample(1:nrow(EORsxsp[[i]]@coords), herbspecimennumxsp[i]),],
            paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORsamplesizematch",g1g2names$AcceptedName[i],".csv", sep=""))
  }
}

# spatial thinning instead of just matching the points that the herbarium specimens have. Spatially thin the herbarium specimens as well. 

```






From VisTrails
```{r}
DrEx_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

DrEx <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata/Maxent_1/maxent_bin_map.tif")

#DrExEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1
#EOR <- DrEx_EOR

plot(DrEx_EOR)
plot(DrEx)
#plot(DrExEORmatchss)

AsMi_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

AsMi <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus/Maxent_1/maxent_bin_map.tif")

#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1

AsMiEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


plot(AsMi_EOR,main="EOR")
plot(colocounties, add=TRUE)
plot(AsMi, main="Herb")
plot(colocounties, add=TRUE)
plot(AsMiEORmatchss, main="EOR sample size match herb")
plot(colocounties, add=TRUE)

```
http://www.earthskysea.org/!ecology/sdmShortCourseKState2012/grinnellExercise/exercise-tuning-maxent-using-beta-and-aic.html  
https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html    
Use AICc with Maxent to find best model  
```{r}



```







```{r}
str(distXspbind) #1091
hist(distXspbind/1000, main = "Distance (km) from points to nearest EOR",
     xlab="km", breaks=1000, xlim=c(1,500))

vals <- density(distXspbind)
pStar <- vals$x[which(vals$y==max(vals$y))]

plot(density(distXspbind), xlim=c(0,50000))
abline(v=pStar, col="red")
abline(h=max(vals$y)-qlnorm(0.95))

length(distXspbind[distXspbind>100]) #816

length(distXspbind[distXspbind<=100]) #275

median(distXspbind)

reps <- 50000
ssize <- 500
rand.diff <- unlist(lapply(1:reps, function(x) median(sample(distXspbind,ssize))))
ctrl95CI <- sd(rand.diff)/sqrt(ssize)*qt(0.975,df=ssize) 
ctrl95CI.l <- sd(rand.diff)/sqrt(ssize)*qt(0.025,df=ssize)


hist(rand.diff)
abline(v=mean(rand.diff)+ctrl95CI, col="red")
abline(v=mean(rand.diff)+ctrl95CI.l, col="red")

```


```{r}
for(i in want){
  plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),],
       border = rgb(1,0,0,0.5),  main=g1g2names$AcceptedName[i], lwd=3,
       xlim=c(min(distXsp[[i]][[1]]@coords[,1]),max(distXsp[[i]][[1]]@coords[,1])),
       ylim=c(min(distXsp[[i]][[1]]@coords[,2]),max(distXsp[[i]][[1]]@coords[,2])))
  points(distXsp[[i]][[1]], pch=16, cex=0.5, col=rgb(0,0,0,0.75))
  ScaleBar(raster(l1G1G2[l1G1G2$GNAME %in%
                           c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),]), 
           1, .25, .20, .15, .17, lwd=2)
}

```

4/6/2018 - Danielle Gualtieri working on Colton Gaines protocol for mapping and adding uncertainty protocol (similar to Mo's work)  6/11/2018 - long gone, hasn't been back in a while, not doing the work any more


#Mechanistic models based on matrix models growth rates and fecundity correlative, use some model selection? for best prediction by species  
```{r}
load("Q:/Research/Stats & Software/COMPADRE_PlantMAtrixDatabase/COMPADRE_v.4.0.1.RData")



```



# How do the maping techniques change by time, herbarium, or collector (amount of informaiton given)?   
#Some data cleaning of colo.all, use colo.test first
```{r}
i <- sapply(colo.all, is.factor)
colo.all[i]<-lapply(colo.all[i], as.character)

colo.all$year <- as.numeric(colo.all$year)
colo.all$decimalLatitude <- as.numeric(colo.all$decimalLatitude)
colo.all$decimalLongitude <- as.numeric(colo.all$decimalLongitude)
colo.all$minimumElevationInMeters <- as.numeric(colo.all$minimumElevationInMeters)

colo.all[colo.all$recordNumber == 9088,] #year is 1999, not recorded at Fort Lewis
colo.all[colo.all$recordNumber == 1596,] #another repeat, should be 1997 not 1007
## Ok to get rid of duplicates that resulted in wrong years
colo.all <- colo.all[colo.all$year > 1800,]

colo.all$coordinateUncertaintyInMeters <-
  as.numeric(colo.all$coordinateUncertaintyInMeters)
#How many specimens give an uncertainty in meters (square meters?)
length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$coordinateUncertaintyInMeters) & !is.na(colo.all$decimalLatitude)])/
  length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$decimalLatitude)])

colo.all[is.na(colo.all$coordinateUncertaintyInMeters),]

colo.all[colo.all$distNearEOR>50000 & !is.na(colo.all$distNearEOR),c(56,57,83,61,63,66,67)] #611551meters is width of colorado about

#Can't have missing data
colo.all.ll <- colo.all[!is.na(colo.all$decimalLatitude)&
                       !is.na(colo.all$decimalLongitude),]
coordinates(colo.all.ll) <-~decimalLongitude+decimalLatitude
proj4string(colo.all.ll) <- CRS("+proj=longlat +datum=WGS84") 
colo.all.ll <- spTransform(colo.all.ll, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

all.county <- over(colo.all.ll, colocounties.UTM[,"COUNTY"])
colo.all.ll$InCounty <- all.county[,1]


table(colo.all.ll$county[colo.all.ll$county != colo.all.ll$InCounty])
colo.all.ll@data[colo.all.ll$county != colo.all.ll$InCounty &
              !is.na(colo.all.ll$county),c("scientificName","institutionCode","county","InCounty","georeferencedBy")]

#Which county are the points really in
colocounties.UTM.df <- as.data.frame(colocounties.UTM)
colocounties.UTM.df$id <- as.numeric(rownames(colocounties.UTM.df))

plot(colocounties.UTM, border="grey80")
plot(colo.all.ll, add=TRUE, pch=16, cex=0.5)

#Make a Levenshtein distance matric between names (like that paper!)
countymatch<- adist(colo.all.ll@data$county[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    colo.all.ll@data$InCounty[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    partial=TRUE,ignore.case = TRUE)
#Single linkage method
#hcfar <- hclust(as.dist(countymatch), method='single')


#errors so try clean up county column
colo.all.ll$countyclean <- gsub("Co.*","",colo.all.ll$county)
#trim trailing white space
colo.all.ll$countyclean <- sub("\\s+$", "", colo.all.ll$countyclean)
colo.all.ll$countyclean <- toupper(colo.all.ll$countyclean)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]

nrow(colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),])/
       nrow(colo.all.ll)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]

```


Assuming CNHP is nearly correct 
```{r}
colo.all[colo.all$recordNumber%in%
           colo.all.ll@data$recordNumber[colo.all.ll$distNearEOR>50000] & !is.na(colo.all$decimalLatitude),
         c("scientificName","decimalLatitude","decimalLongitude","county","verbatimCoordinates",
           "habitat","locality","recordNumber")] ##Location would be correct but decimalLongitude goes to -160.4; The easting has an extra zero: -105.7131, 38.1374; 3972 m


#row 99: 21864 entered 27.47444 lat, probably meant 37.47
#Row 753 21864 again with an extra trailing 0 for the easting got to -160 for long
#row 1382 5733 zone 12 and NAD27, SEINet couldnt' figure that out maybe?
#row 1393 5733 seems correct! **keep this one, lose the rest 5733; the other two are duplicates!
#row 1394 5733 This time missing a digit in the easting so it put it in Nevada

colo.all.ll@data[rownames(colo.all.ll@data) == 99,]
```


#Random forest - classification and regression trees   
Either a formula, or (1) data.frame with predictor variables, (2) vector with response. categorical will classification, interval will do regression. 
```{r}

library(randomForest)

bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)


plot(bioclim.colorado$bio1_12)
plot(colocounties, add=TRUE)

#When loop, use "i in want" or lapply(want, function(i)) which are non-NA in the list for distXsp[[i]][[1]]@coords

#need back to lat long
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[1]]][[1]], CRS("+proj=longlat +datum=WGS84"))
}

##COME BACK TO THIS AND SEE WHAT NEEDS TO BE Done to do random forest
presvals <- extract(bioclim.colorado, )


library(dismo)

kfoldnum <- 5

#Make a training and testing set, will want to iterative over all sets, keeping one out each time
#lapply(1:kfoldnum, function(x){})

group <- kfold(distXsp[[1]][[1]]@coords, kfoldnum)
pres_train <- distXsp[[1]][[1]]@coords[group != 1,]
pres_test <- distXsp[[1]][[1]]@coords[group == 1,]

ext <- extent(-109, -102, 36, 41)
backg <- randomPoints(bioclim.colorado, n=300, ext=ext, extf=1.25)
colnames(backg) <- dimnames(pres_test)[[2]]
groupbk <- kfold(backg, kfoldnum)
backg_train <- backg[groupbk != 1,]
backg_test <- backg[groupbk == 1,]






```


```{r}
ggplot(colo.all.ll@data[colo.all.ll$distNearEOR<500000,], aes(year, distNearEOR/1000))+
  geom_point()+
  theme_bw()+
#  geom_hline(yintercept=(57936.4/1000), col="red")+ #if off by 36 miles
#  geom_hline(yintercept=(666), col="blue")+ #if off by 1 zone, 111km wide at equator
  stat_smooth(method="lm")+
  ylab("Distance (km) to nearest EOR")
```

Total area of EOR polygons per spcies for the the 46 species with mapped herbarium records 
```{r}
speciesNoEOR <- lapply(want, function(x){
  out <- sum(distXsp[[x]][[3]])
  out
})

areaxsp <- unlist(lapply(want, function(x) sum(distXsp[[x]][[3]])))
areaxsp <- data.frame(Area = areaxsp, AccName = g1g2names$AcceptedName[want])

mediandistxsp <- unlist(lapply(want, function(x) median(distXsp[[x]][[2]])))

area_distxsp <- rep(areaxsp$Area, times= unlist(lapply(want,function(x) length(distXsp[[x]][[2]]))))
sp_distxsp <- rep(areaxsp$AccName, times= unlist(lapply(want,function(x) length(distXsp[[x]][[2]]))))

toplot <- data.frame(area_distxsp, sp_distxsp, distXspbind)

#One is factor, but they are the same! 46 species
areaxsp$AccName <- as.character(areaxsp$AccName) 

colo.all.ll$AcceptedName <- paste(colo.all.ll$New.Genus,colo.all.ll$New.Species)
colo.all.ll@data <- merge(colo.all.ll@data,areaxsp, 
                          by.x ="AcceptedName", by.y="AccName")

toplot[which(toplot$area_distxsp==max(toplot$area_distxsp)),]


ggplot(toplot[toplot$distXspbind<200000,], aes(area_distxsp, distXspbind/1000))+
  geom_point()+
  theme_bw()+
  stat_smooth(method="lm")+
  ylab("Distance (km) to nearest EOR")+
  xlab("Total EOR area")

```

```{r}
require(AICcmodavg)



f1 <- lm(distNearEOR ~ New.Genus + year + institutionCode + Family + Area, data= colo.all.ll@data)
f2 <- lm(distNearEOR ~ year + institutionCode + Family + Area, data= colo.all.ll)
f3 <- lm(distNearEOR ~ year + Area, data= colo.all.ll)
f4 <- lm(distNearEOR ~ year, data= colo.all.ll)
f5 <- lm(distNearEOR ~ Area, data= colo.all.ll)
f6 <- lm(distNearEOR ~ 1, data= colo.all.ll)

(lmresults <- aictab(list(f1,f2,f3,f4,f5,f6),
       modnames=as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6),formula)))))

evidence(aictab(cand.set = list(f1,f2,f3,f4,f5,f6),
                modnames = as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6),formula)))))

sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})

(er1 <- exp(0.5*lmresults$Delta_AICc[2]))
(er2 <- exp(0.5*lmresults$Delta_AICc[3]))
```


#how many are in the county they say they should be in? Check Jessie's code for help
```{r}
ggplot(colo.all[colo.all$distNearEOR<50000,], aes(as.numeric(year), distNearEOR))+
  geom_point()+
  stat_smooth(se=FALSE)+
  theme_bw()+
  xlab("Year")+
  ylab("Distance from nearest EOR")+
#  labs(color='Georeference Method')+
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```



#By herbarium 
Maybe regional collections are much better?   
FLD = Fort Lewis College    
DES = Dessert Arizona   
MESA = Colorado Mesa University   
MISSA = Miss State Univ   
NMC = New Mexico State University    
SJNM = San Juan College, New Mexico   
UCR = University of California Riverside    
USUUB = Utah STate University Uintah Basin (probably is USU)   
UVSC = Utah Valley University	U.S.A. Utah. Orem.      




```{r}

local <- c("COLO","CS","DBG","FLD","MESA","RM","RMBL")
Adjacent <- c("ASC","ASU","BRY","DES","NMC","RM","SJNM","UNM","USUUB","UVSC")
far <- c("CM","F","MISSA","MO","NY","RENO","UCR")

colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% local] <- "Local"
colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% Adjacent] <- "Near"
colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% far] <- "Far"

colo.all.ll@data$InstNear <- factor(colo.all.ll@data$InstNear, levels = c("Far","Near","Local"))

ggplot(colo.all.ll@data[colo.all.ll$distNearEOR<500000,], 
       aes(InstNear, distNearEOR/1000))+
  geom_jitter()+
  stat_summary(fun.y=mean, geom="point", shape=18,
                 size=3, color="red")+ 
  geom_violin(trim = FALSE)+
  theme_bw()+
  ylab("Distance (km) to nearest EOR")+
  xlab("Total EOR area")

```

```{r}
colo.all.ll@data$verbatimCoordinates[colo.all.ll$distNearEOR>100]

#How many gave 1/4 section
colo.all.ll@data[grep("1/4", colo.all.ll$verbatimCoordinates),]
mean(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])
sd(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])


mean(colo.all.ll@data$distNearEOR[grep("", colo.all.ll$verbatimCoordinates)])
sd(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])
```



#Convext hull or tiny circles around eors vs herbarium vs. both together, how much change?    

The Weber example of an out of state person mapping a collection to Gray's Peak from Chicago Lake that should be Mount Evans.     
- <https://conps.org/wp-content/uploads/2015/05/Mount_Evans_Summit_Lake_4-24-2011.pdf>    

"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet      
"coloG1G2" == species matching names in L1 EORs but without any synonym corrections     
"seinet" == Mo worked on this, made distances from nearest EOR and looked at posted informaiton to figure out what might have been used to pick point on map        
      
ITISlist is synonmy pulled from ITIS into SQL    
TPL is pulling from synonym from the plant list   "colo"   
      "seinetMo"is synonymy with TPL for Mo's list of 53 species (that are G1/G2s)   
      "colonames" is synonym with TPL for all colorado specimens from SEINet   


#ENMeval vignette
<https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html>   

#Spatial thinning
<https://cran.r-project.org/web/packages/spThin/vignettes/spThin_vignette.html>     
Thin to create dataset in which all occurences are at least _thin.par_ distance apart. helps reduce effect of uneven, biased, species occurence collections on spatial model outcomes   
Got through 19 of them, go from 20 to length(wantEOR)
```{r}
# The EOR grid points from distXsp is the forth item. Wants lat long, they are. 
#for(i in 1:length(wantEOR)){
#for(i in 20:length(wantEOR)){
#for(i in 21:length(wantEOR)){

EORgrid.thin <- list()
for(i in c(1:18,21:47,49:length(wantEOR))){
  EORsp <- data.frame(distXsp[[wantEOR[i]]][[4]]@coords,
                      species=g1g2names$AcceptedName[wantEOR[i]])
    EORgrid.thin[[i]] <- thin(loc.data = EORsp,
                         lat.col = "x2", long.col = "x1",
                         spec.col="species",
                         thin.par = 1, reps=10,
                         locs.thinned.list.return = TRUE,
                         write.files = TRUE, max.files=5,
                         out.dir = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Thinned/",paste(out.base="EORsp_1km",i,"_",g1g2names$AcceptedName[wantEOR[i]],sep=""), write.log.file = TRUE,
                         log.file = "EORgridthinned_log.txt")
}



```


### Background points  
Can either do bounding box (plus some buffer) or circles at some diameter
```{r}

#Circles around EOR points. 


#Circles around herbarium points
circle <- list()
for(i in 1:length(wantEOR)){
  circle[[i]] <- circles((EORsxsp[[wantEOR[i]]]@coords), d=50000, lonlat=TRUE)
}

plot(colocounties)
plot(circle[[2]], add=TRUE)
points(EORsxsp[[2]]@coords, pch=16)
plot(circle[[3]], add=TRUE, border="red")
points(EORsxsp[[3]]@coords, pch=16)
plot(circle[[4]], add=TRUE, border="blue")
points(EORsxsp[[4]]@coords, pch=16)
plot(circle[[5]], add=TRUE, border="orange")
points(EORsxsp[[5]]@coords, pch=16)

# Need stack of environmental layers  
#Raster stack of just the bioclim variables
class(bioclim.colorado)

# Get backgrounds for each species
envs.backg <- list()
for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- crop(bioclim.colorado, circle[[i]]@polygons)
}

for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- mask(envs.backg[[i]], circle[[i]]@polygons)
}

#Plot presence points
plot(envs.backg[[15]][[1]])
points(EORsxsp[[15]], pch=16)


# Background points from set area, select number of background points equal to the number of presence points
bg <- list()
for(i in 1:length(wantEOR)){
  bg[[i]] <- as.data.frame(randomPoints(envs.backg[[i]][[1]], n=nrow(EORsxsp[[wantEOR[i]]]@coords)))
}

#Plot presence and background points
for(i in 1:length(wantEOR)){
  plot(envs.backg[[i]][[1]])
  points(EORsxsp[[i]], pch=16, cex=0.5)
  points(bg[[i]], pch=21, cex=0.5, col="red")
}

#Try some thinning first, for some that's lots of points that are 

```



