---
title: "R Notebook"
output: html_notebook
---

Ignore blocks with 'morethan12' and skip to atleast12

I think the outputs that don't have _EOR or _herb are the ones that used Herb to train and EOR to test. can see what the AUC patterns were for those. 
Making new herb train and EOR test ones with VisTrails and same size EOR numbers
```{r}
rm(list=ls())

library(ggplot2)
library(rgeos)
library(raster)
library(foreach)
library(parallel)
library(doParallel)

library(maptools)
library(dismo)
library(Rmisc)
library(rgdal)
library(ENMeval)
library(Taxonstand)
library(grid) # for rectGrob

load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/habspEORandNoandWith.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/coloradosps.g1g2_168.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2namesall68.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid68.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid1_68.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/nichevaluesbySDM.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/toplot.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/nicheoverlaps.Rdata")

l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")
#Each polygon might be made of multiple disconnected polygons. Need to separate and label
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))

l1G1G2and <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2","G1G2"),])
l1G1G2and$PolyID <- do.call(rbind, lapply(split(l1G1G2and,l1G1G2and$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))
namesg1g2 <- table(as.character(l1G1G2$GNAME))
length(namesg1g2) #60 species
namesg1g2and <- table(as.character(l1G1G2and$GNAME))
length(namesg1g2and) # 68
#What? are some in twice? 
namesg1g2and <- c(namesg1g2and[match(names(namesg1g2),names(namesg1g2and))],
                  namesg1g2and[setdiff(names(namesg1g2and),names(namesg1g2))])

# g1g2names <- TPL(names(namesg1g2))
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2names.Rda")
g1g2names61_68 <- TPL(names(namesg1g2and[61:68]))


notnull <- which(!unlist(lapply(distXsp_noEORgrid, is.null)))
notnull61668 <- which(!unlist(lapply(distXsp_noEORgrid68, is.null)))

distXspall <- do.call(rbind, Map(cbind, distXsp_noEORgrid[notnull], 
                     SpNum = notnull))
distXsp6168 <- do.call(rbind, Map(cbind, distXsp_noEORgrid68[notnull61668],
                                  SpNum = notnull61668+60))

atleast12 <- c()
for(l in notnull){
  atleast12[match(l,notnull)] <- if(nrow(distXsp_noEORgrid[[l]])>11){ 
    l } else {
      NA
    }
}
atleast12 <- atleast12[!is.na(atleast12)] 
rm(l)

#Each polygon might be made of multiple disconnected polygons. Need to separate and label
length(namesg1g2) #60 species
length(namesg1g2and) # 68
g1g2namesall68 <- TPL(names(namesg1g2and))
g1g2namesall68$AcceptedName <- paste(g1g2namesall68$New.Genus,g1g2namesall68$New.Species)


# atleast1268 <- c()
# for(l in notnull61668){
#   atleast1268[match(l,notnull61668)] <- if(nrow(distXsp_noEORgrid68[[l]])>11){ 
#     l } else {
#       NA
#     }
# }
# atleast1268 <- atleast1268[!is.na(atleast1268)] 
# rm(l)

# February 26, 2019 added _bioclim where I've resampled to the bioclim raster cell size (less than 1km)
# coplus50int.tif
coElev_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/ElevationResampled_bioclim.tif")
# aspect50int.tif
coAspect_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/AspectResampled_bioclim.tif")

# COplus_ruggedInt50.tif
coRugged_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/RuggedResampled_bioclim.tif")

# plot(coRugged_res)
# bio1 <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/BioClim/20190122_WorldClimv2/wc2.0_bio_30s_01.tif")
# bio12 <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/BioClim/20190122_WorldClimv2/wc2.0_bio_30s_12.tif")
# 
# bio1_res <- resample(bio1, coRugged_res, method="bilinear",
#                      filename = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif", overwrite=TRUE)
# 
# bio12_res <- resample(bio12, coRugged_res, method="bilinear",
                       # filename = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif", overwrite=TRUE)


bio1_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif")
bio12_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif")
gc()


rasterstack <- stack(coElev_res,coAspect_res,coRugged_res,bio1_res,bio12_res)
rm(bio1_res,bio12_res,coAspect_res,coElev_res,coRugged_res)
gc()

stitchtogether <- function(whichones, pathstart, patternmatch, rasternames){
  lapply(whichones, function(i){
  gc()
  lapply(1:10, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste(patternmatch,i,"kfold",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiffSp",i,rasternames,k,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
})
}

thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}

#Need to averge and SD each of the 10

library(cluster)
library(snow)

forhistofaverage <- function(whichones = atleast12, pathstart, patternmatch){
  stacktoaverage <- lapply(whichones, function(i){
    rasterstoavg <- list.files(path = pathstart, 
                               pattern = paste("ProbTiffSp",
                                               i,
                                               patternmatch,sep=""), 
                               full.names=TRUE)
    ras <- stack(lapply(rasterstoavg, function(y){
      raster(y)
    }))
    beginCluster(10)
    ras.mean <- clusterR(ras, calc, args=list(mean, na.rm=T))
    writeRaster(ras.mean, paste(pathstart,
                                "AvgTiffSp",i,patternmatch,g1g2namesall68$AcceptedName[i],
                                ".tif", sep=""),overwrite=TRUE)
    gc()
    endCluster()
    ras.mean
  })
  stacktoaverage
}

forhistofsd <- function(whichones = atleast12, pathstart, patternmatch){
  stacktoaverage <- lapply(whichones, function(i){
    rasterstoavg <- list.files(path = pathstart, 
                               pattern = paste("ProbTiffSp",
                                               i,
                                               patternmatch,sep=""), 
                               full.names=TRUE)
    ras <- stack(lapply(rasterstoavg, function(y){
      raster(y)
    }))
    beginCluster(10)
    ras.mean <- clusterR(ras, calc, args=list(sd, na.rm=T))
    writeRaster(ras.mean, paste(pathstart, "SDTiffSp",i,patternmatch,
                              g1g2namesall68$AcceptedName[i],
                              ".tif", sep=""),overwrite=TRUE)
    gc()
    endCluster()
    ras.mean
  })
  stacktoaverage
}

# 
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid.Rda")
# distXspall <- do.call(rbind, distXsp_noEORgrid)
#The over 400 km is clearly wrong; so are any above 300km is out of the state (if point is in the middle) so need to cutoff below 300,000

whichnull <- lapply(1:68, function(i){
  gc()
  g1g2now <- coloradosps.g1g2_168[coloradosps.g1g2_168$scientificName %in%
                                c(g1g2namesall68$Taxon[i],g1g2namesall68$AcceptedName[i])&
                                !is.na(coloradosps.g1g2_168$decimalLatitude),]
  if(nrow(g1g2now)>0){
    g1g2namesall68$AcceptedName[i]
  }
})
  
notnulls68 <- which(!unlist(lapply(whichnull, is.null)))
which(unlist(lapply(whichnull, is.null))) # 39

distXspall52 <- do.call(rbind, Map(cbind, SpNum = notnulls68[notnull1_68],
                                   distXsp_noEORgrid1_68[notnull1_68]))
```

```{r}
# Now just putting in a data.frame that needs to be made into a spatialpointsdataframe, from herbarium so x y are longitude and latitude
maxentrun <- function(whichones, spatialpointsdataframe_herb, numberofReps = 10, 
                      maxentarguments = FALSE, predictorvariables = rasterstack, 
                      pathstart, filenames, kfoldnum = 4, 
                      error = FALSE, distdistribution = NULL){
        for(x in whichones){
            pointsspdf <- SpatialPointsDataFrame(coords = spatialpointsdataframe_herb[[x]][,c("decimalLongitude","decimalLatitude")],
                                                 data = spatialpointsdataframe_herb[[x]],
                                                 proj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
          if(error == TRUE){
               # for each point I will draw a circle of size (drawn from the distribution of error seen distXspall$Dist) and then pick a random point along the circle.
              errorpointsout <- do.call(rbind,lapply(1:nrow(pointsspdf), function(r){
                errordist <- sample(distdistribution, 1)
                if(errordist>0){
                  erroraround <- gBuffer(pointsspdf[r,], width=errordist)
                  newpoint <- erroraround@polygons[[1]]@Polygons[[1]]@coords
                  out <- newpoint[sample(1:nrow(newpoint),1),]
                } else {
                  out <- pointsspdf@coords[r,]
                  }
                out
                }))
                
                df <- SpatialPointsDataFrame(coords = errorpointsout,
                                             data = pointsspdf@data,
                                             proj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
                circlesout <- circles(df, d = 5000) #Should be 5km around
                polygns <- polygons(circlesout)
                bgpnts <- spsample(polygns, 300, "stratified") # one single random location in each 'cell' 
                
                convertxy <- spTransform(df, CRS("+proj=longlat +datum=WGS84"))
                proj4string(bgpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
                bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
          } else {
                convertxy <- spTransform(pointsspdf, CRS("+proj=longlat +datum=WGS84"))
                circlesout <- circles(pointsspdf, d = 5000)
                polygns <- polygons(circlesout)
                bgpnts <- spsample(polygns, 300, "stratified")
                proj4string(bgpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
                bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
            
          }
                
          for(rep in 1:numberofReps){
                convertxy$kfold <- kfold(convertxy, k=kfoldnum) # to have 75:25%
                if(maxentarguments == TRUE){
                  xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords,
                           args=c("noautofeature","noproduct","nothreshold"))
                } else {
                  xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords)
                }
                
                write.csv(data.frame(convertxy@coords,convertxy@data),
                          paste(pathstart,"presenceHerb",filenames,"Sp",x,"kfold",rep,".csv", sep=""))
                save(xm, file= paste(pathstart,"maxentHerb",filenames,"Sp",x,"kfold",rep,".Rda", sep=""))        
        
                gc()
                #register parallel computing backend
                cl = parallel::makeCluster(ncores)
                doParallel::registerDoParallel(cl,ncores)
                #compute indices for data splitting
                rows = 1:nrow(predictorvariables)
                split = sort(rows%%ncores)+1
                outname = paste(pathstart,"PredictHerb",filenames,"Sp", x,"kfold",rep, sep="")
                #perform the prediction on subsets of the predictor dataset
                foreach(i=unique(split), .combine=c)%dopar%{
                  rows_sub = rows[split==i]
                  sub = raster::crop(predictorvariables,raster::extent(predictorvariables, min(rows_sub), max(rows_sub), 
                                                                1, ncol(predictorvariables)))
                  raster::predict(sub, xm, filename=paste(outname, i, sep="_"), overwrite=TRUE)
                }
        
                e <- evaluate(convertxy[convertxy$kfold==1,], bgpnts, xm, predictorvariables)
                save(e, file= paste(pathstart,"evaluateHerb",filenames,"Sp",x,"kfold",rep,".Rda", sep=""))
        
                rm(xm)
                gc()
                stopCluster(cl)
          }
                e
        }
}

```


```{r}
# x<-atleast12[3]
for(x in atleast12168[21]){
            pointsspdf <- SpatialPointsDataFrame(coords = spatialpointsdataframe_herb[[x]][,c("decimalLongitude","decimalLatitude")],
                                                 data = spatialpointsdataframe_herb[[x]],
                                                 proj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
          # if(error == TRUE){
               # for each point I will draw a circle of size (drawn from the distribution of error seen distXspall$Dist) and then pick a random point along the circle.
              errorpointsout <- do.call(rbind,lapply(1:nrow(pointsspdf), function(r){
                errordist <- sample(distdistribution, 1)
                if(errordist>0){
                  erroraround <- gBuffer(pointsspdf[r,], width=errordist)
                  newpoint <- erroraround@polygons[[1]]@Polygons[[1]]@coords
                  out <- newpoint[sample(1:nrow(newpoint),1),]
                } else {
                  out <- pointsspdf@coords[r,]
                  }
                out
                }
                ))
                
                df <- SpatialPointsDataFrame(coords = errorpointsout,
                                             data = pointsspdf@data,
                                             proj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
                circlesout <- circles(df, d = 5000) #Should be 5km around
                polygns <- polygons(circlesout)
                bgpnts <- spsample(polygns, 300, "stratified") # one single random location in each 'cell' 
                
                convertxy <- spTransform(df, CRS("+proj=longlat +datum=WGS84"))
                proj4string(bgpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
                bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
          # } else {
          #       convertxy <- spTransform(pointsspdf, CRS("+proj=longlat +datum=WGS84"))
          #       # bgpnts <- background_points[[match(x,notnull)]]
          #       circlesout <- circles(pointsspdf, d = 5000)
          #       polygns <- polygons(circlesout)
          #       bgpnts <- spsample(polygns, 300, "stratified")
          #       proj4string(bgpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
          #       bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
          #   
          # }
                
          # for(rep in 1:4){
                convertxy$kfold <- kfold(convertxy, k=4) # to have 75:25%
                # if(maxentarguments == TRUE){
                #   xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords,
                #            args=c("noautofeature","noproduct","nothreshold"))
                # } else {
                  # xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords)
                # }                
          plot(colocounties) # , xlim=c(139982.4, 773240.8), ylim=c(408691.88, 4761362.70))
          points(bgpnts, pch=16, col="purple", cex=0.5)
          points(spTransform(pointsspdf, CRS("+init=epsg:4326")), pch=21, col="orange")

          # }
}

```

# More functions 

```{r}

habitatSpecificity <- function(whichones, pathstart, whichmaxentpoints, replicates){
  habsp <- do.call(rbind,lapply(whichones, function(x){
  out <- do.call(rbind,lapply(1:replicates, function(rep){
    load(paste(pathstart,whichmaxentpoints,"Sp", x,"kfold",rep, ".Rda",sep=""))
    outinner <- data.frame(SpeciesNum = x, kfold = rep, xm@presence)
    outinner
    }))
  out
  }))
  habsp
  
  prPCA <- princomp(habsp[,-c(1:2)])
  allsphabsp <- data.frame(habsp[,1:2],prPCA$scores)

  # for all species
  habspecificity <- do.call(rbind,lapply(atleast12, function(x){
    # get and average across the replicates
    ellipseout <- do.call(rbind,lapply(1:10, function(rep){
      p <- ggplot(allsphabsp[allsphabsp$SpeciesNum==x&allsphabsp$kfold==rep,], 
                  aes(Comp.1,Comp.2))+
        geom_point()+
        stat_ellipse(segments=201) #default is to draw 51 line segments to make the ellipse
      # get ellipse coordinates
      pb <- ggplot_build(p)
      table(pb$data[[2]]$group)
      
      el <- pb$data[[2]][c("x","y")]
      
      # Center of ellipse
      ctr <- MASS::cov.trob(el)$center
      
      # Distance to center from each point on ellipse
      dist2center <- sqrt(rowSums((t(t(el)-ctr))^2))
      
      # Area of ellipse from semi-major and semi-minor axes which are largest and smallest of dist to center
      habitatspecificity <- pi*min(dist2center)*max(dist2center)
      hsout <- data.frame(SpeciesNum = x, kfold = rep, habspec = habitatspecificity)
      hsout
      }))
    ellipseout
    }))
  habspecificity
}
```




# Maxent    
Get the years of the herbarium specimens and the years of the EORs mapped from distXsp_noEORgrid from "Herb to EORs_v4"   
Run this for some lists
```{r}
# All are fine, should work. 
EORpolys <- lapply(1:68, function(i){
  polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                        g1g2namesall68$Taxon[i]),]
  polys
})

notnull1_68 <- which(!unlist(lapply(distXsp_noEORgrid1_68, is.null)))
length(notnull1_68) # 52 have data

whichnull <- lapply(1:68, function(i){
  gc()
  g1g2now <- coloradosps.g1g2_168[coloradosps.g1g2_168$scientificName %in%
                                c(g1g2namesall68$Taxon[i],g1g2namesall68$AcceptedName[i])&
                                !is.na(coloradosps.g1g2_168$decimalLatitude),]
  if(nrow(g1g2now)>0){
    g1g2namesall68$AcceptedName[i]
  }
})
  
notnulls68 <- which(!unlist(lapply(whichnull, is.null)))
which(unlist(lapply(whichnull, is.null))) # 39
EORpolys[[39]] # Oonopsis sp. 1, hasn't been published yet!


# See Herb to EORs_v4 for notnulls68
length(notnull1_68) # 52
length(distXsp_noEORgrid1_68) # 68
length(notnulls68) # 67
length(notnulls68[notnull1_68]) # 52

which(!unlist(lapply(distXsp_noEORgrid1_68, is.null)))

distXspall <- do.call(rbind, Map(cbind, SpNum = which(!unlist(lapply(distXsp_noEORgrid1_68, is.null))), 
                                   distXsp_noEORgrid1_68[which(!unlist(lapply(distXsp_noEORgrid1_68, is.null)))]))
unique(distXspall[,c("SpNum","Species")])

# Oops, seem to have lost this somewhere some git push...
distdistribution <- distXspall52$Dist[distXspall52$Dist<300000]

df_dist <- distXspall

df_dist$EORYear <- as.numeric(substring(df_dist$EORdate,1,4))
hist(df_dist$EORYear)

plot(log(df_dist$Area+1),log(df_dist$Dist+1),
     xlab=expression(paste("Species Range log(meters" ^2,")" )),
     ylab=expression(paste("log (meters)")))
abline(lm(log(Dist+1)~log(Area+1), data=df_dist))
length(unique(df_dist$SpNum))
max(df_dist$Dist) # 411023.6 meters

atleast12168 <- c()
for(l in notnull1_68){  # notnull1_68
  atleast12168[match(l,notnull1_68)] <- if(nrow(distXsp_noEORgrid1_68[[l]])>11){   # match(l,notnull1_68)
    l } else {
      NA
    }
}
atleast12168 <- atleast12168[!is.na(atleast12168)] 


reallistofTiffs <- atleast12168[!atleast12168 %in% c(29,40)]
```

# NEED TO RUN THIS
Table 1 - species in niche models, put a column indiciating which are SDM
```{r}

out <- Rmisc::summarySE(df_dist, "Dist", "SpNum")

tab1 <- do.call(rbind, lapply(split(df_dist, df_dist$SpNum), function(x){
  data.frame(SpNum1 = unique(x$SpNum), Species = x$Species[1], DistAvg = summarySE(x, "Dist"), 
             Area = unique(x$Area), minYearDiff = min(x$Yeardiff), maxYearDiff = max(x$Yeardiff),
             SpNum= unique(x$SpNum), YearDiffAvg = summarySE(x, "Yeardiff"),
             MinEORyear = min(x$EORYear), MaxEORyear = max(x$EORYear), 
             MinYear = min(x$year), MaxYear = max(x$year), AvgLat = summarySE(x, "decimalLatitude"))
})) 



tab1$SDM[tab1$SpNum %in% atleast12168] <- "x"

write.csv(tab1, "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/table1.csv")

library(Taxonstand)
TPLtbl1 <- TPL(tab1$Species)
write.csv(TPLtbl1, "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/TPLtbl1.csv")

```



```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure2.jpg",
     width=225, height=105,units='mm', res=300)

layout(matrix(c(1,2), 1,2))

#default margins par(mar=c(5.1, 4.1, 4.1, 2.1))

par(mar=c(5.1, 4.1, 1.1, 0.1))
plot(log(df_dist$Area+1),
     log(df_dist$Dist+1),
     xlab=expression(paste("Species Range log(km" ^2,")")),
     ylab=expression(paste("log(km)")),
     pch=16, cex= .25) 
abline(lm((log(Dist+1))~ (log(Area+1)), data=df_dist))  # 
mtext("a)", side=3, line=0, adj=0)

par(mar=c(5.1, 2.1, 1.1, 0.1))
plot(jitter(df_dist$year),log(df_dist$Dist+1),
     xlab="Year",
     ylab="",
     pch=16, cex= .25)
abline(lm(log(Dist+1)~year, data=df_dist))
mtext("b)", side=3, line=0, adj=0)


dev.off()

```


The EOR AUC results from VisTrails, samesize runs from ForVisTrails.Rmd where I either thinned to match number of Herbarium records or where I bootstrapped and jittered to get matching points. Used a standalone Dan Warren thinning process to get equally spaced out but covering the most area (of information) for points.


Make maxent commands
for output prediction map, use ENMeval   
Using KDE at 95% in MAxent VisTrails which mostly makes it so that it appears there's less effort in areas with fewer points but might not be the case, just might be smaller popualtions so maybe just circles with a certain radius would be better. Maybe add some points along roads for circles. Could put points along roads and then first pick points along roads within a certain distance from presence points and then 'circles' around all those points ?  
follow https://rspatial.org/sdm/SDM.pdf while VisTrails isnt' working

Background points, in a 5km circle around presence points, 300 stratified across the circles. Not quite the KDE but if I combined other collections from same trip I think that would be a correct estimator, circles represent the posibility that folks hiked 3 miles on their collection trip, seems likely enough. 

```{r}

yearsofPolys <- lapply(atleast12168, function(i){
    polys <- l1G1G2and [l1G1G2and $GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                        g1g2namesall68$Taxon[i]),] 
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    # years <- format(as.Date(polys$LASTOBS, format="%Y-%m-%d"),"%Y")
    years <- as.numeric(substring(polys$LASTOBS, 1,4))
    if(length(years)>0){
    yearlater1980 <- c()
    for(l in 1:length(years)){
      yearlater1980[l] <- if(years[l]>1980){ 
        l } else {
          NA
        }
    }
    yearlater1980 <- yearlater1980[!is.na(yearlater1980)]  
    table(years[which(years>1980)])
    }
})
for(d in 1:length(yearsofPolys)) print(yearsofPolys[[d]])



```



## Herbarium without more error, 10 replicates   
```{r}

jar <- paste(system.file(package="dismo"), "/java/maxent.jar", sep='')
ncores=10
    
atleast12168

# notes in NicheSpaialIssuesManuscript_PCA from Merow et al. 2013 and ENMTools
# Set parameters for maxent niche models
argsformax <- make.args(RMvalues = 0, # regularization multiplier, seq(0.5, 4,0.5) to test multiple values?
                  fc = "LQH")  # features class combinations; fc = c("L", "LQ", "H", "LQH", "LQHP", "LQHPT"),)


```

Still missing 29 with error, 40 no error
```{r}
# from 1-60
notnull
atleast12

# from testing 1-68
atleast12168

pathstart6266 <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/" 
spatialpointsdataframe_herb <- distXsp_noEORgrid1_68
hist(distdistribution) # ca line 300

#29 worked 40 can't get all the way through!
maxentrun(whichones = c(62,66), numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "noerror", 
          pathstart = pathstart6266)

# c(3,62,66)
# c(29,40,62,66)
maxentrun(whichones = 29, numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "WITHerror", error = TRUE,
          pathstart = pathstart6266, distdistribution = distdistribution)

# Errors for 66, maybe error put them too far away?? No, for some reason sometimes x,y sometimes lat,lon
maxentrun(whichones = 66, numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "WITHerror", error = TRUE,
          pathstart = pathstart6266, distdistribution = distdistribution)

# (whichones, pathstart, patternmatch, rasternames) where rasternames is what the written raster will be called beyond the Sp # from whichones




stitchtogether(c(29,40,62,66), pathstart6266, "PredictHerbnoerrorSp", "Herbnoerrorkfold")
stitchtogether(3, pathstart6266, "PredictHerbnoerrorSp", "Herbnoerrorkfold")
stitchtogether(c(3,62,66), pathstart6266, "PredictHerbWITHerrorSp", "HerbWITHerrorkfold")
stitchtogether(66, pathstart6266, "PredictHerbWITHerrorSp", "HerbWITHerrorkfold")
stitchtogether(3, pathstart6266, "PredictHerbWITHerrorSp", "HerbWITHerrorkfold")

# Accounted for the missing on in the function, can do the ones I really want 
# 31? atleast12168[c(17:18,21:24,28)]  
atleast12168[28]
# 40 won't work
forhistofaverage(whichones = c(62,66), pathstart = pathstart6266, patternmatch = "Herbnoerrorkfold")
forhistofsd(whichones = c(29,40,62,66), pathstart = pathstart6266, patternmatch = "Herbnoerrorkfold")


forhistofaverage(whichones = c(40,62,66), pathstart = pathstart6266, patternmatch = "HerbWITHerrorkfold")
forhistofsd(whichones = c(29,40,62,66), pathstart = pathstart6266, patternmatch = "HerbWITHerrorkfold")

maxentrun(whichones = atleast12168[c(17:18,21:24,28)], numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "noerror", 
          pathstart = pathstart6266)

# atleast12168[c(17:18,21:24,28)]
maxentrun(whichones = c(29,40,62,66), numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "WITHerror", error = TRUE,
          pathstart = pathstart6266, distdistribution = distdistribution)

```

```{r}
### come back here, run these!!! 
maxentrun(whichones = atleast12168[c(22:24,28)] , numberofReps = 10, 
          # atleast12168[c(17:18,21:24,28)]; 40
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "noerror", 
          pathstart = pathstart6266)

# 43 had errors
# atleast12168[c(17:18,21:24,28)]
maxentrun(whichones =  atleast12168[c(24,28)] , numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "WITHerror", error = TRUE,
          pathstart = pathstart6266, distdistribution = distdistribution[distdistribution<150000])

maxentrun(whichones =  40, numberofReps = 10, 
          spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = FALSE,  filenames = "WITHerror", error = TRUE,
          pathstart = pathstart6266, distdistribution = distdistribution[distdistribution<150000])


hist(distdistribution[distdistribution<200000])

stitchtogether(atleast12168[c(17:18,21:24,28)], pathstart6266, "PredictHerbnoerrorSp", "Herbnoerrorkfold")
stitchtogether(atleast12168[c(17:18,21:24,28)], pathstart6266, "PredictHerbWITHerrorSp", "HerbWITHerrorkfold")

forhistofaverage(whichones = atleast12168[c(17:18,21:24,28)], pathstart = pathstart6266, patternmatch = "Herbnoerrorkfold")
forhistofsd(whichones = atleast12168[c(17:18,21:24,28)], pathstart = pathstart6266, patternmatch = "Herbnoerrorkfold")

# atleast12168[c(17:18,21:24,28)]
forhistofaverage(whichones = 3, pathstart = pathstart6266, patternmatch = "HerbWITHerrorkfold")
forhistofsd(whichones = 3, pathstart = pathstart6266, patternmatch = "HerbWITHerrorkfold")
###############
```




Stitch together the maps 
```{r}
pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/")

  lapply(atleast12, function(i){
  gc()
  lapply(1:10, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste("PredictHerbnoerrorSp",i,"kfold",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/ProbTiffSp",i,"Herbnoerrorkfold",k,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
})
```

MAxent and stitch together rasters as functions
```{r}

for(x in atleast12){
        convertxy <- spTransform(distXsp_v3[[x]][[1]], CRS("+proj=longlat +datum=WGS84"))
        bgpnts <- bgHerb[[match(x,notnull)]]
        proj4string(bgpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
        bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
        
  for(rep in 1:10){
        convertxy$kfold <- kfold(convertxy, k=4) # to have 75:25%
        xm <- maxent(x = rasterstack,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords) # , 
                    # args=c("noautofeature","noproduct","nothreshold"))
        write.csv(data.frame(convertxy@coords,convertxy@data),
                  paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/presenceHerbnoerrorSp",x,"kfold",rep,".csv", sep=""))
        save(xm, file= paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/maxentHerbnoerrorSp",x,"kfold",rep,".Rda", sep=""))        

        gc()
        #register parallel computing backend
        cl = parallel::makeCluster(ncores)
        doParallel::registerDoParallel(cl,ncores)
        #compute indices for data splitting
        rows = 1:nrow(rasterstack)
        split = sort(rows%%ncores)+1
        outname = paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/PredictHerbnoerrorSp", x,"kfold",rep, sep="")
        #perform the prediction on subsets of the predictor dataset
        foreach(i=unique(split), .combine=c)%dopar%{
          rows_sub = rows[split==i]
          sub = raster::crop(rasterstack,raster::extent(rasterstack, min(rows_sub), max(rows_sub), 
                                                        1, ncol(rasterstack)))
          raster::predict(sub, xm, filename=paste(outname, i, sep="_"), overwrite=TRUE)
        }

        e <- evaluate(convertxy[convertxy$kfold==1,], bgpnts, xm, rasterstack)
        save(e, file= paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/evaluateHerbnoerrorSp",x,"kfold",rep,".Rda", sep=""))

        rm(xm)
        gc()
        stopCluster(cl)
  }
        e
}
################################# for loop works but lapply and function do not work


pathstart <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
stitchtogether(atleast12[1:21], pathstart,"PredictHerbWITHerrorSp", "HerbWITHerrorkfold")
```


# Use the function to run maxent with or without arguments   
Don't know how to make the function act like the for loop where lapply didnt' send info out of function and similarly the for loop within the function doesn't send the information out
```{r}

pathstart2 <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_witharguments/" 
spatialpointsdataframe_herb <- distXsp_v3

maxentrun(whichones = atleast12[1], numberofReps = 2, spatialpointsdataframe_herb = spatialpointsdataframe_herb,
          maxentarguments = TRUE,  filenames = "noFooerror", 
          pathstart = pathstart2)

```



Take the overlap of all EOR polygons and the rasters to get the histograms of probability   
    1. Do I average over all the replicates? Then what do I do with the SD? That could also be extracted to see how much uncertainty falls within the spatial extent of the known populations. - that is another measure of precision.    
    2. I should get the EORs from Utah, Wyoming, Arizona?, and New Mexico   
    
Accuracy
```{r}
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2names.Rda")
library(maptools)
l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))
plot(l1G1G2)



pathstartwithnoerror <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
patternmatchno <- "Herbnoerrorkfold"
patternmatchwith <- "HerbWITHerrorkfold"

noerroravg <- forhistofaverage(pathstartwithnoerror, patternmatchno)
witherroravg <- forhistofaverage(pathstartwithnoerror, patternmatchwith)

# Extract takes the raster value only when the center of the raster is covered by the polygon. I've got lots of polygons that do not cover the center of the larger raster cells. Use weights for partialy covered cells and argument small for getting values for small polygons. 
# get map, make lists of 10 each for each replicate
accuracyhists <- function(pathstart, probmapnames, whichones, reps = 1:10){
  lapply(whichones, function(i){
  lrhist <- lapply(reps, function(k){
    r <- raster(paste(pathstart,"ProbTiffSp",i,probmapnames,k,".tif", sep=""))
    
    polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                    g1g2names$Taxon[i]),] 
    # Only want ones after 1980
    years <- as.numeric(substring(polys$LASTOBS,1,4))
    yearlater1980 <- c()
    for(l in 1:length(years)){
      yearlater1980[l] <- if(years[l]>1980){
                                      l } else {
                                        NA
                                      }
    }
    yearlater1980 <- yearlater1980[!is.na(yearlater1980)]
    polys <- polys[yearlater1980,]
    
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
    f <- spTransform(polys, CRS("+proj=longlat +datum=WGS84"))   
    fr <- extract(r, f, small = TRUE, weights = TRUE)
    # Want the value [,1] for each polygon but use the weight [,2] to get the pro-rated cells 
    out <- do.call(rbind,fr)
    out
    })
  lrhist
  })
}


accuracyhistsOneperSp <- function(pathstart, whichones = reallistofTiffs, SDorAVG = "Avg", WithorNo = "WITH"){
  lapply(whichones, function(i){
    resultpath <- list.files(path = pathstart, 
                         pattern = paste(SDorAVG,"TiffSp",i,"Herb",WithorNo, sep=""), 
                         full.names=TRUE)
    
    r <- raster(resultpath)
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                              g1g2namesall68$Taxon[i]),] 

    # Only want ones after 1980
    years <- as.numeric(substring(polys$LASTOBS,1,4))
    yearlater1980 <- c()
    for(l in 1:length(years)){
      yearlater1980[l] <- if(years[l]>1980){
                                      l } else {
                                        NA
                                      }
    }
    yearlater1980 <- yearlater1980[!is.na(yearlater1980)]
    polys <- polys[yearlater1980,]
    
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
    f <- spTransform(polys, CRS("+proj=longlat +datum=WGS84"))   
    fr <- extract(r, f, small = TRUE, weights = TRUE)
    # Want the value [,1] for each polygon but use the weight [,2] to get the pro-rated cells 
    out <- do.call(rbind,fr)
    out
    })
}

pathstart1 <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
# probmapnames1 <- "Herbnoerrorkfold"
# whichones1 <- atleast12[atleast12!=8]

noerrorhists <- accuracyhists(pathstart1, probmapnames1, whichones1)
hist(noerrorhists[[1]][[2]])

witherrorhists <- accuracyhists("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/", "HerbWITHerrorkfold", whichones1)

reallistofTiffs <- atleast12168[!atleast12168 %in% c(29,40)]

#Averaged and SD of probability maps second function
noerrorhistsofavg <- accuracyhistsOneperSp(whichones = reallistofTiffs, pathstart1, WithorNo = "no")
witherrorhistsofavg <- accuracyhistsOneperSp(whichones = reallistofTiffs,pathstart1)


# kstest are they different or the same,
# then 
# lognormal distribution the median and geometric mean are identical. 
# Oh, the log(mn) =  log(m) + log(n) where log(m/n) =  log(m) - log(n) and log(m^2) = n * log(m)

# Check sum of logs first.. 
# plots same or different 
# AICc has wrong parameters

save(noerrorhistsofavg, file = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/noerrorhistsofavg.Rdata")

save(witherrorhistsofavg, file = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/witherrorhistsofavg.Rdata")
```


# RUN THIS to get which are significantly different for accuracy
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/noerrorhistsofavg.Rdata")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/witherrorhistsofavg.Rdata")

ks.outs <- lapply(1:length(noerrorhistsofavg), function(i){
  ks.test(log(data.frame(noerrorhistsofavg[[i]])$value)+
             log(data.frame(noerrorhistsofavg[[i]])$weight),
          log(data.frame(witherrorhistsofavg[[i]])$value)+
             log(data.frame(witherrorhistsofavg[[i]])$weight))
})

length(notnulls68) # 67

sapply(ks.outs, '[',2)

whichsigndiff <- sapply(1:length(noerrorhistsofavg), function(s){
  if(ks.outs[[s]]$p.value < 0.05){
    1
  } else {
    NA
  }
})



whichsigndiff
which(!unlist(lapply(whichsigndiff, is.na)))
length(which(!unlist(lapply(whichsigndiff, is.na)))) # 19
length(ks.outs) # 33

# install.packages("latticeExtra")
# library(latticeExtra)
# Default margins
par(mar=c(5.1, 4.1, 4.1, 2.1))

# https://chemicalstatistician.wordpress.com/2013/06/24/exploratory-data-analysis-conceptual-foundations-of-empirical-cumulative-distribution-functions/  
```


<https://stats.stackexchange.com/questions/324841/fitting-regression-spline> 
Using Stan to fit regression splines  
<https://mc-stan.org/users/documentation/case-studies/splines_in_stan.html> 
```{r}
library(splines)
functions {
  vector f(vector t, real gamma_1, real gamma_3); // implement this spline
}
data {
  int<lower=1> subjects;
  int<lower=1> periods;
  vector[periods] y[subjects];
  vector[periods] t[subjects];
}
parameters {
  vector[3] gamma_raw[subjects]; // using non-centered parameterization
  cholesky_factor_corr[3] C:
  vector<lower=0>[3] lambda; // standard deviations for gamma
  real<lower=0> sigma;
}
model {
  matrix[3,3] L = diag_pre_multiply(C, lambda);
  for (s in 1:subjects) {
    vector[3] gamma = L * gamma_raw[s];
    target += normal_lpdf(gamma_raw[s] | 0, 1); // implies gamma is MVN
    target += normal_lpdf(y[s] | gamma[2] + f(t[s], gamma[1], gamma[3]),
                                 sigma);
  }
  target += lkj_corr_cholesky(C | 1); // C * C' is uniform a priori
  // priors on lambda and sigma
}
```



```{r}

jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure4_accuracy.jpg",
     width=500, height=300,units='mm', res=300)

layout(matrix(c(1:20),nrow=4, byrow = TRUE))

par(mar=c(5.1,4.1,4.1,0.1))
for(i in which(!unlist(lapply(whichsigndiff, is.na)))){
  noerr <- data.frame(noerrorhistsofavg[[i]])
  # noerr <- noerr[sample(1:nrow(noerr),1000),]
  witherr <- data.frame(witherrorhistsofavg[[i]])
  # witherr <- witherr[sample(1:nrow(witherr),1000)]
  as_is <- ecdf(log(noerr$value)+log(noerr$weight))
  additional <- ecdf(log(witherr$value)+log(witherr$weight))
  # as_is <- ecdf(log(data.frame(noerrorhistsofavg[[i]])$value)+
  #            log(data.frame(noerrorhistsofavg[[i]])$weight))
  # additional  <- ecdf(log(data.frame(witherrorhistsofavg[[i]])$value)+
  #            log(data.frame(witherrorhistsofavg[[i]])$weight))
  plot(as_is, col = "blue", main=paste(g1g2namesall68$AcceptedName[reallistofTiffs[i]]), cex.main=2)  
  lines(additional, col = rgb(0,1,0,0.5))
}


dev.off()
```

# try ks plot kernel smoothing, smothing optimized for display
# plot ecdf, the sum of logs 
?ecdf

Compare those with and not with different accuracy curves for distances and sample size and area  
```{r}

# The SpNum that were compared, that had SDM for both as-is and with error
reallistofTiffs

# of reallistofTiffs the ones that are significantly different 
whichsigndiff


df_dist
df_dist$AccuracyDifferent <- "No"
df_dist$AccuracyDifferent[df_dist$SpNum %in% reallistofTiffs[which(!unlist(lapply(whichsigndiff, is.na)))]] <- "Yes"
# 
# N_after1980<-sapply(split(df_dist, df_dist$SpNum), function(x){
#   nrow(x[x$year>1980,])
# })
# 
# SampleSize <- data.frame(SpNum = as.numeric(names(N_after1980)), N_SDM = N_after1980)
# 
# df_dist[df_dist$SpNum >60,] # What da, I have ErBr as 63 and Lygodesmia grandiflora as 67 not 62 and 66. Ugg, but otherwise correct
# 
# df_dist2 <- merge(df_dist)

summary(glm(as.factor(AccuracyDifferent) ~ (Area+Dist+year)^2, data=df_dist[df_dist$year>1980,], family="binomial"))
```


# But wait! Distance isn't calculated as the distance for the error points, only for the initial points, take out all the distance errors. 
```{r}
  f1 <- glm(as.factor(AccuracyDifferent) ~ year + Area, data= df_dist[df_dist$year>1980,], family="binomial")
  f8 <- glm(as.factor(AccuracyDifferent) ~ Yeardiff + Area, data= df_dist[df_dist$year>1980,], family="binomial")
  # f2 <- glm(as.factor(AccuracyDifferent) ~ (Area*Dist*year), data= df_dist[df_dist$year>1980,], family="binomial")
  # f9 <- glm(as.factor(AccuracyDifferent) ~ (Area*Dist*Yeardiff), data= df_dist[df_dist$year>1980,], family="binomial")
  f3 <- glm(as.factor(AccuracyDifferent) ~ year, data= df_dist[df_dist$year>1980,], family="binomial")
  f4 <- glm(as.factor(AccuracyDifferent) ~ Area, data= df_dist[df_dist$year>1980,], family="binomial")
  f5 <- glm(as.factor(AccuracyDifferent) ~ 1, data= df_dist[df_dist$year>1980,], family="binomial")
  f6 <- glm(as.factor(AccuracyDifferent) ~ year*Area, data= df_dist[df_dist$year>1980,], family="binomial")
# f7 <- glm(as.factor(AccuracyDifferent) ~ Area*Dist, data= df_dist[df_dist$year>1980,], family="binomial")
  f10 <- glm(as.factor(AccuracyDifferent) ~ Yeardiff*Area, data= df_dist[df_dist$year>1980,], family="binomial")
  f11 <- glm(as.factor(AccuracyDifferent) ~ Yeardiff, data= df_dist[df_dist$year>1980,], family="binomial")


(lmresults <- aictab(list(f1,f3,f4,f5,f6,f8,f10,f11),# list(f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11),
       modnames=as.character(unlist(lapply(list(f1,f3,f4,f5,f6,f8,f10,f11),formula)))))
evidence(aictab(cand.set = list(f1,f3,f4,f5,f6,f8,f10,f11),
                modnames = as.character(unlist(lapply(list(f1,f3,f4,f5,f6,f8,f10,f11),formula)))))
sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})
(er1 <- exp(0.5*lmresults$Delta_AICc[2]))
(er2 <- exp(0.5*lmresults$Delta_AICc[3]))
(er3 <- exp(0.5*lmresults$Delta_AICc[4]))
(er4 <- exp(0.5*lmresults$Delta_AICc[5]))

summary(f6)
hist(df_dist$year[df_dist$year>1980])  
length(unique(distXspall52$SpNum)) # 52 species with herbarium
min(distXspall52$year) # 1871
max(distXspall52$year)
hist(distXspall52$year)

hist(df_dist$Area[df_dist$year>1980])  
min(distXspall52$Area)/10000 # 1871
max(distXspall52$Area)/(10000)
hist(distXspall52$Area/10000)
mean(unique(distXspall52$Area))/10000
sd(unique(distXspall52$Area))/10000

median(unique(distXspall52$Area))/10000

distXspall52[distXspall52$Area == max(distXspall52$Area),]
distXspall52[distXspall52$Area == min(distXspall52$Area),]



ggplot(df_dist[df_dist$year>1980,], aes(y = Area, x = AccuracyDifferent))+
  geom_boxplot()


ggplot(df_dist[df_dist$year>1980,], aes(y = year, x = AccuracyDifferent))+
  geom_boxplot()

ggplot(df_dist[df_dist$year>1980,], aes(year, Area, colour=AccuracyDifferent))+
  stat_smooth(method="lm")


ggplot(df_dist[df_dist$year>1980,], aes(year, Area, colour=AccuracyDifferent))+
  stat_smooth(method="lm")+
  geom_point()


ggplot(df_dist[df_dist$year>1980,], aes(Area, year, colour=AccuracyDifferent))+
  stat_smooth(method="lm")+
  geom_point()

ggplot(df_dist[df_dist$year>1980 & df_dist$Area<60000000,], aes(year, Area, colour=AccuracyDifferent))+
  stat_smooth(method="lm")+
  geom_point()


ggplot(df_dist[df_dist$year>1980& df_dist$Area<60000000,], aes(Area, year, colour=AccuracyDifferent))+
  stat_smooth(method="lm")+
  geom_point()


# How many different collectors? Split words, select most common repeats. 
collectors <- as.character(df_dist$recordedBy)
colcommalastname <- sapply(collectors, function(x) strsplit(gsub('[[:digit:]]+', '', x), 
                                                           split = "\\s+"))

collectorssplit <- sapply(collectors, function(x){
  first <- gsub(";",",",x)
  first <- gsub("-","",first)
  first <- gsub(" and ",",",first)
  first <- gsub("\\|",",",first)
  first <- gsub("&",",",first)
  first <- gsub("Adn",",",first)
  strsplit(gsub('[[:digit:]]+', '', first),split = ",")
})

firstcollector <- sapply(1:length(collectorssplit),function(i) head(collectorssplit[[i]],1))
lastname <- sapply(1:length(firstcollector), function(n) tail(strsplit(firstcollector[n],split=" ")[[1]],1))

allofem <- do.call(c, collectorssplit)
# allofem <- gsub('[[:digit:]]+', '', allofem)
sort(table(allofem))
sort(table(firstcollector))
sort(table(lastname))
length(sort(table(lastname)))

unique(df_dist$recordedBy)

# writeClipboard(allofem)

length(table(as.character(df_dist$institutionCode))) # 28 but USU and USUUB are the same so 27
table(as.character(df_dist$institutionCode))
```



Compare distributions that are not the same using performR from Silas Tittes
```{r}
#install
devtools::install_github("silastittes/performr", local = FALSE)

#load other libraries used below
library(performr)
library(tidyverse)
library(ggridges)
theme_set(theme_minimal())

```

# run my own Bayesian analysis to look at the differeces in distirbutions by comparing moments (where \mu = (alpha/(alpha+beta))) and variance \sigma^2 = (alpha*beta)/((alpha+beta)^2(alpha+beta+1))     
<https://www4.stat.ncsu.edu/~reich/st590/code/BetaBinomJAGS>   
```{r}
#install.packages("R2jags")
library(R2jags)
library(rjags)
library(coda)

dat <- data.frame(x = log(data.frame(noerrorhistsofavg[[1]])$value), 
                  y = log(data.frame(noerrorhistsofavg[[1]])$value)+
                    log(data.frame(noerrorhistsofavg[[1]])$weight) )



```


RUN THIS for whichsigndiffprec <-     
# Precision
```{r}
# pathstart <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
precisionplotsno1 <- lapply(reallistofTiffs, function(i){
                        gc()
                        resultpath <- list.files(path = pathstart, 
                                                 pattern = paste("AvgTiffSp",i,"Herbnoerror",sep=""), 
                                                 full.names=TRUE)
                        rastout <- raster(resultpath)
                        out <- sample(values(rastout),1000000)
                        out
                        })

precisionplotswith1 <- lapply(reallistofTiffs, function(i){
                        gc()
                        resultpath <- list.files(path = pathstart, 
                                                 pattern = paste("AvgTiffSp",i,"HerbWITHerror",sep=""), 
                                                 full.names=TRUE)
                        rastout <- raster(resultpath)
                        out <- sample(values(rastout),1000000)
                        out
                        })

precisionplotsno <- precisionplotsno1
precisionplotswith <- precisionplotswith1

# takes a long time, what if I subsample the points, does it go faster without losing information?
plot(ecdf(precisionplots[[1]]))

# it is 43,740,000 long
plot(ecdf(sample(precisionplots[[1]], 100000)))

# data:  precisionplots[[1]] and sample(precisionplots[[1]], 1e+06)
# D = 0.00072771, p-value = 0.6785
# alternative hypothesis: two-sided
ks.test(precisionplots[[1]],
        sample(precisionplots[[1]], 1000000))

# data:  precisionplots[[1]] and sample(precisionplots[[1]], 1e+05)
# D = 0.0021953, p-value = 0.7221
# alternative hypothesis: two-sided
ks.test(precisionplots[[1]],
        sample(precisionplots[[1]], 100000))

for(i in 1:length(precisionplotsno)){
  plot(ecdf(sample(precisionplotsno[[i]],10000)),
       main = g1g2namesall68$AcceptedName[reallistofTiffs[i]], xlim = c(0,1), col="red")
  plot(ecdf(sample(precisionplotswith[[i]],10000)), add = TRUE, col="blue")
}

ks.precision <- lapply(1:length(precisionplotsno), function(i){
  ks.test(precisionplotsno[[i]],
          precisionplotswith[[i]])
})

whichsigndiffprec <- sapply(1:length(ks.precision), function(s){
  if(ks.precision[[s]]$p.value < 0.05){
    1
  } else {
    NA
  }
})
whichsigndiffprec
length(which(!unlist(lapply(whichsigndiffprec, is.na))))

# evaluate when the percent of values is greater than 0 and what the suitability values are once it approaches 100% of the cells
Fn <- ecdf(precisionplotsno[[1]])

# How about t-tests for the percentiles? 
ttestpercentiles <- lapply(1:length(ks.precision), function(p){
  Fn.no <- ecdf(precisionplotsno[[p]])
  Fn.with <- ecdf(precisionplotswith[[p]])
  out <- t.test(Fn.no(seq(0,1,by=0.01)),
                Fn.with(seq(0,1,by=0.01)))
  out
})
ttestpercentiles[[1]]
ttestpercentiles[[12]]

# No, want to know at what suitability score does it start to climb and what score does it asymptote
smoothedcurves <- do.call(rbind,lapply(1:length(ks.precision), function(p){
  Fn.no <- ecdf(precisionplotsno[[p]])
  Fn.with <- ecdf(precisionplotswith[[p]])
  # Find where change in Y changes
  inflno <- c(FALSE, diff(diff(Fn.no(seq(0,1,by=0.001)))>0)!=0)
  inflwith <- c(FALSE, diff(diff(Fn.with(seq(0,1,by=0.001)))>0)!=0)
  inno <-seq(0,1,by=0.001)[which(TRUE == inflno)]
  inwith <- seq(0,1,by=0.001)[which(TRUE == inflwith)]
  data.frame(noerrorrange = max(inno)-min(inno), witherrorrange = max(inwith)-min(inwith))
}))
smoothedcurves

t.test(smoothedcurves$noerrorrange,smoothedcurves$witherrorrange)

Fn(seq(0,1,by=0.1)) # Gives the percentiles for x: Fn(x); Once you hit 0.6, have 99% of raster cells covered

# When 
which(Fn(seq(0,1,by=0.01))[Fn(seq(0,1,by=0.01))>0.01] == min(Fn(seq(0,1,by=0.01))[Fn(seq(0,1,by=0.01))>0.1]))

for(i in 1:length(precisionplotsno)){
  plot(density(precisionplotsno[[i]], col="green"),xlim=c(0,1),main = g1g2namesall68$AcceptedName[reallistofTiffs[i]])
  lines(density(precisionplotswith[[i]], col="red"))
}
plot(density(precisionplotsno[[1]]))

PDF <- hist(precisionplotsno[[1]], freq=FALSE)
PDF$counts
PDF$breaks
```

```{r}
# sort by smallest to largest area
sort(unique(df_dist$Area[df_dist$SpNum %in% reallistofTiffs[which(!is.na(whichsigndiffprec))]]))/(1000^2)
areasort <- unique(df_dist[df_dist$SpNum %in% reallistofTiffs[which(!is.na(whichsigndiffprec))], 
                                                              c("SpNum","Area")])
areasort <- areasort[order(areasort$Area),"SpNum"]
areasort <- c(1:33)[match(reallistofTiffs ,areasort)]


jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure5_precision.jpg",
     width=300, height=900,units='mm', res=300)

layout(matrix(c(1:33),nrow=11, byrow = TRUE))

par(mar=c(5.1,4.1,4.1,0.1))
for(i in areasort){
  plot(ecdf(sample(precisionplotsno[[i]],10000)),
       main = g1g2namesall68$AcceptedName[reallistofTiffs[which(!is.na(whichsigndiffprec))][i]], xlim = c(0,1), col="red",
       cex.main=2)
  lines(ecdf(sample(precisionplotswith[[i]],10000)), col="blue")
}


dev.off()

```



#  Habitat specificity    
Principal components analysis of predictor variables    
PCA for how different - 
calculate the size of the ellipse <https://stackoverflow.com/questions/38782051/how-to-calculate-the-area-of-ellipse-drawn-by-ggplot2>
```{r}
# habsp_EOR <- do.call(rbind, lapply(reallistofTiffs, function(i){
#   polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
#                                         g1g2namesall68$Taxon[i]),]
#   proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
#   EORpnts <- do.call(rbind, lapply(1:length(polys), function(f){
#     # area <- sapply(slot(polys,"polygons"), slot, "area")[f]
#     # round(sqrt(area)/165,0) # meters along a 'side'; bioclim is at about 165 meters
#       outline <- polys@polygons[[f]]@Polygons[[1]]@coords # Use these and grid within
#       grid <- makegrid(polys, cellsize = 165) # cellsize in map units!
#       names(grid) <- c("x","y")
#       gridout <- grid[splancs::inout(grid,outline), ]
#       if(nrow(gridout)==0){
#         gridout <- data.frame(x=outline[,1],y=outline[,2])
#       }
#       gridout
#       }))
#   coordinates(EORpnts) <- ~x+y
#   proj4string(EORpnts) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
#   EORpnts <- spTransform(EORpnts, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
#   gridextract <- extract(rasterstack,EORpnts)
#     out <- data.frame(SpeciesNum=i, Species=g1g2names$AcceptedName[i], EORpnts@coords, gridextract)
#     out
#     }))
# 
# 
# prPCAEOR <- princomp(habsp_EOR[,-c(1:4)])
# summary(prPCAEOR)
# loadings(prPCAEOR)
# 
# allsphabsp_EOR <- data.frame(habsp_EOR[,1:2],prPCAEOR$scores)
# 
# # for one run of one species
# ggplot(allsphabsp_EOR, aes(Comp.1,Comp.2, colour=as.factor(SpeciesNum)))+
#   geom_point(alpha=0.1)+
#   stat_ellipse(segments=201)+ #default is to draw 51 line segments to make the ellipse
#   theme_bw()+
#   xlab(expression(atop("PC1 (97% variance explained)", "Elevation = 0.981; Bio12 = 0.193")))+
#   ylab(expression(atop("PC2 (2% variance explained)","Aspect = -0.990; Bio12 = -0.139")))
#   
# 
# # link to all the data about from table 1
# habsp
# specificitytotab1 <- merge(allsphabsp_EOR, tab1, by.x = "SpeciesNum", by.y = "SpNum1")
# 
# 
# ggplot()
# 
# 
# table(allsphabsp_EOR$SpeciesNum) # 8 has one point, 51 has 2
# 
# # for all species
# habspecificity_EOR <- do.call(rbind,lapply(atleast12[!atleast12 %in% c(8, 51)], function(x){
#   # get and average across the replicates
#   p <- ggplot(allsphabsp_EOR[allsphabsp_EOR$SpeciesNum==x,],
#               aes(Comp.1,Comp.2))+
#           geom_point()+
#           stat_ellipse(segments=201) #default is to draw 51 line segments to make the ellipse
#   # get ellipse coordinates
#   pb <- ggplot_build(p)
#   table(pb$data[[2]]$group)
#   el <- pb$data[[2]][c("x","y")]
#   
#   # Center of ellipse
#   ctr <- MASS::cov.trob(el)$center
#   
#   # Distance to center from each point on ellipse
#   dist2center <- sqrt(rowSums((t(t(el)-ctr))^2))
#   
#   # Area of ellipse from semi-major and semi-minor axes which are largest and smallest of dist to center
#   habitatspecificity <- pi*min(dist2center)*max(dist2center)
#   hsout <- data.frame(SpeciesNum = x, habspec = habitatspecificity)
#   hsout
#   }))

```




# Need to have both in same PCA!!! 
```{r}
# pathstarterror <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
# maxones <- "maxentHerbnoerror"
# 
# # But these treat each separetly and I need them together in one getting the ellipse model!
# habspsame <- habitatSpecificity(whichones = atleast12, pathstart = pathstarterror,
#                                 whichmaxentpoints = maxones, replicates = 10)
# 
# habspwitherror <- habitatSpecificity(whichones = atleast12, pathstart = pathstarterror,
#                                 whichmaxentpoints = "maxentHerbWITHerror", replicates = 10)
# #####
# 
# pathstarterrornoerror <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/"
# 
# ### both at once in one PCA
# # HABitat SPecificity with NO error
#  habspno <- do.call(rbind,lapply(atleast12, function(x){
#    outno <- do.call(rbind,lapply(1:10, function(rep){
#       load(paste(pathstarterrornoerror,"maxentHerbnoerror","Sp", x,"kfold",rep, ".Rda",sep=""))
#       outinner <- data.frame(ErrorType = "no", SpeciesNum = x, kfold = rep, xm@presence)
#       outinner
#       }))
#    outno
#    }))
#  
#  habspwith <- do.call(rbind,lapply(atleast12, function(x){
#   outno <- do.call(rbind,lapply(1:10, function(rep){
#     load(paste(pathstarterrornoerror,"maxentHerbWITHerror","Sp", x,"kfold",rep, ".Rda",sep=""))
#     outinner <- data.frame(ErrorType = "yes", SpeciesNum = x, kfold = rep, xm@presence)
#     outinner
#     }))
#   outno
#   }))
#  
#  names(habsp_EOR)
#  names(habsp)
#   
# habsp <- rbind(habspno,habspwith)
# table(habsp$ErrorType)
# habsp_andEOR <- rbind(habsp, 
#                       data.frame(ErrorType = "EOR", SpeciesNum = habsp_EOR[,1], 
#                                  kfold = 1, habsp_EOR[,c(5:9)]))
# 
# table(habsp_andEOR$ErrorType)
# 
# # prPCA <- princomp(habsp[,-c(1:3)])
# # allsphabsp <- data.frame(habsp[,1:3],prPCA$scores)
# 
# prPCA <- princomp(habsp_andEOR[,-c(1:3)])
# allsphabsp <- data.frame(habsp_andEOR[,1:3],prPCA$scores)
# 
# 
# EORpnts_8 <- do.call(rbind, lapply(1:length(polys), function(f){
#       outline <- polys@polygons[[f]]@Polygons[[1]]@coords
#       grid <- makegrid(polys, cellsize = 5) # cellsize in map units!
#       names(grid) <- c("x","y")
#       gridout <- grid[inout(grid,outline), ]
#       if(nrow(gridout)>1){
#         gridthin <- thin.max(gridout,c("x","y"),nrow(distXsp_v3[[8]][[1]]@coords))
#         gc()
#         } else {
#           if(nrow(gridout)>0) {
#             gridthin <- gridout 
#           } else {
#             gridthin <- NULL
#           }}
#       gridthin
# }))
```

# Habitat Specificity with EORs, no error and additional error in one PCA
# This one is the final one!     
Another function to have all the habitat specificities in one place
```{r}
pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/")

habitatSpecificity <- function(whichones, pathstart, replicates){
  habspno <- do.call(rbind,lapply(whichones, function(x){
  outno <- do.call(rbind,lapply(1:replicates, function(rep){
    load(paste(pathstart,"maxentHerbnoerror","Sp", x,"kfold",rep, ".Rda",sep=""))
    outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = "NoError", xm@presence)
    outinner
    }))
  outno
  }))
  
  habspwith <- do.call(rbind,lapply(whichones, function(x){
  outWITH <- do.call(rbind,lapply(1:replicates, function(rep){
    load(paste(pathstart,"maxentHerbWITHerror","Sp", x,"kfold",rep, ".Rda",sep=""))
    outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = "WithError",xm@presence)
    outinner
    }))
  outWITH
  }))
  
  habspEOR <- do.call(rbind,lapply(whichones, function(x){
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[x],
                                          g1g2namesall68$Taxon[x]),]
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") 
    EORpnts <- do.call(rbind, lapply(1:length(polys), function(f){
         outline <- polys@polygons[[f]]@Polygons[[1]]@coords
         outlineout <- data.frame(x=outline[,1],y=outline[,2])
         grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
         names(grid) <- c("x","y")
         gridout <- grid[inout(grid,outline), ]
         pntsout <- rbind(gridout,outlineout)
         pntsout
         }))
    # Get the sample size of one replicate of each species (x)
    habspeor <- habspno[habspno$SpeciesNum==x & habspno$kfold == 1,]
    if(nrow(EORpnts)>nrow(habspeor)) EORpnts <- thin.max(EORpnts, c("x","y"), nrow(habspeor))
    convertgridthin <- SpatialPoints(EORpnts[,c("x","y")], proj4string = CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
    gridthin <- spTransform(convertgridthin, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
    gridextract <- extract(rasterstack,gridthin)
    out <- data.frame(SpeciesNum = x, kfold = 1, HerbType = "EOR",gridextract)
    out
  }))

  habsp <- rbind(habspno,habspwith, habspEOR)
  
  prPCA <- princomp(habsp[,-c(1:3)])
  allsphabsp <- data.frame(habsp[,1:3],prPCA$scores)
  allsphabsp$HerbType <- as.character(allsphabsp$HerbType)
  
  # for all species and both error and no and EOR, dropping parameters (kfolds in EOR; only 1)
  habspecificity <- do.call(rbind,
                            lapply(split(allsphabsp, 
                                         list(allsphabsp$SpeciesNum,
                                              allsphabsp$kfold,
                                              allsphabsp$HerbType), drop=TRUE), function(x){

      p <- ggplot(x, aes(Comp.1,Comp.2))+
      geom_point()+
      stat_ellipse(segments=201) #default is to draw 51 line segments to make the ellipse
    # get ellipse coordinates
    pb <- ggplot_build(p)
    table(pb$data[[2]]$group)
    el <- pb$data[[2]][c("x","y")]
    
    # Center of ellipse
    ctr <- MASS::cov.trob(el)$center
    
    # Distance to center from each point on ellipse
    dist2center <- sqrt(rowSums((t(t(el)-ctr))^2))
    
    # Area of ellipse from semi-major and semi-minor axes which are largest and smallest of dist to center
    habitatspecificity <- pi*min(dist2center)*max(dist2center)
    ellipseoutfold <- data.frame(SpeciesNum = unique(x$SpeciesNum), 
                                 kfold = unique(x$kfold), 
                                 HerbType = unique(x$HerbType), habspec = habitatspecificity)
    ellipseoutfold
        }))
  habspecificity
}
```

# load the saved habspEORandNoandWith
```{r}  

pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/")

habspEORandNoandWith <- habitatSpecificity(reallistofTiffs, pathstart, replicates = 10)
save(habspEORandNoandWith, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/habspEORandNoandWith.Rda")

```

# oops, need the loadings of the PCA    
```{r}
whichones <- reallistofTiffs
  habspno <- do.call(rbind,lapply(whichones, function(x){
  outno <- do.call(rbind,lapply(1:10, function(rep){
    load(paste(pathstart,"maxentHerbnoerror","Sp", x,"kfold",rep, ".Rda",sep=""))
    outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = "NoError", xm@presence)
    outinner
    }))
  outno
  }))
  
  habspwith <- do.call(rbind,lapply(whichones, function(x){
  outWITH <- do.call(rbind,lapply(1:10, function(rep){
    load(paste(pathstart,"maxentHerbWITHerror","Sp", x,"kfold",rep, ".Rda",sep=""))
    outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = "WithError",xm@presence)
    outinner
    }))
  outWITH
  }))
  
  habspEOR <- do.call(rbind,lapply(whichones, function(x){
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[x],
                                          g1g2namesall68$Taxon[x]),]
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") 
    EORpnts <- do.call(rbind, lapply(1:length(polys), function(f){
         outline <- polys@polygons[[f]]@Polygons[[1]]@coords
         outlineout <- data.frame(x=outline[,1],y=outline[,2])
         grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
         names(grid) <- c("x","y")
         gridout <- grid[inout(grid,outline), ]
         pntsout <- rbind(gridout,outlineout)
         pntsout
         }))
    # Get the sample size of one replicate of each species (x)
    habspeor <- habspno[habspno$SpeciesNum==x & habspno$kfold == 1,]
    if(nrow(EORpnts)>nrow(habspeor)) EORpnts <- thin.max(EORpnts, c("x","y"), nrow(habspeor))
    convertgridthin <- SpatialPoints(EORpnts[,c("x","y")], proj4string = CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
    gridthin <- spTransform(convertgridthin, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
    gridextract <- extract(rasterstack,gridthin)
    out <- data.frame(SpeciesNum = x, kfold = 1, HerbType = "EOR",gridextract)
    out
  }))

  habsp <- rbind(habspno,habspwith, habspEOR)
  
  prPCA <- princomp(habsp[,-c(1:3)])

  loadings(prPCA)
summary(prPCA)
```

Plot habitat specificity    
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/habspEORandNoandWith.Rda")
habspEORandNoandWith[habspEORandNoandWith$HerbType=="EOR",] # 33 rows, yay!

habspe <- merge(habspEORandNoandWith, tab1, by.x = "SpeciesNum", by.y = "SpNum1")
table(habspe$HerbType)

ggplot(habspe, aes(as.factor(round(Area/(1000^2),0)), habspec, colour=HerbType))+
  geom_jitter()+
  geom_boxplot(position=position_dodge(width=-0.5))+
  theme_bw()+
  theme(legend.position = "none")+
  xlab(expression("Area km"^2))+
  geom_boxplot()

ggplot(habspe, aes(round(Area/(1000^2),0), habspec, colour=HerbType))+
  geom_violin()+
  stat_smooth(method="lm")+
  geom_point()+
  theme_bw()+
  theme(legend.position = "none")+
  xlab(expression("Area km"^2))
```



```{r}


jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure3.jpg",
     width=150, height=100,units='mm', res=300)

g <- ggplotGrob(ggplot(habspe[habspe$Area<max(habspe$Area),], 
          aes(Area/(1000^2), habspec/1000, shape=HerbType, colour=HerbType))+
      geom_point(position = position_dodge(width=.25), size = 0.25)+
      stat_smooth(method="lm", se=FALSE, lwd = 0.5)+
      theme_bw()+
      scale_colour_manual(values = c("black","grey50","goldenrod"))+ #  "EOR"  "NoError"   "WithError"==grey80
      theme(legend.position = "none")+
        xlab("")+
        ylab(""))
      # xlab(expression("Area km"^2))+
      # ylab("Habitat specificity (size of 95% CI)"))
  
ggplot(habspe, aes(Area/(1000^2), habspec/1000, shape=HerbType, colour=HerbType))+
            geom_point(position = position_dodge(width=2))+
            stat_smooth(method="lm", se=FALSE)+
            theme_bw()+
            scale_colour_manual(values = c("black","grey50","goldenrod"))+ #  "EOR"   "NoError"   "WithError"
            theme(legend.position = "none")+
            xlab(expression("Area km"^2))+
            ylab("Habitat specificity (size of 95% CI)")+
  annotation_custom(
    grob = g,
    xmin = 42,
    xmax = 80,
    ymin = 820,
    ymax = 1410
  # ) +
  # annotation_custom(
  #   grob = rectGrob(gp = gpar(col = rgb(0,0,0,0.25), fill = rgb(0,0,0,0))),
  #   xmin = -2,
  #   xmax = max(habspe$Area[habspe$Area<max(habspe$Area)])/(1000^2)+5,
  #   ymin = 0,
  #   ymax = max(habspe$habspec[habspe$Area<max(habspe$Area)])/1000+15
  )


dev.off()

```


Distance from all specimens to all EORs regardless of year      
See distXsp_noEORgrid <- in "Herb to EORs_v4.Rmd"
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid.Rda")

allspecies <- mapply(cbind, distXsp_noEORgrid, "SpeciesNum" = 1:60, SIMPLIFY = FALSE)
allspecies <- do.call(rbind, allspecies[notnull])
gc()

```



# What are the AUC scores for all of these?   
```{r}

pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/")
aucs_all <- do.call(rbind,lapply(c("no","WITH"), function(w){
      aucs_no <- do.call(rbind,lapply(reallistofTiffs, function(x){
        aucs.out <- do.call(rbind,lapply(1:10, function(rep){
          load(paste(pathstart,"evaluateHerb",w,"error","Sp",x,"kfold",rep,".Rda", sep=""))
          data.frame(SpNum = x, kfold = rep, AUC = e@auc, DataType = w)
        }))
        aucs.out
      }))
      aucs_no
      }))
ggplot(aucs_all, aes(DataType, AUC))+
  geom_boxplot()+
  geom_jitter(position=position_jitter(0.1), alpha=0.5, pch=16)+
  theme_bw()

t.test(aucs_all$AUC~aucs_all$DataType) # CI of  0.047 of the difference
levels(aucs_all$DataType) # no = x, WITH = y

aggregate(AUC~DataType, data = aucs_all, mean)
aggregate(AUC~DataType, data = aucs_all, sd)

```




# Niche overlap   
<https://www.rdocumentation.org/packages/phyloclim/versions/0.9.5/topics/niche.overlap>   
<https://rdrr.io/rforge/fuzzySim/man/modOverlap.html>   
<https://github.com/danlwarren/ENMTools>  

```{r}

library(devtools)
# install.packages("ecospat")
library(ecospat)
install_github("danlwarren/ENMTools")
install_local("C:/Users/deprengm/Downloads/ENMTools-master.zip")
library(ENMTools)

library(ENMeval)
# ?nicheOverlap


nicheoverlaps <- lapply(c(reallistofTiffs), function(i){
      gc()
      resultpathno <- list.files(path = pathstart, 
                               pattern = paste("AvgTiffSp",i,"Herbnoerror",sep=""), 
                               full.names=TRUE)
      rastoutno <- raster(resultpathno)
      resultpathWITH <- list.files(path = pathstart, 
                         pattern = paste("AvgTiffSp",i,"HerbWITHerror",sep=""), 
                         full.names=TRUE)
      rastoutWITH <- raster(resultpathWITH)
      nicheOverlap(rastoutno,rastoutWITH)
      })
save(nicheoverlaps, file = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/nicheoverlaps.Rdata")


differentones <- do.call(rbind,nicheoverlaps)[which(!is.na(whichsigndiff))]
sameones <- do.call(rbind,nicheoverlaps)[which(is.na(whichsigndiff))]

hist(differentones, breaks=20, col=rgb(0,1,0,0.5))
hist(sameones, breaks=20, col="blue", add=TRUE)

```

```{r}
rasterOptions(maxmemory = 1e+09)
# default:
# rasterOptions(maxmemory = 1e+08)

# Where in rasterstack are the values of highest, next chunck and so on in the SDM for no and with?
nichevaluesbySDM <- do.call(rbind,lapply(c(reallistofTiffs), function(i){
      gc()
      resultpathno <- list.files(path = pathstart, 
                               pattern = paste("AvgTiffSp",i,"Herbnoerror",sep=""), 
                               full.names=TRUE)
      rastoutno <- raster(resultpathno)
      
      resultpathWITH <- list.files(path = pathstart, 
                         pattern = paste("AvgTiffSp",i,"HerbWITHerror",sep=""), 
                         full.names=TRUE)
      rastoutWITH <- raster(resultpathWITH)
      
      valuesout <- values(stack(rasterstack,rastoutno,rastoutWITH))
      df <- data.frame(matrix(valuesout,ncol=7))
      out <- data.frame(df[sample(1:nrow(df),1000000),], SpNum = i)
      gc()
      names(out) <- c("Elevation","Aspect","Rugged","Bio1","Bio12","no","WITH","SpNum")
      out
}))

 save(nichevaluesbySDM, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/nichevaluesbySDM.Rda")        

head(nichevaluesbySDM)
```


# Want the niche values of as-is and additional error for within the EOR polygons 

```{r}

i <- reallistofTiffs[1]
pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/")

# make zones?
nichevaluesinCNHP <- do.call(rbind,lapply(c(reallistofTiffs), function(i){
      gc()
  polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                        g1g2namesall68$Taxon[i]),]
  polys
  
      resultpathno <- list.files(path = pathstart, 
                               pattern = paste("AvgTiffSp",i,"Herbnoerror",sep=""), 
                               full.names=TRUE)
      rastoutno <- raster(resultpathno)
      
      proj4string(polys) <-  CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
      polylatlon <- spTransform(polys, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      maskr <- crop(rastoutno, extent(polylatlon))
      maskr <- mask(maskr, polylatlon)
      maskr <- trim(maskr, values=NA)
      rastoutno[rastoutno]
      
  plot(rastoutno)
  lines(polylatlon)
  
  plot(maskr)
      
      resultpathWITH <- list.files(path = pathstart, 
                         pattern = paste("AvgTiffSp",i,"HerbWITHerror",sep=""), 
                         full.names=TRUE)
      rastoutWITH <- raster(resultpathWITH)
      
      valuesout <- values(stack(rasterstack,rastoutno,rastoutWITH))
      df <- data.frame(matrix(valuesout,ncol=7))
      out <- data.frame(df[sample(1:nrow(df),1000000),], SpNum = i)
      gc()
      names(out) <- c("Elevation","Aspect","Rugged","Bio1","Bio12","no","WITH","SpNum")
      out
}))

 save(nichevaluesinCNHP, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/nichevaluesinCNHP.Rda")        



```




# Very big! slow your roll, don't load unless you need it
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/nichevaluesbySDM.Rda") 

toplot <- do.call(rbind,lapply(names(nichevaluesbySDM)[1:5], function(column){
  print(column)
  tenths <- do.call(rbind,lapply(seq(0.1,1,by = 0.1), function(t){
    print(t)
    noerrorvalues <- data.frame(nichevaluesbySDM[nichevaluesbySDM$no>(t-0.0999) & nichevaluesbySDM$no<=t,
                     c(column,"SpNum","no")],DataType = "no", 
                     SuitableRange = paste(t-0.1,t,sep="-"), Predictor = column)
    names(noerrorvalues) <- c("Value",names(noerrorvalues)[2], "SDM",names(noerrorvalues)[4:6])
    WITHerrorvalues <- data.frame(nichevaluesbySDM[nichevaluesbySDM$WITH>(t-0.0999) & nichevaluesbySDM$WITH<t,
                     c(column,"SpNum","WITH")], DataType = "WITH", 
                     SuitableRange = paste(t-0.1,t,sep="-"), Predictor = column)
    names(WITHerrorvalues) <- c("Value",names(noerrorvalues)[2], "SDM",names(WITHerrorvalues)[4:6])
    out <- rbind(noerrorvalues, WITHerrorvalues)
    # out <- out[sample(1:nrow(out), 100000),]
    gc()
    out
  }))
  tenths
}))


 save(toplot, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/toplot.Rda")   

```

# Across the entire raster 
# ### RUN AGAIN AND SEE IF DIFFERENT since nothing is a strong predictor.  
# find one 
```{r} 
 load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_herb_herbpluserror/toplot.Rda")

ggplot(toplot, aes(DataType,Value))+
  geom_boxplot()+
  theme_bw()+
  facet_wrap(~Predictor, scales="free")

boxplot(Value~DataType:SuitableRange, data=toplot)

ggplot(toplot[toplot$DataType=="no",], aes(SuitableRange,Value))+
  geom_boxplot()+
  theme_bw()+
  facet_wrap(~Predictor, scales="free")

ggplot(toplot, aes(SuitableRange, Value, colour= DataType))+
  geom_boxplot()+
  theme_bw()+
  scale_colour_grey()+
  facet_wrap(~Predictor, scales="free")

boxplot(nichevaluesbySDM$Elevation[nichevaluesbySDM$no > 0 & nichevaluesbySDM$no <= .1 ])
### Optional analysis 1
 # what are the probabilities with in the mapped CNHP - likely are the same but could still look 
# Could compare variables within to without the polygons - might still not really show much pattern 
```

```{r}
boxplot(Value~DataType:SuitableRange, data=toplot[toplot$Predictor == "Elevation",],
        main="Elevation",
        col=rep(c("green","blue"),10), par(las=2))
boxplot(Value~DataType:SuitableRange, data=toplot[toplot$Predictor == "Aspect",],
        main="Aspect",
        col=rep(c("green","blue"),10), par(las=2))

boxplot(Value~DataType:SuitableRange, data=toplot[toplot$Predictor == "Rugged",],
        main="Rugged",
        col=rep(c("green","blue"),10), par(las=2))

boxplot(Value~DataType:SuitableRange, data=toplot[toplot$Predictor == "Bio1",],
        main="Bio1",
        col=rep(c("green","blue"),10), par(las=2))

boxplot(Value~DataType:SuitableRange, data=toplot[toplot$Predictor == "Bio12",],
        main="Bio12",
        col=rep(c("green","blue"),10), par(las=2))

```




```{r}
nichevaluesbySDM[[1]]

calcnicheoverlaps <- lapply(c(reallistofTiffs), function(i){
      gc()
      resultpathno <- list.files(path = pathstart, 
                               pattern = paste("AvgTiffSp",i,"Herbnoerror",sep=""), 
                               full.names=TRUE)
      rastoutno <- raster(resultpathno)
      resultpathWITH <- list.files(path = pathstart, 
                         pattern = paste("AvgTiffSp",i,"HerbWITHerror",sep=""), 
                         full.names=TRUE)
      rastoutWITH <- raster(resultpathWITH)
      calc.niche.overlap(stack(rastoutno,rastoutWITH), stat="I")
      })

nicheoverlaps[[1]]
nicheoverlaps

nicheoverlapstobind <- data.frame(SpNum = reallistofTiffs, NicheOverlap = do.call(c,nicheoverlaps))
df_dist2 <- merge(df_dist, nicheoverlapstobind, by = "SpNum")

ggplot(df_dist2, aes(Area, NicheOverlap, colour = AccuracyDifferent))+
  geom_point()

ggplot(df_dist2, aes(AccuracyDifferent, NicheOverlap))+
  geom_boxplot()

df_dist3 <- merge(df_dist2, aucs_all, by = "SpNum")


ggplot(df_dist3, aes(NicheOverlap,AUC,colour = DataType))+
  geom_point()+
  stat_smooth(method="lm")+
  theme_bw()

ggplot(df_dist2, aes(Area, NicheOverlap, colour = AccuracyDifferent))+
  geom_point()+
  stat_smooth(method="lm")+
  theme_bw()

df_dist3 # DataType no, WITH
habspEORandNoandWith # HerbType: EOR, NoError, WithError
habsptomerge <- habspEORandNoandWith
habsptomerge$HerbType <- as.character(habsptomerge$HerbType)
habsptomerge$HerbType[habsptomerge$HerbType == "NoError"] <- "no"
habsptomerge$HerbType[habsptomerge$HerbType == "WithError"] <- "WITH"

df_dist4 <- merge(df_dist3,habsptomerge, by.x =c("SpNum","DataType"), 
                  by.y = c("SpeciesNum","HerbType"))

ggplot(df_dist4, aes(Dist, NicheOverlap))+
  geom_point()+
  stat_smooth(method="lm")
```


# Sample from random simulated data to say, given the same amount of random error seen above, can I get the patters of strange distribution?
```{r}



```




