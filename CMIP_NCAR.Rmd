---
title: "CMIP_NCAR"
author: "Michelle DePrenger-Levin"
date: "February 22, 2017"
output: html_document
---
<https://www2.cisl.ucar.edu/sites/default/files/NychkaCMIP.pdf>

cygwin (not the 64 one!)         
cygwin Rclick run as administrator            
  DISPLAY=localhost:0.0              
ssh -XY deprengm@yellowstone.ucar.edu            

cd file name (otherwise Permission denied because it's trying to run as an excecutable)

# Don't open as administrator! open cygwin terminal
Point to where you want to save stuff (to use ".")
cd ..
   /home
cd ..
   /
cd P:
cd hackathon
cd Simulations

#Save file to yellowstone
from cygwin pointing to local drive, pre logging into yellowstone
scp FILENAME.r deprengm@yellowstone.ucar.edu:/glade/u/home/deprengm

#Save files from yellowstone to local
from cygwin pointing to local drive, 

#Run an R script that I've saved to yellowstone
R --no-save < FILENAME.r

#Open R script to edit but keep background so can still type in yellowstone cygwin
emacs FILENAME.r & 
or maybe just type bg after? can't remember excactly...
cnrt+x then s to save

Middle mouse click to paste whatever is highlighted

look around in yellowstone /glade/
large-sclae (stable) precipitation rate (liq+ice)
ncdump -h /glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/PRECL/b.e11

look up cmd line and nco operators for more shotcuts

ls for whats in there


R to get into R
q() to get out of R
http://www.cs.dartmouth.edu/~campbell/cs50/putty-cygwin-tutorial.pdf 


netCDF to raster <http://www.timassal.com/?p=1210>

Look up cmd line operators and NCO operators
use emacs (when xterm working) to open editable window for R
  ctrl X + 1 ?? or & after to keep in background
Save = cntr + X S
Run script with R --no-save < file name


scp deprengm@yellowstone.ucar.edu:/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.001.cam.h0.TS.185001-200512.nc .

rcp85.toga??

Data https://www.ncdc.noaa.gov/data-access/model-data/model-datasets 

https://www.esrl.noaa.gov/psd/data/gridded/data.cmap.html 

~100 KM 
PRECC foat means double
date is first day of next month for the previous month

just subtract 360 - my longitude

add PRECC and PRECL - monthly mean data, matching 
or average over the 30 runs for past and future
Rain and snow: PRECC + PRECL
SNOW: PRECSC + PRECSL


TS instead of PRECC is temperature "TS" is the variable for surface temperature averaged over a month

or in monthly TSMN and TSMX monthly min and max 

for daily
2 meter temerature TREFHTMN TREFHTMX for daily min and max

like evaporation is flux

bracket annual, chuncks, see change. 2081 - 2100 and see change precip. 

Later chunk up spots
25 KM in a couple simulations Rich will find it for me. 

#######################
#######################
#######################  
# NCAR data
#######################
#######################
#######################
Quick description
CESM model has an 

'B' prefix = individual land, atmosphere, ocean and sea-ice model running together. 
 
'F' prefix = turns off ocean
 
'E" prefix = replace the complex ocean model in 'B' and replace it with a very simple model - probably not important for us.
 
The next set of numbers refers to the time period.
'1850' - Says it is a simulation that is meant to be in 1850, or more generally pre-industrial conditions, where the influence of humans was effectively zero, so a 'natural' climate thought to be in equilibrium.
 
'20TRC' - Simulation of the20th century climate from 1850 to 2005 where the influence of humans due to e.g., increasing greenhouse gasses (CO2) is gradually ramped up leading to warming of the whole system. including surface temperatures.
 
F1850C5/FAMIPC5 - Not really important to us. Just relates to the time periods for the prescribed Sea Surface Temperatures in the 'F' cases
 
'RCP8.5' - These are simulations where we are estimating what the future impact on climate would be (e.g., in terms of increasing greenhouse gases) by the end of this century. This is a pretty non-optimistic scenario where we don't do much to curb fossil fuel and generally results in a very large temperature increase by 2100.
 
The different time periods are just a file size convenience. The B1850 simulation is just split int 100-yr chunks. The RCP files split off the last 20-years not sure why. You should be able to read in the whole period of runs and stitch the data together.

<http://www.cesm.ucar.edu/models/cesm1.0/cam/docs/ug5_0/hist_flds_fv_cam5.html> 


```{r}
library(ncdf4)
library(maptools)
library(extRemes)
library(fields)
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(abind)
library(prism)

library(raster)
library(rasterVis)


library(measurements)
library(ENMeval)
library(RCurl)

library(devtools)
#install_github('hadley/rvest')
library(rvest)

# add the command to bind along the 3rd dimension
abind3 <- function(...) { abind(along = 3, ...) }

load("P:/hackathon/Simulations/modelavg_annual.RData")
load("P:/hackathon/Simulations/model25_annual.RData")
load("P:/hackathon/Simulations/modelavg.raster.RData")
load("P:/hackathon/Simulations/modelavg_annual2081.RData")
load("P:/hackathon/Simulations/modelavg_annual2081.RData")
load("P:/hackathon/Simulations/model20_annual.RData")
load("P:/hackathon/Simulations/model25_raster.RData")
load("P:/hackathon/Simulations/TSMNmodelavg_annual.RData")
load("P:/hackathon/Simulations/TSMNmodel25_annual.RData")


```

Average and variation of 30 models
```{r}
sims <- sprintf('%0.3d', 1:30)

precc2006_2080 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECC.200601-208012.nc", collapse="", sep=""))
})

precl2006_2080 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECL.200601-208012.nc", collapse="", sep=""))
})


time <- ncvar_get(precl2006_2080[[1]], "date")
length(time)
```


Sum 12 months for annual conductive and bigger precipitation
Then add two together
####Conductive
```{r}
precc_annual <- lapply(1:30, function(model){
  precc1 <- ncvar_get(precc2006_2080[[model]], "PRECC")
  cl <- makeCluster(6)
  registerDoParallel(cl)
  precc <- foreach(timeindex = seq(1,900,12),
                   .combine = "abind3") %dopar% 
    {
      apply(precc1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
      }
  stopCluster(cl)
  gc()
  rm(precc1)
  precc
  })

```

####Large Scale precipitation
```{r}
precl_annual <- lapply(1:30, function(model){
  precl1 <- ncvar_get(precl2006_2080[[model]], "PRECL")
  cl <- makeCluster(6)
  registerDoParallel(cl)
  precl <- foreach(timeindex = seq(1,900,12),
                   .combine = "abind3") %dopar% 
    {
      apply(precl1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
      }
  stopCluster(cl)
  gc()
  rm(precl1)
  precl
  })

save(list="precl_annual", file="P:/hackathon/Simulations/precl_annual.RData")
```

Check dimensions
```{r}
dimens <- lapply(1:30, function(mod){
  rbind(dim(precc_annual[[mod]]),
        dim(precl_annual[[mod]]))
        })

do.call(rbind, dimens)
  
```


Sum annual CC and CL precip
```{r}
prectotal_annual <- lapply(1:30, function(model){
  Ycc <- precc_annual[[model]]
  Ycl <- precl_annual[[model]]
  cl <- makeCluster(6)
  registerDoParallel(cl)
  total <- foreach(timeindex = 1:75,
                   .combine = "abind3") %dopar% 
    {
      Ycc[,,timeindex]+Ycl[,,timeindex]
    }
  stopCluster(cl)
  gc()
  rm(Ycc,Ycl)
  total
  })

```


Average and variance over the 30 models, annual
modelavg_annual is [lon, lat, avg then var per year]   

```{r}
cl <- makeCluster(6)
registerDoParallel(cl)
modelavg_annual <- foreach(timeindex = 1:75,
                           .packages = "abind") %dopar%
  {
    byyearmodel <- lapply(prectotal_annual, function(model1){
      model1[,,timeindex]
    })
    oneyear <- abind(byyearmodel, along=3)
    abind(apply(oneyear, MARGIN = c(1,2), mean),
          apply(oneyear, MARGIN = c(1,2), var), along=3)
  }
stopCluster(cl)

save(list="modelavg_annual", file="P:/hackathon/Simulations/modelavg_annual.RData")
class(modelavg_annual[[1]])
dim(modelavg_annual[[1]])

```

#Can skip all preceeding for 2006 2080 precip
```{r}
load("P:/hackathon/Simulations/modelavg_annual.RData")

```


Error in ncvar_add(nc, vars[[ivar]], verbose = verbose, indefine = TRUE) : 
  Error in ncvar_add, defining var lon
```{r}

# make netCDF and export 
Longvector <- seq(-180, 180, length = 288)
Latvector <- seq(-90, 90, length = 192)
# Define data, will go down each column 
dataset <- c(modelavg_annual[[1]][,,1])

#Dimensions
dimX <- ncdim_def("lon","degree_east", Longvector)
dimY <- ncdim_def("lat", "degree_noth", Latvector)
dimT <- ncdim_def("date", "year", 1:(288*192), unlim=TRUE)

#define missing value
mv <- -9999

# Define the data, normalize to cm/day Y*3600*(24/10)
var_precip <- ncvar_def( "Prec_cm/day", "cm/day", 
                         list(dimX,dimY,dimT), mv, prec="double")
varlon <- ncvar_def("lon","degrees", list(dimX,dimY), mv, prec="double")
varlat <- ncvar_def("lat", "degrees", dimX, mv, prec="double")

# Create the NetCDF file
# If you want a NetCDF4 file, explicitly add force_v4=T
nc <- nc_create("annual2006_2080.nc", list(varlon, varlat, var_precip))

# Write data to the NetCDF file
ncvar_put(nc, var_precip, dataset, start=c(1, 1, 1),
    count=c(288, 192, 1)) #dim(modelavg_annual[[1]][,,1])

# Close your new file to finish writing
nc_close(nc)

levelplot(raster(annual2006_2008.nc))
```

Visualize the average and var
Don't need to make these again
```{r, eval=FALSE}
#make rasters
modelavg.raster <- lapply(modelavg_annual, function(model1){
  e <- extent(c(0,360,-90,90))
  mean.raster <- flip(raster(t(as.matrix(model1[,,1], nrow=192))),
                      direction = "y")
  extent(mean.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  var.raster <- flip(raster(t(as.matrix(model1[,,2], nrow=192))),
                      direction = "y")
  extent(var.raster) <- e
  proj4string(var.raster) <- CRS("+init=epsg:4326")
  list(mean.raster,var.raster)
  })

years <- c(2006:2080)
type <- c("avg","var")


#Might need to rerun! I think I saved over this with the temp min, I think I saved again after loading it...
save(list="modelavg.raster", file="P:/hackathon/Simulations/modelavg.raster.RData")



#projection
#"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
# EPSG:4326

#write rasters
for(i in 1:75){
  for(j in 1:2){
    writeRaster(rotate(modelavg.raster[[i]][[j]]), 
                paste("P:/hackathon/Simulations/",type[j],years[i],collapse="",sep=""),
                format = "GTiff")
  }
}

```

```{r}
data(wrld_simpl)
plot(rotate(modelavg.raster[[70]][[1]]))
plot(wrld_simpl, add=TRUE)
```

Average of      
Average and variance over 25 year periods: 2006:2030, 2031:2055, and 2056:2080   
```{r}

model25_annual <- lapply(seq(1,75,25), function(year){
  model1 <- abind(modelavg_annual[year+24], along=3)
  mn <- apply(model1[,,1], MARGIN = c(1,2), mean)
  vr <- apply(model1[,,2], MARGIN = c(1,2), mean)
  list(mn,vr)
})



save(list="model25_annual", file="P:/hackathon/Simulations/model25_annual.RData")


```

#Just load model25_annual
```{r}
load("P:/hackathon/Simulations/model25_annual.RData")

```


```{r}
model25.raster <- lapply(model25_annual, function(model1){
  e <- extent(c(0,360,-90,90))
  mean.raster <- flip(raster(t(as.matrix(model1[[1]], nrow=192))),
                      direction="y")
  extent(mean.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  var.raster <- flip(raster(t(as.matrix(model1[[2]], nrow=192))),
                      direction = "y")
  extent(var.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  list(mean.raster,var.raster)
  })

save(list="model25.raster", file="P:/hackathon/Simulations/model25_raster.RData")

yrs <- c("2006_2030","2031_2055","2056_2080")
#write rasters
for(k in 1:3){
  for(j in 1:2){
    writeRaster(rotate(model25.raster[[k]][[j]]),
                paste("P:/hackathon/Simulations/",type[j],yrs[k],collapse="",sep=""),
                format = "GTiff")
    
  }
}

```

Average and SD of 29 models (the first one is funny, different time span)   
Can't figure out why not working!
```{r}
sims <- sprintf('%0.3d', 2:30)

snow1920_2005 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.B20TRC5CNBDRD.f09_g16.",sims,".cam.h0.SNOWHLND.192001-200512.nc", collapse="",sep=""))
})

#models have 86 years
snow+annual <- lapply(1:2, function(model){
  snow1 <- ncvar_get(snow1920_2005[[model]], "SNOWHLND")
  cl <- makeCluster(8)
  registerDoParallel(cl)
  snow <- foreach(timeindex = seq(1,1032,12),
                  .combine = "abind3") %dopar%
                  {
                    apply(snow1[,,(timeindex+11)], MARGIN = c(1,2), sum)
                  }
  stopCluster(cl)
  gc()
  rm(snow1)
  snow
  })


```



#sdsmR   
sdsmR - downscaling with Snow  
<https://www2.cisl.ucar.edu/sites/default/files/Richardson.pdf> 
```{r}
#install.packages("sdsmR")
library("devtools")
devtools::install_github('leerichardson/sdsmR')
library(sdsmR)
```
<https://github.com/leerichardson/sdsmR/blob/master/vignettes/blogsville-example.Rmd>
```{r}
head(blogsville)
quality_control(blogsville)


```

#clim.pact - climate analysis and empirical-statistical downscaling 
```{r}
install.packages("clim.pact")
```
#esd R package   

```{r}
library(RCurl)
url <- "ftp://ftp.met.no/users/rasmusb/" 
esdpack <- getURL(url, ftp.use.epsv = FALSE, dirlistonly = TRUE)

install.packages(esdpack)
install.packages("esd_0.5-5.tar.gz")

```

<http://eremrah.com/articles/How-to-extract-data-from-PRISM-raster/>
#before downscale, extract points from raster stack
```{r}
library(tidyr)
library(stringr)
library(raster)
library(magrittr)

options(prism.path = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/World BioClim Date/prism_ppt_1895_2016") 

# stack rasters
rasterstack <- ls_prism_data()[7:100,] %>% prism_stack(.)
rscrs <- rasterstack@crs@projargs
```

```{r}
#Poa secunda
pose <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Alpine_Phenology_2/Species_SEINet/Poa secunda_georeferencing.csv")

posell <- pose[,c("scientificName","year","decimalLatitude","decimalLongitude","minimumElevationInMeters","startDayOfYear")]

ggplot(posell[posell$year>1800,], aes( year,startDayOfYear))+
  geom_point()

library(RColorBrewer)
    cols <- rev(brewer.pal(11, 'RdYlBu'))

ggplot(posell[posell$year>1800,])+
  geom_point(aes(year,startDayOfYear, colour=minimumElevationInMeters))+
  scale_colour_gradientn(colours = cols)+
  ggtitle("Poa secunda")


summary(lm(startDayOfYear~minimumElevationInMeters*year, data=posell))
table(posell$year)

ggplot(posell[posell$year>1949,])+
  geom_point(aes(year,minimumElevationInMeters, colour=startDayOfYear))+
  scale_colour_gradientn(colours = cols)

summary(lm(startDayOfYear~minimumElevationInMeters*year, data=posell[posell$year>1949,]))

summary(lm(minimumElevationInMeters~startDayOfYear*year, data=posell[posell$year>1949,]))

summary(lm(minimumElevationInMeters~year, data=posell[posell$year>1949,]))

```

```{r}
#Hieracium triste
hitr <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Alpine_Phenology_2/Species_SEINet/Hierarcium triste.csv")

hitrll <- hitr[,c("scientificName","year","decimalLatitude","decimalLongitude","minimumElevationInMeters","startDayOfYear")]

hitrll <- hitrll[hitrll$startDayOfYear>10,]

ggplot(hitrll[hitrll$year>1800,], aes( year,startDayOfYear))+
  geom_point()

library(RColorBrewer)
    cols <- rev(brewer.pal(11, 'RdYlBu'))

ggplot(hitrll[hitrll$year>1800,])+
  geom_point(aes(year,startDayOfYear, colour=minimumElevationInMeters))+
  scale_colour_gradientn(colours = cols)


summary(lm(startDayOfYear~minimumElevationInMeters*year, data=hitrll))
table(hitrll$year)

ggplot(hitrll[hitrll$year>1949,])+
  geom_point(aes(year,minimumElevationInMeters, colour=startDayOfYear))+
  scale_colour_gradientn(colours = cols)+
  ggtitle("Hieracium triste")

summary(lm(startDayOfYear~minimumElevationInMeters*year, data=hitrll[hitrll$year>1949,]))

summary(lm(minimumElevationInMeters~startDayOfYear*year, data=hitrll[hitrll$year>1949,]))

summary(lm(minimumElevationInMeters~year, data=hitrll[hitrll$year>1949,]))
```




Downscaled PRISM layer  
```{r}
options(prism.path = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/PRISM_layers")

x <- seq(-109,-102,.005) # 3 decimal places == 111m at equator (555 meters)
y <- seq(37,41,0.005)

length(x)*length(y)
cogridpts <- expand.grid(x,y)

# 1971-2000 period
#get_prism_normals(type="ppt",resolution = "800m", mon=1:12, annual=FALSE)

ppts <- grep("ppt", ls_prism_data(absPath=TRUE)[,2])
latlongs <- read.delim(path.expand("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_QGIS Projects/Alpine Phenology Project/Sampling Points/LatLongabove3200m.txt"), sep=",")


ppt.co <- lapply(ppts, function(x){
  rastertemps <- raster(ls_prism_data(absPath=TRUE)[x,2])
  date <- ls_prism_data()[x,]
  data.frame(data = extract(rastertemps, latlongs[,c(4,3)]), date)
  })
ppt.co[[4]] #april
ppt.co.months <- do.call(rbind, ppt.co)
ppt.ann.avg <- aggregate(data~date, data=ppt.co.months, mean)
sum(ppt.ann.avg$data) #933.6855mm/year averaged over points and then all months summed
sum(ppt.ann.avg$data)*(10/365.25) #25.56292 cm/day averaged over points and then all months summed

sum(ppt.ann.avg$data)*(10/365.25)/100 #0.2556292 cm/day averaged over points and then all months summed


ppt.ann.sd <- aggregate(data~date, data=ppt.co.months, sd)
sum(ppt.ann.sd$data) #276.9792mm/year SD over points and then all months summed


#Extract from model times model25.raster; modelavg.raster
model25.raster[[1]] # average and variance over 2006:2030
model25.raster[[2]] # average and variance over 2031:2055

ppt.2006_2030 <- data.frame(data= extract(rotate(model25.raster[[1]][[1]]), 
                                          latlongs[,c(4,3)]), 
                            date = "Avg2006:2030")
ppt.2006_2030$data <- ppt.2006_2030$data*3600*24/10
table(ppt.2006_2030$data) #cm/day
mean(ppt.2006_2030$data) #0.002472653 cm/day


ppt.2006_2080 <- lapply(model25.raster, function(rast){
  avg <- extract(rotate(rast[[1]]), latlongs[,c(4,3)])
  sddev <- extract(rotate(rast[[2]]), latlongs[,c(4,3)])
  cbind(avg*3600*24/10,sddev*3600*24/10)
})


head(ppt.2006_2080[[1]]) #
head(ppt.2006_2030) #Yay, same!!!

#1271 points per model
oh6oh8 <- data.frame(ppt.2006_2080[[1]])
oh855 <- data.frame(ppt.2006_2080[[2]])
oh5580 <- data.frame(ppt.2006_2080[[3]])
three2006_2080 <- rbind(oh6oh8,oh855,oh5580)

#three2006_2080 <- do.call(rbind,ppt.2006_2080)
names(three2006_2080) <- c("avg","sddev")
three2006_2080$model <- c(rep("M06-30",1271),
                          rep("M31-55",1271),
                          rep("M56-80",1271))


three2006_2080$timeframe <- c(rep(1,1271),
                          rep(2,1271),
                          rep(3,1271))


#Plot values
ggplot(three2006_2080, aes(timeframe, avg, colour = model))+
  geom_errorbar(aes(ymax = avg+sddev*100000, ymin= avg-sddev*100000))+
  geom_point()


avgoverpoints <- aggregate(cbind(avg,sddev)~timeframe+model, data= three2006_2080, mean)

ggplot(avgoverpoints, aes(timeframe, avg,colour=model))+
  geom_errorbar(aes(ymax = avg+sddev*100000, ymin= avg-sddev*100000, width = 0.3))+
  geom_point() +
  theme_bw()+
  xlab("25 year average")+
  ylab("Annual precipitation cm/day")
```
030.cam.h0.TSMN.200601-208012
Now Temperature over time periods
```{r}
sims <- sprintf('%0.3d', 1:30)

TSMN2006_2080 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.TSMN.200601-208012.nc", collapse="", sep=""))
})

#Sum 12 months for annual 
TSMN_annual <- lapply(1:30, function(model){
  precc1 <- ncvar_get(TSMN2006_2080[[model]], "TSMN")
  cl <- makeCluster(6)
  registerDoParallel(cl)
  precc <- foreach(timeindex = seq(1,900,12),
                   .combine = "abind3") %dopar% 
    {
      apply(precc1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
      }
  stopCluster(cl)
  gc()
  rm(precc1)
  precc
  })


# Avreage and variance over the 30 models 
#Temp Surface Minimum
cl <- makeCluster(6)
registerDoParallel(cl)
TSMNmodelavg_annual <- foreach(timeindex = 1:75,
                           .packages = "abind") %dopar%
  {
    byyearmodel <- lapply(TSMN_annual, function(model1){
      model1[,,timeindex]
    })
    oneyear <- abind(byyearmodel, along=3)
    abind(apply(oneyear, MARGIN = c(1,2), mean),
          apply(oneyear, MARGIN = c(1,2), var), along=3)
  }
stopCluster(cl)

save(list="TSMNmodelavg_annual", file="P:/hackathon/Simulations/TSMNmodelavg_annual.RData")
class(TSMNmodelavg_annual[[1]])
dim(TSMNmodelavg_annual[[1]]) #288 192 2

#make rasters
TSMNmodelavg.raster <- lapply(TSMNmodelavg_annual, function(model1){
  e <- extent(c(0,360,-90,90))
  mean.raster <- flip(raster(t(as.matrix(model1[,,1], nrow=192))),
                      direction = "y")
  extent(mean.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  var.raster <- flip(raster(t(as.matrix(model1[,,2], nrow=192))),
                      direction = "y")
  extent(var.raster) <- e
  proj4string(var.raster) <- CRS("+init=epsg:4326")
  list(mean.raster,var.raster)
  })

years <- c(2006:2080)
type <- c("avg","var")

save(list="TSMNmodelavg.raster", file="P:/hackathon/Simulations/TSMNmodelavg.raster.RData")
#write rasters
for(i in 1:75){
  for(j in 1:2){
    writeRaster(rotate(TSMNmodelavg.raster[[i]][[j]]), 
                paste("P:/hackathon/Simulations/TSMN",type[j],years[i],collapse="",sep=""),
                format = "GTiff")
  }
}

#Average and variance over 25 year periods  2006:2030, 2031:2055, and 2056:2080
TSMNmodel25_annual <- lapply(seq(1,75,25), function(year){
  model1 <- abind(TSMNmodelavg_annual[year+24], along=3)
  mn <- apply(model1[,,1], MARGIN = c(1,2), mean)
  vr <- apply(model1[,,2], MARGIN = c(1,2), mean)
  list(mn,vr)
})



save(list="TSMNmodel25_annual", file="P:/hackathon/Simulations/TSMNmodel25_annual.RData")


TSMNmodel25.raster <- lapply(TSMNmodel25_annual, function(model1){
  e <- extent(c(0,360,-90,90))
  mean.raster <- flip(raster(t(as.matrix(model1[[1]], nrow=192))),
                      direction="y")
  extent(mean.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  var.raster <- flip(raster(t(as.matrix(model1[[2]], nrow=192))),
                      direction = "y")
  extent(var.raster) <- e
  proj4string(mean.raster) <- CRS("+init=epsg:4326")
  list(mean.raster,var.raster)
  })

save(list="TSMNmodel25.raster", file="P:/hackathon/Simulations/TSMNmodel25_raster.RData")


yrs <- c("2006_2030","2031_2055","2056_2080")
#write rasters
for(k in 1:3){
  for(j in 1:2){
    writeRaster(rotate(TSMNmodel25.raster[[k]][[j]]),
                paste("P:/hackathon/Simulations/TSMN",type[j],yrs[k],collapse="",sep=""),
                format = "GTiff")
    
  }
}



#Extract from model times model25.raster; modelavg.raster
TSMNmodel25.raster[[1]] # average and variance over 2006:2030
TSMNmodel25.raster[[2]] # average and variance over 2031:2055

TSMNppt.2006_2030 <- data.frame(data= extract(rotate(TSMNmodel25.raster[[1]][[1]]), 
                                          latlongs[,c(4,3)]), 
                            date = "Avg2006:2030")


TSMNppt.2006_2080 <- lapply(TSMNmodel25.raster, function(rast){
  avg <- extract(rotate(rast[[1]]), latlongs[,c(4,3)])
  sddev <- extract(rotate(rast[[2]]), latlongs[,c(4,3)])
  cbind(avg,sddev)
})


head(TSMNppt.2006_2080[[1]]) #
table(TSMNppt.2006_2080[[1]][,1])
head(TSMNppt.2006_2030) #Yay, same!!!

#1271 points per model
TSMNoh6oh8 <- data.frame(TSMNppt.2006_2080[[1]])
TSMNoh855 <- data.frame(TSMNppt.2006_2080[[2]])
TSMNoh5580 <- data.frame(TSMNppt.2006_2080[[3]])
TSMNthree2006_2080 <- rbind(TSMNoh6oh8,TSMNoh855,TSMNoh5580)

#three2006_2080 <- do.call(rbind,ppt.2006_2080)
names(TSMNthree2006_2080) <- c("avg","sddev")
TSMNthree2006_2080$model <- c(rep("M06-30",1271),
                          rep("M31-55",1271),
                          rep("M56-80",1271))


TSMNthree2006_2080$timeframe <- c(rep(1,1271),
                          rep(2,1271),
                          rep(3,1271))


#Plot values
ggplot(TSMNthree2006_2080, aes(timeframe, avg, colour = model))+
  geom_errorbar(aes(ymax = avg+sddev*100000, ymin= avg-sddev*100000))+
  geom_point()


TSMNavgoverpoints <- aggregate(cbind(avg,sddev)~timeframe+model, data= TSMNthree2006_2080, mean)

ggplot(TSMNavgoverpoints, aes(timeframe, avg,colour=model))+
  geom_errorbar(aes(ymax = avg+sddev*100000, ymin= avg-sddev*100000, width = 0.3))+
  geom_point() +
  theme_bw()+
  xlab("25 year average")+
  ylab("Annual precipitation cm/day")


```




#### START HERE ############
###########################################################################        
If previously loaded run from here         
###########################################################################






```{r}
data(wrld_simpl)
plot(rotate(model25.raster[[3]][[1]]))
plot(wrld_simpl, add=TRUE)
```


#Clip 25 year CC and CL averages to Colorado, or west at least
```{r}
us <- getData("GADM", country="USA", level=2)
us1 <- getData("GADM", country="USA", level=1)
westernUS1 <- us1[us1$NAME_1 %in% c("Utah","Wyoming","Nebraska",
                                  "Kansas","New Mexico","Oklahoma"),]
colo <- us[us$NAME_1 == "Colorado",]

west <- bind(westernUS1,colo)

```

Clip to west
```{r}
#take raster in 0-360 to -180-180
rot.25_2006 <- rotate(model25.raster[[1]][[1]])
rot.25_2031 <- rotate(model25.raster[[2]][[1]])
rot.25_2056 <- rotate(model25.raster[[3]][[1]])

#Crop and mask
prec25_2006 <- crop(rot.25_2006, extent(west))
prec25_2031 <- crop(rot.25_2031, extent(west))
prec25_2056 <- crop(rot.25_2056, extent(west))
```

Extract points from species for these models  
```{r}


```



ctrl+shift+enter
```{r}
plot(prec25_2006)
plot(west, add=TRUE)
```

```{r}
plot(prec25_2056)
plot(west, add=TRUE)
```
This when I lose a block of data... maybe they're all there (from before?) so it's fine?
```{r}
mapTheme <- rasterTheme(region = (brewer.pal(10, "RdBu")))
cutpts <- seq(range(getValues(prec25_2006))[1],
              range(getValues(prec25_2006))[2], by = 0.00000001)
plt <- levelplot(prec25_2056, margin = F, 
                 at=cutpts, cuts=11, pretty=TRUE, par.settings = mapTheme,
                 main="Average precipitation 2006 - 2030")
plt + layer(sp.lines(west, col = "black", lwd = 0.5))

```

```{r}
mapTheme <- rasterTheme(region = (brewer.pal(10, "RdBu")))
cutpts <- seq(range(getValues(prec25_2056))[1],
              range(getValues(prec25_2056))[2], by = 0.00000001)
plt <- levelplot(prec25_2056, margin = F, 
                 at=cutpts, cuts=11, pretty=TRUE, par.settings = mapTheme,
                 main="Average precipitation 2056 - 2080")
plt + layer(sp.lines(west, col = "black", lwd = 0.5))

```



# Precipitation 2081:2100
```{r}
sims <- sprintf('%0.3d', 1:30)

precc2081_2100 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECC.208101-210012.nc", collapse="", sep=""))
})

precl2081_2100 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECL.208101-210012.nc", collapse="", sep=""))
})
```

#Sum 12 months CC and CL, will be 20 lists long     
Tried to create a netCDF from the combined data, need to run nc_open again on these to have them work.    
###Conductive
```{r}
precc_annual_2081 <- lapply(1:30, function(model){
  precc1 <- ncvar_get(precc2081_2100[[model]], "PRECC")
  cl <- makeCluster(6)
  registerDoParallel(cl)
  precc <- foreach(timeindex = seq(1,240,12), # 240? not 900?
                   .combine = "abind3") %dopar% 
    {
      apply(precc1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
      }
  stopCluster(cl)
  gc()
  rm(precc1)
  precc
  })


```

###Large
```{r}
precl_annual_2081 <- lapply(1:30, function(model){
  precl1 <- ncvar_get(precl2081_2100[[model]], "PRECL")
  cl <- makeCluster(6)
  registerDoParallel(cl)
  precl <- foreach(timeindex = seq(1,240,12), # 240? not 900?
                   .combine = "abind3") %dopar% 
    {
      apply(precl1[,,timeindex:(timeindex+11)], MARGIN = c(1,2), sum)
      }
  stopCluster(cl)
  gc()
  rm(precl1)
  precl
  })


```

Sum CC and CL for 2081:2100
```{r}
prectotal_annual_2081 <- lapply(1:30, function(model){
  Ycc <- precc_annual_2081[[model]]
  Ycl <- precl_annual_2081[[model]]
  cl <- makeCluster(6)
  registerDoParallel(cl)
  total <- foreach(timeindex = 1:20,
                   .combine = "abind3") %dopar%
                   {
                     Ycc[,,timeindex]+Ycl[,,timeindex]
                   }
  stopCluster(cl)
  gc()
  rm(Ycc,Ycl)
  total
})


```

Average and variance over the 30 models, annual
```{r}
cl <- makeCluster(6)
registerDoParallel(cl)
modelavg_annual_2081 <- foreach(timeindex = 1:20,
                           .packages = "abind") %dopar%
  {
    byyearmodel <- lapply(prectotal_annual_2081, function(model1){
      model1[,,timeindex]
    })
    oneyear <- abind(byyearmodel, along=3)
    abind(apply(oneyear, MARGIN = c(1,2), mean),
          apply(oneyear, MARGIN = c(1,2), var), along=3)
  }
stopCluster(cl)

save(list="modelavg_annual_2081", file="P:/hackathon/Simulations/modelavg_annual2081.RData")


```


Average of average and variance over the 20 year period      
Maybe extent should be different?   - no, the netCDF is 192, 288 as well, yarg!
class       : RasterLayer        
dimensions  : 192, 288, 55296  (nrow, ncol, ncell)        
resolution  : 1.25, 0.9375  (x, y)         
extent      : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)        
```{r}
model20_annual <- list(apply(abind(modelavg_annual_2081, along=3)[,,1], MARGIN=c(1,2), mean),
                       apply(abind(modelavg_annual_2081, along=3)[,,2], MARGIN=c(1,2), mean))

e <- extent(c(0,360,-90,90))
mod1 <- flip(raster(t(as.matrix(model20_annual[[1]], nrow=192))),
                      direction="y")
extent(mod1) <- e
mod2 <- flip(raster(t(as.matrix(model20_annual[[2]], nrow=192))),
            direction = "y")
extent(mod2) <- e

model20.raster <- list(mod1,mod2)
  
save(list="model20_annual", file="P:/hackathon/Simulations/model20_annual.RData")
```

225 values for precip   
resolution (res(model20.raster[[1]]) = 1.2500 0.9375) but shouldn't it be 0.94241?? that's the steps for lat. Lon seems right, maybe that's why I'm missing a few grid cell values!
```{r}
rot.20_2081 <- rotate(model20.raster[[1]])
prec20_2081 <- crop(rot.20_2081, extent(west))

```

```{r}
mapTheme <- rasterTheme(region = (brewer.pal(10, "RdBu")))
cutpts <- seq(range(getValues(prec20_2081))[1],
              range(getValues(prec20_2081))[2], by = 0.00000001)
plt <- levelplot(prec20_2081, margin = F,
                 at=cutpts, cuts=11, pretty=TRUE, par.settings = mapTheme,
                 main="Average precipitation 2081 - 2100")
plt + layer(sp.lines(west, col = "black", lwd = 0.5))

```


Past models    
```{r}
sims <- sprintf('%0.3d', 1:30)

#The first one is 1850 through 2005 while the rest 2:30 are 1920:2005

precc1920_2005 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.B20TRC5CNBDRD.f09_g16.",sims,".cam.h0.PRECL.185001-200512.nc", collapse="", sep=""))
})

precl1920_2005 <- lapply(sims, function(sims){
  nc_open(paste("P:/hackathon/Simulations/b.e11.B20TRC5CNBDRD.f09_g16.",sims,".cam.h0.PRECL.185001-200512.nc", collapse="", sep=""))
})


time <- ncvar_get(precl2006_2080[[1]], "date")
length(time)

```
















#ENMeval - builds a series of candidate models in MaxEnt with variety of settings and provides multiple evaluation metrics to select optimal model
```{r}
#install.packages("ENMeval")
#install.packages("measurements")


```

Only implements Maxent     
process to optimize model complexity to balance goodness-of-fit and predictive ability     
<https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html>
     
     
Read in Alpine phenology data
```{r}
colo <- read.csv("P:/hackathon/alpine-phenology/datasets/Complete COLO DB .csv") 
seinet <- read.csv(text = getURL("https://raw.githubusercontent.com/DenverBotanicGardens/alpine-phenology/master/datasets/CO_SEINet_3200_20160304.csv"))

colo$scientificName <- paste(colo$Genus,colo$Specific.Epithet,sep=" ")
colo$Year <- (sub('.*(\\d{4}).*', '\\1',
                  as.character(colo$Date..dd.month.yyyy.)))

seinet <- seinet[seinet$scientificName!="",]

colo <- colo[!is.na(colo$Decimal.Latitude),]
seinet <- seinet[!is.na(seinet$decimalLatitude),]

names(colo) # 40 scientificName, 19 Latitude, 20 Longitude
names(seinet) # 13 scientificName, 63 decimalLatitude, 64 decimalLongitude

cololl <- colo[,c(40,19,20)]
str(cololl)
cololl[,2:3] <- lapply(cololl[,2:3], function(x) as.character(x))

names <- names(cololl)
seinetll <- seinet[,c(13,63,64)]
names(seinetll) <- names
str(seinetll)
seinetll$scientificName <- as.character(seinetll$scientificName)

#already in decimal degrees
table(seinetll$Latitude)

table(cololl$Latitude)
# change the degree symbol to a space
cololl[,2:3] <- lapply(cololl[,2:3], function(x) gsub('°', ' ', x))

# change the ' to a space
cololl[,2:3] <- lapply(cololl[,2:3],  function(x) gsub("'", ' ',x))

# change the " to a space
cololl[,2:3] <- lapply(cololl[,2:3],  function(x) gsub("\"", ' ',x))

# get rid of letters
cololl[,2:3] <- lapply(cololl[,2:3],  function(x) gsub("[a-zA-Z]", "", x))

# get rid of punctuation characters
cololl[,2:3] <- lapply(cololl[,2:3],  function(x) gsub("[^[:alnum:]]", " ", x))

#trim leading trailing whitespace
cololl[,2:3] <- lapply(cololl[,2:3],  function(x) trimws(x))


table(cololl$Latitude)
# convert from decimal minutes to decimal degrees
cololl[,2:3] <- lapply(cololl[,2:3],  
                       function(x) conv_unit(x, from = "deg_min_sec",
                                             to = "dec_deg"))

cololl[,2:3] <- lapply(cololl[,2:3], function(x) as.numeric(x))

# Anything below 37 or greater than 41 is an error, remove
cololl <- cololl[cololl$Latitude > 37,]
cololl <- cololl[cololl$Latitude < 41,]

str(cololl)


samples <- rbind(cololl,seinetll)

head(samples)

llsamples <- samples[!is.na(samples$Latitude),]
llsamples <- llsamples[!is.na(llsamples$Longitude),]

head(llsamples)


llsamples[,2:3] <- lapply(llsamples[,2:3], function(x) as.numeric(x))


llsamples <- llsamples[abs(llsamples$Longitude) > 102,]
llsamples <- llsamples[abs(llsamples$Latitude) > 36,]

#round to three decimals which is around 147 to 125 meters
table(round(llsamples$Latitude,3))
length(unique(round(llsamples$Latitude,3))) #2049

length(unique(round(llsamples$Longitude,3))) #1940
length(unique(round(llsamples$Longitude,4))) #4131 - across CO seems like a better number... 

```
















#is it adding over the thrid dimension?
yes it is
```{r}

array.1 <- array(1:24, dim = c(3,4,2))
array.2 <- array(100:124, dim =c(3,4,2))


array.1[,,1]

apply(array.1, MARGIN = c(1,2), sum)
array.2[,,1]
array.1[,,1]+array.2[,,1]
array.1[,,2]+array.2[,,2]


cl <- makeCluster(6)
registerDoParallel(cl)
array_sum <- foreach(timeindex = 1:2,
                     .combine = "abind3") %dopar%
                     {
                       a1 <- array.1[,,timeindex]
                       a2 <- array.2[,,timeindex]
                       a1+a2
                     }
stopCluster(cl)

```



Clip to Colorado - less data? faster?
netCDF coordinates are center of grid, raster coordinates are cell corners 
are the values the same so data same? the lat and lon are same...

#1. create rasters
```{r}

precc2006_2080.raster <- lapply(sims, function(sims){
  raster(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECC.200601-208012.nc", collapse="", sep=""))
})

precl2006_2080.raster <- lapply(sims, function(sims){
  raster(paste("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.",sims,".cam.h0.PRECL.200601-208012.nc", collapse="", sep=""))
})


projection(precc2006_2080.raster[[1]]) #"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"

# raster coordinates - the center of cell
setdiff(xFromCol(precc2006_2080.raster[[1]]),
          ncvar_get(precc2006_2080[[1]], "lon"))

# 
setdiff(rev(yFromRow(precc2006_2080.raster[[1]])),
  -ncvar_get(precc2006_2080[[1]], "lat"))

cc <- ncvar_get(precc2006_2080[[1]], "PRECC")
max(cc[,,1])
min(cc[,,1])



```


#World2 is centered in the pacific ocean like the ncar data!!!
```{r}
# map the data
world.outlines <- map("world2", plot=FALSE)
world.outlines.sp <- map2SpatialLines(world.outlines, proj4string = CRS("+proj=longlat"))

mapTheme <- rasterTheme(region = rev(brewer.pal(10, "RdBu")))
cutpts <- seq(-.000000005,0.0000004, by = 0.00000001)
plt <- levelplot(precc2006_2080.raster[[8]], margin = F, 
                 at=cutpts, cuts=11, pretty=TRUE, par.settings = mapTheme,
                 main="test variable -- as raster layer")
plt + layer(sp.lines(world.outlines.sp, col = "black", lwd = 0.5))

```

#2. Clip rasters
world2MapEnv is based on lat [0,360]
```{r}
data("world2MapEnv")
us <- getData("GADM", country="USA", level=2)
colo <- us[us$NAME_1 == "Colorado",]

plot(colo)

class(colo)

# rotate raster that is in 0,360 to standard -180,180
cc1.rot <- rotate(precc2006_2080.raster[[1]])

#crop and mask
cc1 <- crop(cc1.rot, extent(colo))
# -109.375:-101.875 and 36.75393:41.46597

plot(cc1)

```


#my machine has 12 processors 
```{r}
cl <- makeCluster(6)
registerDoParallel(cl)

#use foreach for loop instead of for or lapply
#by default results are returned in a list
#objects from parent environment not available
cc_cl_30 <- lapply(1:30, function(model){
  Ycl <- ncvar_get(precl2006_2080[[model]], "PRECL")
  Ycc <- ncvar_get(precc2006_2080[[model]], "PRECC")
  cccl <- foreach(timeindex = 1:dim(time), 
                  .combine = "abind3",
                  .packages = "ncdf4") %dopar% {
                    cc <- Ycc[,,timeindex]
                    cl <- Ycl[,,timeindex]
                    cc+cl
                  }
  rm(Ycl,Ycc)
  gc()
  })

stopCluster(cl)

```

BRCP model from 2006 to 2080 
monthly data given as the first day of the following month
```{r}
#First model of conductive precipitation (snow and ice) small storms
precc1 <- nc_open("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.001.cam.h0.PRECC.200601-208012.nc")

dates <- ncvar_get(precc1, "date")
lon <- ncvar_get(precc1, "lon") # 0-360 so take -180 through 180... 


lon2 <- lon
lon2[lon2 > 180] <- lon2[lon2 > 180] - 360
which(lon2 > -109 & lon2 < -102) # 202:207

which(c(-180:0) == -109) #72 is the west edge of colorado - don't think this is right!
which(c(-180:0) == -102) #79 is the east edge of colorado - don't think this is right!

lat <- ncvar_get(precc1, "lat") # -90 to 90 36.9 to 41 is colorado

names(precc1$var)

#the half that's less than 180 should be -180 to 0
#lon[lon > 180] <- lon[lon > 180] - 360


precipCC <- ncvar_get(precc1, "PRECC")

precip_t1 <- precipCC[,,1]

library(maps)

# will do it if run in concole, not here
image((lon-180),lat,precip_t1)
map("world", c("USA"))

data(wrld_simpl)
plot(wrld_simpl, add=TRUE)

wrld_simpl$NAME


plot(wrld_simpl$REGION)
```


Ctrl+Shift+Enter
```{r}
#run this in the console, won't work here 
lon2 <- lon
lon2[lon2>180] <- lon2[lon2>180]-360
lon2 <- sort(lon2)
```

```{r}
#image.plot(lon-360, lat, precip_t1)
image.plot(lon, lat, precip_t1)
plot(wrld_simpl, add=TRUE)
```


```{r}
which(lon<=79 & lon>=72)
lon[59:64]
which(lat <= 41 & lat >= 37)
lat[136:140]

us <- getData("GADM", country="USA", level=2)
us.states <- us[us$NAME_1 %in% c("Colorado","Utah","Wyoming","Nebraska","Kansas","New Mexico","Oklahoma"),]

colo <- us[us$NAME_1 == "Colorado",]

```

Or do cntl+shift+enter to run whole chunch
```{r}
# have to copy and paste to console
image.plot(lon[202:207]-360, lat[136:140], precip_t1[202:207,136:140]) #is this right?
plot(us.states, add=TRUE)

```

```{r}

image.plot(lon[58:63]-180, lat[136:140], precip_t1[58:63,136:140])
plot(us.states, add=TRUE)

```

Add together the Large-scale (stable) precipitation rate (liq+ice) (PRECL) and the Convective precipitation rate (liq+ice) (PRECC)

```{r}
precl2080 <- nc_open("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.009.cam.h0.PRECL.208101-210012.nc")

Y <- ncvar_get(precl2080, "PRECL")
Y1 <- Y[60,137,] #all times, one point somewhere in CO

#normalize to cm/day
Y1 <- Y1 *3600*24/10

#threshold as the 0.99 quantile
tailProb <- 0.01

threshold <- quantile(Y1,1-tailProb)

head(data.frame(Y1, YrMo = 1:240))
ggplot(data.frame(Y1, YrMo = 1:240), aes(YrMo, Y1))+
  geom_line()
  
# rotate a raster object that has x coordinates longitude from 0 to 360, to standard coordinates between -180 and 180 degrees. Longitude between 0 and 360 is frequently used in global climate models.
raster.precc2080 <- raster("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.009.cam.h0.PRECL.208101-210012.nc", varname = "PRECL")
proj4string(raster.precc2080) = CRS("+init=EPSG:4326")
raster.precc2080 <- rotate(raster.precc2080)


precc2080 <- nc_open("P:/hackathon/Simulations/b.e11.BRCP85C5CNBDRD.f09_g16.009.cam.h0.PRECC.208101-210012.nc")

lon <- ncvar_get(precl2080, "lon")
dim(lon)

lat <- ncvar_get(precl2080, "lat")
dim(lat)

time <- ncvar_get(precl2080, "date")
dim(time)


    Ycl <- ncvar_get(precl2080, "PRECL")
    Ycc <- ncvar_get(precc2080, "PRECC")
```


doParallel to add together
This is per month
```{r}
#my machine has 12 processors 
cl <- makeCluster(12)
registerDoParallel(cl)

#use foreach for loop instead of for or lapply
# I think I want for each time period add the lat and lon? so .combine=array or something? 

cc_cl_mon <- foreach(timeindex = 1:dim(time),
                 .combine = "abind3") %dopar%
  {
    library(ncdf4)
    cc <- Ycc[,,timeindex]
    cl <- Ycl[,,timeindex]
    cc+cl
  }

stopCluster(cl)
```

```{r}
lon2 <- lon
lon2[lon2 > 180] <- lon2[lon2 > 180] - 360

#image.plot(sort(lon2), lat, cc_cl_mon[,,1])
#plot(us.states, add=TRUE)

image.plot(sort(lon2),lat,cc_cl_mon[c(145:288,1:144),,2])
plot(wrld_simpl, add=TRUE)
#plot(us.states, add=TRUE)
```


both little and big precip at time something
```{r}
image.plot(lon[58:63]-180, lat[136:140], cc_cl_mon[58:63,136:140,1])
plot(us.states, add=TRUE)
```

Should add together for a year for loop over every 12 months...
```{r}
#my machine has 12 processors 
cl <- makeCluster(12)
registerDoParallel(cl)

#use foreach for loop instead of for or lapply
# 20 years, want to add together the 12 months for each year
cc_cl_annual <- foreach(timeindex = seq(0,dim(time),by = 12)[-1],
                 .combine = "abind3") %dopar%
  {
    library(ncdf4)
    cc <- Ycc[,,(timeindex-11):timeindex]
    cl <- Ycl[,,(timeindex-11):timeindex]
    cc1 <- apply(cc, MARGIN = c(1,2), sum) #over rows and cols meaning the thrid dimension I guess
    cl1 <- apply(cl, MARGIN = c(1,2), sum)
    cc1+cl1
  }

stopCluster(cl)
```

```{r}

data("wrld_simpl")
image((lon-180),lat,cc_cl_annual[,,5])
plot(wrld_simpl, add=TRUE)
```

###START HERE###
Now find the extremes and mean and variation over a 20 or 30 year period and compare earlier to later
```{r}
#my machine has 12 processors 
cl <- makeCluster(12)
registerDoParallel(cl)

#use foreach for loop instead of for or lapply
# 20 years, want to add together the 12 months for each year
cc_cl_annual <- foreach(timeindex = seq(0,dim(time),by = 12)[-1],
                 .combine = "abind3") %dopar%
  {
    library(ncdf4)
    cc <- Ycc[,,(timeindex-11):timeindex]
    cl <- Ycl[,,(timeindex-11):timeindex]
    cc1 <- apply(cc, MARGIN = c(1,2), sum) #over rows and cols meaning the thrid dimension I guess
    cl1 <- apply(cl, MARGIN = c(1,2), sum)
    cc1+cl1
  }

stopCluster(cl)
```

```{r}

data("wrld_simpl")
image((lon-180),lat,cc_cl_annual[,,5])
plot(wrld_simpl, add=TRUE)
```

```{r}

 nc <- nc_open("/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/PRECC/b.e11.BRCP85C5CNBDRD.f09_g16.105.cam.h0.PRECC.200601-210012.nc")


dates <- ncvar_get(nc, "date") 
point1_prec <- ncvar_get(nc, "PRECC")
```

```{r}


nc1 <- nc_open("P:/hackathon/Simulations/precip.mon.mean.nc")

lon <- ncvar_get(nc1, "lon")
lat <- ncvar_get(nc1, "lat")
tm <- ncvar_get(nc1, "time")


lon <- ncvar_get(nc, "lon")

precip <- ncvar_get(nc1, "precip")
head(precip)
str(precip)

precip <- precip[,,200] # lat is the second dimension

min(precip, na.rm = T)
max(precip, na.rm = T)

data(wrld_simpl)

data("wrld_simpl")
image((lon-180),rev(lat),precip)
plot(wrld_simpl, add=TRUE)
```

create a vector 
latitude task? so for each longitude along a latitude? 
```{r}


outSummary[latindex,,]<- foreach(lonindex = 1:dim(lon),
                                 .combine=rbind) %dopar% {
                                   library(extRemes) # library that is needed in inner loop
                                   Y<- nc1[lonindex,]
                                   # convert to cm/day
                                   Y<- Y * 3600*24/10
                                   threshold<- quantile( Y, 1- tailProb)
                                   frac<- sum(Y > threshold) / length( Y)
                                   GPFit<- fevd( Y, threshold=threshold, type="GP",
                                                 method="MLE")
                                   ReturnLevel<- return.level( GPFit, returnLevelYear,
                                                               do.ci=TRUE)
                                   #last line is the returned vector -- this is weird to me!
                                   c( threshold,GPFit$results$par,frac = frac,ReturnLevel )
                                   }

```


```{r}
install.packages("extRemes")
library(extRemes)
```

```{r}
#load("CMIPExample.rda")

load("C:/Users/deprengm/Documents/R/win-library/3.3/extRemes/data/Flood.rda")
ls()

dim(Flood)
Flood[1,1]
head(Flood)
```





#parallel for loop - convert a serial loop in R code to a parallel loop
1. load libraries you need
```{r}
library(ncdf4)
library(fields)
library(extRemes)
library(parallel)
library(doParallel)
library(foreach)

library(raster) #?

```

2. read data
```{r}
dataHandle <- nc_open(dataFileName) # location of data file relative to working directory
lon<- ncvar_get(dataHandle, "lon")
lat<- ncvar_get(dataHandle,"lat")
tm<- ncvar_get(dataHandle,"time")
nc_close(dataHandle)

summary.ncdf(dataHandle)

```

3. set up cluster
```{r}
c1 <- makePSOCKcluster(numCores) # create cluster , define cores?
registerDoParallel(cl)


```

4. parallel loop
```{r}


```

in case I want to use this <https://github.com/tidyverse/magrittr>

# rvest - to get point data from <http://www.climatewna.com/>    

Calculate
table:nth-child(1) input
//*[(@id = "Button1")] #the XPath for the Calculate button. 

```{r}
climatewna <- read_html("http://www.climatewna.com/")

wna.session <- html_session("http://www.climatewna.com/")

wna.form <- html_form(read_html(wna.session))

# values for dropdown of historical
hists <- climatewna %>% html_nodes("#DropD_hist option") %>% html_attr("value")
# values for dropdown of future climate models
futs <- climatewna %>% html_nodes("#DropD_gcm option") %>% html_attr("value")

# lapply or function or loop of some sort for all lat/longs -> lat_n, lon_n

filled_form <- set_values(wna.form[[1]],
                          txt_lat = lat_n,
                          txt_lng = lon_n,
                          DropD_hist = "")

climatewna %>% html_nodes(xpath = '//*[(@id = "Button1")]') %>% html_attr("value")

submit_form(wna.session, )


lat <- html_nodes(climatewna, "#txt_lat")
elev <- html_nodes(climatewna, "tr:nth-child(5) td , #dl_ovl_tif , #txt_lng") #maybe just get from download?

movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast span.itemprop")
html_text(cast)
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")

climatewna

html_nodes(climatewna, "table td")



html_text(climatewna)

html_text(climatewna, "#txt_lat")
html_attrs(climatewna)



#Historical
#dl_ovl_tif , #txt_lng , #txt_el

#Future
td:nth-child(2) , #dl_ovl_tif , #txt_lng , #txt_el


#download overlay raster files
#dl_ovl_tif , tr:nth-child(1) td , #txt_lng , #txt_el
//*[(@id = "dl_ovl_tif")] | //*[(@id = "Button1")] #xpath 


submit <- Filter(function(x) identical(x$type, "Calculate"))
```



