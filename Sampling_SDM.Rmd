---
title: "Sampling impacts on SDM"
author: "Michelle DePrenger-Levin"
date: "2/15/2020"
output: html_document
---

```{r}
save.image(file = "C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/maxent_sampling.Rda")

```


```{r}
# install.packages("installr")
# require(installr)
# updateR()

rm(list=ls())

# install.packages("maxnet")

# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jre1.8.0_241/lib/") # for 64-bit version
# install.packages("rJava")
# library(rJava)

library(diagram)
library(ggplot2)
# library(ggpubr)
library(gridExtra)

require(DiagrammeR)

# simmulations
library(virtualspecies)

# SDM
library(rJava)
library(maxnet)
library(dismo)
# library(MIAmaxent)
# install.packages("devtools")
library(devtools)
# install.packages("ecospat")
# install_github("danlwarren/ENMTools")
# library(ENMTools)

# Mapping
library(rgeos)
library(raster)
library(maptools)
library(dismo)
library(RNRCS)
library(rgdal)


# Parallelization
library(foreach)
library(parallel)
library(doParallel)
library(magrittr) # pipe %>%


thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}
```

Systematic should either be removed or treated as spatially unbiased while simple random is both spatially and environmentally unbiased because environemtnal variables are spatially autocorrelated. 
Systematic across a small area to mimic plots

      tab8 -> tab3;
      
      {rank = same ; tab8; tab3;}

DiagrammeR 
```{r}
grViz("digraph flowchart {

      graph [rankdir = TB, fontsize = 14]
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = box]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']
      tab8 [label = '@@8-1']
      tab9 [label = '@@8-2']
      tab10 [label = '@@8-3']
      tab11 [label = '@@8-4']
      tab12 [label = <Bio<SUB>1</SUB>>]
      tab13 [label = <Bio<SUB>11</SUB>>]
      tab14 [label = <Bio<SUB>12</SUB>>]
      tab15 [label = <Bio<SUB>19</SUB>>]
      tab16 [label = <dummy>]
     
      # edge definitions with the node IDs
      tab1 -> tab3;
      tab4 -> tab5;
      tab2 -> tab3;
      tab5 -> tab6;
      tab3 -> tab8;
      tab3 -> tab9;
      tab3 -> tab10;
      tab3 -> tab11;
      tab7 -> tab2;
      tab12 -> tab2;
      tab13 -> tab2;
      tab14 -> tab2;
      tab15 -> tab2;
      tab16 -> tab2;
      tab8 -> tab4;
      tab9 -> tab4;
      tab10 -> tab4;
      tab11 -> tab4;
      }

      [1]: 'Model organism (Claytonia rubra)'
      [2]: 'Climate Drivers'
      [3]: 'Build true distribution (SDM)'
      [4]: 'sample'
      [5]: 'Build SDM'
      [6]: 'Compare with true'
      [7]: 'Elevation'
      [8]: c('Systematic','Adaptive cluster','Convenience','Simple random')
      ")


```


Bayesian model to treat amount of the niche sampled as a latent vairable  
```{r}

grViz("digraph flowchart {

        
      graph [layout = dot, overlap = FALSE]
        
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = plain, rankdir = LR]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab4 [label = '@@4-1']
      tab5 [label = '@@4-2']
      tab6 [label = '@@4-3']
      tab7 [label = '@@4-4']
      tab8 [label = '@@4-5']
       node [label = '&alpha;&#x2080;&beta;&#x2080;']
      tab9 
       node [label = '&alpha;&#x2081;&beta;&#x2081;']
      tab10
       node [label = '&alpha;&#x2084;&beta;&#x2082;']
      tab11 
       node [label = '&alpha;&#x2082;&beta;&#x2083;']
      tab12 
       node [label = '&alpha;&#x2083;&beta;&#x2084;']
      tab13
       node [label = '&sigma;&#x00B2;']
      tab14
      tab3 [label = '@@3']
      tab15 [label = '@@5']
      
      # edge
      edge[ dir = forw];
      tab3 -> tab9 [style=dotted]; {rank = same tab1 tab3};
      tab3 -> tab10 [style=dotted];
      tab3 -> tab12 [style=dotted];
      tab3 -> tab13 [style=dotted];
      tab3 -> tab11 [style=dotted];
      

      
      # edge definitions with the node IDs
      edge[ dir = back];
      tab1 -> tab2; 
      tab2 -> tab15;
      tab15 -> tab4;
      tab15 -> tab5;
      tab15 -> tab6;
      tab15 -> tab7;
      tab15 -> tab8;
      tab4 -> tab9;
      tab5 -> tab10;
      tab6 -> tab12;
      tab7 -> tab13;
      tab8 -> tab11;
      tab2 -> tab14;{rank = same tab11 tab14};
      }

      [1]: '(Data) &gamma;&#x2081;'
      [2]: 'SDM (Pattern) &eta;'
      [3]: 'Sampling design'
      [4]: c('Annual Precipition','Annual Temperature','Precip of coldest quarter','Mean temp of coldest quarter','Elevation')
      [5]: 'Variable contribution (Process)'
      ")
```


### Skip ###
Use maxent, and then refine the relatonships of environment and points  
Virtual species 
<http://borisleroy.com/files/virtualspecies-tutorial.html>
virtual species based on Claytonia rubra   

```{r}
clru <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Claytonia rubra_SEINet/occurrences.csv")

plot(colocounties)
points(clru$decimalLongitude, clru$decimalLatitude, col="blue")
# points(clar$decimalLongitude, clar$decimalLatitude)


clru <- clru[complete.cases(clru$decimalLatitude),]
clruspdf <- SpatialPointsDataFrame(coords = clru[,c("decimalLongitude","decimalLatitude")],
                                     data = clru,
                                     proj4string = CRS("+proj=longlat +datum=WGS84"))

```

```{r}
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")

colocounties <- spTransform(colocounties.UTM,CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
coElev_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/ElevationResampled_bioclim.tif")
coAspect_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/AspectResampled_bioclim.tif")
# COplus_ruggedInt50.tif
coRugged_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/RuggedResampled_bioclim.tif")
bio1_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif")
bio12_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif")
rasterstack <- stack(coElev_res,coAspect_res,coRugged_res,bio1_res,bio12_res)

# or use all worldclim data
# bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)
# save(bioclim.colorado, file = "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
load("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
bioclim.colorado

rasterstack1 <- stack(bioclim.colorado[[c(1,11,12,19)]]) 
coloElev_resam <- resample(coElev_res, bioclim.colorado[[1]], method = 'bilinear')
coloElev <- crop(coloElev_resam, extent(colocounties), snap="out")
bioclim.stack <- crop(rasterstack1,extent(colocounties), snap="out")
rasterstack2 <- stack(coloElev,bioclim.stack)


# dummy variable
sim_layer <- coloElev
sim_layer <- setValues(sim_layer, runif(length(coloElev), min = 0, max = 100))
names(sim_layer) <- "dummy"
plot(sim_layer)
rasterstack3 <- stack(coloElev,bioclim.stack, sim_layer)

# scale will center (subtract the layer mean) and scale by dividing the centered layer by SD
rasterstack3_z <- scale(rasterstack3)
plot(rasterstack3_z)

# Normalize raster layers to 0-1; linear transformations so do not change the shape of the data
# normalize
# X - min(X) /  max(X) - min(X)
# z-score; Standardized
# X - mean(X) / sd(X)

rasterstack_norm <- rasterstack3
# Normalize to 0-1; worked before and now doesn't
for(i in 1:6){
  rasterstack_norm[[i]] <- calc(rasterstack_norm[[i]], function(x){
    (x-min(getValues(rasterstack_norm[[i]])))/(max(getValues(rasterstack_norm[[i]]))-min(getValues(rasterstack_norm[[i]])))
  })
}

plot(rasterstack_norm[[1]], main="noramlized")
plot(rasterstack3[[1]], main="elevation as is")
plot(rasterstack3_z[[1]], main="scaled minus mean/sd")
plot(scale(rasterstack_norm[[1]]), main="scaled normalized")

# scale in raster::scale just give the z-score
names(rasterstack_norm) <- c("Elevation","Bio1","Bio11","Bio12","Bio19","dummy")

# for(i in 1:6){
#     writeRaster(rasterstack_norm[[i]], paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/rasterstack_normalized",i,".tif",sep=""), overwrite=TRUE)
# }



```



Wilson 2011 
None of these take into account spatially how close they are to each other in addition to how close the values are...
```{r}
# install.packages("LaplacesDemon")
library(LaplacesDemon)
# Kullback-Leibler Divergence 
intrinsicdisc <- do.call(rbind,apply(combn(1:6,2), 2, function(x){
  klds <- KLD(getValues(rasterstack_norm[[x[1]]]),getValues(rasterstack_norm[[x[2]]]))
  out <- data.frame(Compar = paste(names(rasterstack_norm[[x[1]]]),
                                   names(rasterstack_norm[[x[2]]]),
                                   sep="-"),
                                   IntrinsicDisc = klds$intrinsic.discrepancy)
  out
}))

# measure of the information gain in moving from a prior distribution to a posterior distribution; mutual informaiton content, divergence is the value reated by adding the two complementary measures. 
intrinsicdisc[order(intrinsicdisc$IntrinsicDisc),]

# Hellinger distance, between two random samples
# install.packages("statip")
library(statip)
HellingerDist <- do.call(rbind,apply(combn(1:6,2), 2, function(x){
  hell <- hellinger(getValues(rasterstack_norm[[x[1]]]),getValues(rasterstack_norm[[x[2]]]))
  out <- data.frame(Compar = paste(names(rasterstack_norm[[x[1]]]),
                                   names(rasterstack_norm[[x[2]]]),
                                   sep="-"),
                                   IHDist = hell)
  out
}))

# smallest distance between bio1 and bio11
HellingerDist[order(HellingerDist$IHDist),]

# Euclidean distance normalized 
EuclideanDist <- do.call(rbind,apply(combn(1:6,2), 2, function(x){
  Edist <- sqrt(sum(((getValues(rasterstack_norm[[x[1]]])/
                        sum(getValues(rasterstack_norm[[x[1]]])))-
                        (getValues(rasterstack_norm[[x[2]]])/
                           sum(getValues(rasterstack_norm[[x[2]]])))
                     )^2))
  out <- data.frame(Compar = paste(names(rasterstack_norm[[x[1]]]),
                                   names(rasterstack_norm[[x[2]]]),
                                   sep="-"),
                                   NormEucldist = Edist)
  out
}))

EuclideanDist[order(EuclideanDist$NormEucldist),]


# Correlation
Corr <- do.call(rbind,apply(combn(1:6,2), 2, function(x){
  Corr <- cor(getValues(rasterstack_norm[[x[1]]]),getValues(rasterstack_norm[[x[2]]]))
  out <- data.frame(Compar = paste(names(rasterstack_norm[[x[1]]]),
                                   names(rasterstack_norm[[x[2]]]),
                                   sep="-"),
                                   cor = Corr)
  out
}))

Corr[order(abs(Corr$cor)),]


```


     See Royle, J. A., R. B. Chandler, C. Yackulic, and J. D. Nichols. 2012. Likelihood analysis of species occurrence probability from presence-only data for modelling species distributions. Methods in Ecology and Evolution 3:545-554. 
e^Xlinearmodel/1+e^linear model so that it's [0-1] of the probability of y|x 
Create my own glm for comparison with maxent
```{r}
# actual covariates
predic_raster <- rasterstack_norm[[1]]
predic_raster_quad <- rasterstack_norm[[1]]
predic_raster_quad_noBio1 <- rasterstack_norm[[1]]
predic_raster_quad_noBio11 <- rasterstack_norm[[1]]

# linear predictor 
betas <- c(0.5, 1,2.5,-7,4,-5,0.005)
lm1 <- as.formula(~ betas[1] + getValues(rasterstack_norm[[1]])*betas[2] + getValues(rasterstack_norm[[2]])*betas[3] +
                    getValues(rasterstack_norm[[3]])*betas[4] + getValues(rasterstack_norm[[4]])*betas[5] +
                    getValues(rasterstack_norm[[5]])*betas[6] + getValues(rasterstack_norm[[6]])*betas[7])

# I should include exponentials to better match maxent?
b <- c(0.5, 1, 1,
          2.5, 2.5,
          -7, -7,
          4, -4,
          -5, 5,
          0.005, 0.005)
lm2 <- as.formula(~ b[1] + getValues(rasterstack_norm[[1]])*b[2] + b[3]*(getValues(rasterstack_norm[[1]])^2) + 
                    b[4]*getValues(rasterstack_norm[[2]]) + b[5]*(getValues(rasterstack_norm[[2]])^2) +
                    b[6]*getValues(rasterstack_norm[[3]]) + b[7]*(getValues(rasterstack_norm[[3]])^2) + 
                    b[8]*getValues(rasterstack_norm[[4]]) + b[9]*(getValues(rasterstack_norm[[4]])^2) +
                    b[10]*getValues(rasterstack_norm[[5]]) + b[11]*(getValues(rasterstack_norm[[5]])^2) + 
                    b[12]*getValues(rasterstack_norm[[6]]) + b[13]*(getValues(rasterstack_norm[[6]])^2))

# Bio1 and Bio11 are very similar. Remove Bio1
names(rasterstack_norm) # remove [[2]]
b_noBio1 <- c(0.5, 1, 1,
          NA, NA, # 2.5, 2.5,
          -7, -7,
          4, -4,
          -5, 5,
          0.005, 0.005)
lm2_nobio1 <- as.formula(~ b_noBio1[1] + getValues(rasterstack_norm[[1]])*b_noBio1[2] + b_noBio1[3]*(getValues(rasterstack_norm[[1]])^2) +
                    b_noBio1[6]*getValues(rasterstack_norm[[3]]) + b_noBio1[7]*(getValues(rasterstack_norm[[3]])^2) + 
                    b_noBio1[8]*getValues(rasterstack_norm[[4]]) + b_noBio1[9]*(getValues(rasterstack_norm[[4]])^2) +
                    b_noBio1[10]*getValues(rasterstack_norm[[5]]) + b_noBio1[11]*(getValues(rasterstack_norm[[5]])^2) + 
                    b_noBio1[12]*getValues(rasterstack_norm[[6]]) + b_noBio1[13]*(getValues(rasterstack_norm[[6]])^2))


# Bio1 and Bio11 are simlar, what if you removed Bio11 instead?
# remove [[3]]
b_noB11 <- c(0.5, 1, 1,
          2.5, 2.5,
          NA, NA,  # -7, -7,
          4, -4,
          -5, 5,
          0.005, 0.005)
lm2_nobio11 <- as.formula(~ b_noB11[1] + getValues(rasterstack_norm[[1]])*b_noB11[2] + b_noB11[3]*(getValues(rasterstack_norm[[1]])^2) + 
                    b_noB11[4]*getValues(rasterstack_norm[[2]]) + b_noB11[5]*(getValues(rasterstack_norm[[2]])^2) +
                    # b[6]*getValues(rasterstack_norm[[3]]) + b[7]*(getValues(rasterstack_norm[[3]])^2) + 
                    b_noB11[8]*getValues(rasterstack_norm[[4]]) + b_noB11[9]*(getValues(rasterstack_norm[[4]])^2) +
                    b_noB11[10]*getValues(rasterstack_norm[[5]]) + b_noB11[11]*(getValues(rasterstack_norm[[5]])^2) + 
                    b_noB11[12]*getValues(rasterstack_norm[[6]]) + b_noB11[13]*(getValues(rasterstack_norm[[6]])^2))

#  suitability data
# Occurrence probability
predic_raster <- setValues(predic_raster, as.vector(exp(as.formula(lm1[[2]]))/(1+exp(as.formula(lm1[[2]])))))

# with quadratic 
predic_raster_quad <- setValues(predic_raster_quad, as.vector(exp(as.formula(lm2[[2]]))/(1+exp(as.formula(lm2[[2]])))))

# with quadratic, no Bio1
predic_raster_quad_noBio1 <- setValues(predic_raster_quad_noBio1, as.vector(exp(as.formula(lm2_nobio1[[2]]))/(1+exp(as.formula(lm2_nobio1[[2]])))))

# with quadratic, no Bio11
predic_raster_quad_noBio11 <- setValues(predic_raster_quad_noBio11, as.vector(exp(as.formula(lm2_nobio11[[2]]))/(1+exp(as.formula(lm2_nobio11[[2]])))))



plot(predic_raster)
plot(predic_raster_quad)
plot(predic_raster_quad_noBio1)
plot(predic_raster_quad_noBio11)
```


```{r}
jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1a_quadsuitability.jpg",
     width=123, height=100,units='mm', res=300) 

plot(predic_raster_quad)
mtext("a)", side=3, line=0, adj=0)

dev.off()
```



```{r}

# # try it with quadratic
# predic_raster <- predic_raster_quad
# 
# # normalize response
# predic_raster <- calc(predic_raster, function(x){
#     (x-min(getValues(predic_raster)))/(max(getValues(predic_raster))-min(getValues(predic_raster)))
# })


# define the neg log-likelihood
lik <- function(parm){
    beta0 <- parm[1] # -1 intercept
    beta1 <- parm[2] # Elevation
    beta2 <- parm[3] # Elevation^2
    beta3 <- parm[4] # Bio1
    beta4 <- parm[5] # Bio1^2
    beta5 <- parm[6] # Bio11
    beta6 <- parm[7] # Bio11^2
    beta7 <- parm[8]   # Bio12
    beta8 <- parm[9]   # Bio12^2
    beta9 <- parm[10]  # Bio19
    beta10 <- parm[11] # Bio19^2
    beta11 <- parm[12] # dummy
    beta12 <- parm[13] # dummy^2
  gridpsi <- exp(beta0 + beta1*getValues(rasterstack_norm[[1]]) + beta2*getValues(rasterstack_norm[[1]])^2 + 
                   beta3*getValues(rasterstack_norm[[2]]) + beta4*(getValues(rasterstack_norm[[2]])^2) +
                   beta5*getValues(rasterstack_norm[[3]]) + beta6*(getValues(rasterstack_norm[[3]])^2) + 
                   beta7*getValues(rasterstack_norm[[4]]) + beta8*(getValues(rasterstack_norm[[4]])^2) +
                   beta9*getValues(rasterstack_norm[[5]]) + beta10*(getValues(rasterstack_norm[[5]])^2) + 
                   beta11*getValues(rasterstack_norm[[6]]) + beta12*(getValues(rasterstack_norm[[6]])^2))/
    (1 + exp(beta0 + beta1*getValues(rasterstack_norm[[1]]) + beta2*getValues(rasterstack_norm[[1]])^2 + 
                   beta3*getValues(rasterstack_norm[[2]]) + beta4*(getValues(rasterstack_norm[[2]])^2) +
                   beta5*getValues(rasterstack_norm[[3]]) + beta6*(getValues(rasterstack_norm[[3]])^2) + 
                   beta7*getValues(rasterstack_norm[[4]]) + beta8*(getValues(rasterstack_norm[[4]])^2) +
                   beta9*getValues(rasterstack_norm[[5]]) + beta10*(getValues(rasterstack_norm[[5]])^2) + 
                   beta11*getValues(rasterstack_norm[[6]]) + beta12*(getValues(rasterstack_norm[[6]])^2)))
  
  datapsi <- exp(beta0 + beta1*y_1[,1] + beta2*y_1[,1]^2 + 
                   beta3*y_1[,2] + beta4*(y_1[,2]^2) -
                   beta5*y_1[,3] - beta6*(y_1[,3]^2) + 
                   beta7*y_1[,4] + beta8*(y_1[,4]^2) -
                   beta9*y_1[,5] - beta10*(y_1[,5]^2) + 
                   beta11*y_1[,6] + beta12*(y_1[,6]^2))/
    (1 + exp(beta0 + beta1*y_1[,1] + beta2*y_1[,1]^2 + 
                   beta3*y_1[,2] + beta4*(y_1[,2]^2) -
                   beta5*y_1[,3] - beta6*(y_1[,3]^2) + 
                   beta7*y_1[,4] + beta8*(y_1[,4]^2) -
                   beta9*y_1[,5] - beta10*(y_1[,5]^2) + 
                   beta11*y_1[,6] + beta12*y_1[,6]^2))
  # -1 and the presence-only sample joint likelihood, the sum of probabilities of presences over the likelihood over the study space
  -1*sum(log(datapsi/(sum(gridpsi)))) # log of zero!!?? but shouldn't be zero in the sums. - Doesn't seem to be here the warning "NA/Inf replaced by maximum positive value"
}


# Maxent does regularization for each of the EV depending on the types allowed (product, hinge, threshold, linear...) This is the penalty factor
lregtable <- list(c(0,10,30,100), c(1,1,0.2,0.05))
y <- predic_raster_quad
y <- setValues(y, as.vector(rbinom(length(y), 1, getValues(predic_raster_quad))))
y_1 <- do.call(cbind,lapply(1:6, function(i){
  rasterstack_norm[[i]][y==1]
  }))
np <- nrow(y_1)
 classregularization <- sapply(1:6, function(n) {
      t <- lregtable
      approx(t[[1]], t[[2]], np, rule=2)$y
   }) / sqrt(np)
p <- length(y)
weights <- nrow(y_1)+(1-nrow(y_1))*100
lambda <- 10^(seq(4,0,length.out=200))*sum(classregularization)/length(classregularization)*sum(p)/sum(weights)

paramsout_sim <- list()
# loop through binomial process (Bernoulli) to get sample across entire study area
for(i in 1:100){
  y <- predic_raster_quad
  y <- setValues(y, as.vector(rbinom(length(y), 1, getValues(predic_raster_quad))))
  y_1 <- do.call(cbind,lapply(1:6, function(i){
    rasterstack_norm[[i]][y==1]
    }))
  out <-  nlm(lik, c(rep(0,13)), print.level = 2, hessian=TRUE)
  diff <- out$estimate-b
  paramsout_sim[[i]] <- diff
}

# save(paramsout_sim, file = "C:/Users/DePrengm/Desktop/Params_100.rda")
# save(paramsout_sim, file = "C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/paramsLogLikelihood.Rda")

for(i in 1:13){
  hist(do.call(rbind,paramsout_sim)[,i], breaks = 15, main = paste("Beta",0:12,sep="")[i])
}
```

Updated exponentials and sampling
```{r}
# try it with quadratic
predic_raster <- predic_raster_quad

# normalize response
predic_raster <- calc(predic_raster, function(x){
    (x-min(getValues(predic_raster)))/(max(getValues(predic_raster))-min(getValues(predic_raster)))
})


# define the neg log-likelihood
lik <- function(parm){
    beta0 <- parm[1] # -1 intercept
    beta1 <- parm[2] # Elevation
    beta2 <- parm[3] # Elevation^2
    beta3 <- parm[4] # Bio1
    beta4 <- parm[5] # Bio1^2
    beta5 <- parm[6] # Bio11
    beta6 <- parm[7] # Bio11^2
    beta7 <- parm[8]   # Bio12
    beta8 <- parm[9]   # Bio12^2
    beta9 <- parm[10]  # Bio19
    beta10 <- parm[11] # Bio19^2
    beta11 <- parm[12] # dummy
    beta12 <- parm[13] # dummy^2
  gridpsi <- exp(beta0 + beta1*getValues(rasterstack_norm[[1]]) + beta2*getValues(rasterstack_norm[[1]])^2 + 
                   beta3*getValues(rasterstack_norm[[2]]) + beta4*(getValues(rasterstack_norm[[2]])^2) +
                   beta5*getValues(rasterstack_norm[[3]]) + beta6*(getValues(rasterstack_norm[[3]])^2) + 
                   beta7*getValues(rasterstack_norm[[4]]) + beta8*(getValues(rasterstack_norm[[4]])^2) +
                   beta9*getValues(rasterstack_norm[[5]]) + beta10*(getValues(rasterstack_norm[[5]])^2) + 
                   beta11*getValues(rasterstack_norm[[6]]) + beta12*(getValues(rasterstack_norm[[6]])^2))/
    (1 + exp(beta0 + beta1*getValues(rasterstack_norm[[1]]) + beta2*getValues(rasterstack_norm[[1]])^2 + 
                   beta3*getValues(rasterstack_norm[[2]]) + beta4*(getValues(rasterstack_norm[[2]])^2) +
                   beta5*getValues(rasterstack_norm[[3]]) + beta6*(getValues(rasterstack_norm[[3]])^2) + 
                   beta7*getValues(rasterstack_norm[[4]]) + beta8*(getValues(rasterstack_norm[[4]])^2) +
                   beta9*getValues(rasterstack_norm[[5]]) + beta10*(getValues(rasterstack_norm[[5]])^2) + 
                   beta11*getValues(rasterstack_norm[[6]]) + beta12*(getValues(rasterstack_norm[[6]])^2)))
  
  datapsi <- exp(beta0 + beta1*y_1[,1] + beta2*y_1[,1]^2 + 
                   beta3*y_1[,2] + beta4*(y_1[,2]^2) +
                   beta5*y_1[,3] + beta6*(y_1[,3]^2) + 
                   beta7*y_1[,4] + beta8*(y_1[,4]^2) +
                   beta9*y_1[,5] + beta10*(y_1[,5]^2) + 
                   beta11*y_1[,6] + beta12*(y_1[,6]^2))/
    (1 + exp(beta0 + beta1*y_1[,1] + beta2*y_1[,1]^2 + 
                   beta3*y_1[,2] + beta4*(y_1[,2]^2) +
                   beta5*y_1[,3] + beta6*(y_1[,3]^2) + 
                   beta7*y_1[,4] + beta8*(y_1[,4]^2) +
                   beta9*y_1[,5] + beta10*(y_1[,5]^2) + 
                   beta11*y_1[,6] + beta12*y_1[,6]^2))
  # -1 and the presence-only sample joint likelihood, the sum of probabilities of presences over the likelihood over the study space
  -1*sum(log(datapsi/(sum(gridpsi)))) # log of zero!!?? but shouldn't be zero in the sums. - Doesn't seem to be here the warning "NA/Inf replaced by maximum positive value"
}

paramsout_sim <- list()
# loop through binomial process (Bernoulli) to get sample across entire study area
for(i in 1:10){
  y <- predic_raster
  y <- setValues(y, as.vector(rbinom(length(y), 1, getValues(predic_raster))))
  y_1 <- do.call(cbind,lapply(1:6, function(i){
    rasterstack_norm[[i]][y==1]
    }))
  out <-  nlm(lik, c(rep(0,13)), print.level = 2, hessian=TRUE)
  diff <- out$estimate-b
  paramsout_sim[[i]] <- diff
}

# save(paramsout_sim, file = "C:/Users/DePrengm/Desktop/Params_100.rda")
# save(paramsout_sim, file = "C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/paramsLogLikelihood.Rda")

for(i in 1:13){
  hist(do.call(rbind,paramsout_sim)[,i], breaks = 15, main = paste("Beta",0:12,sep="")[i])
}

paramsout_sim_sampled <- list()
# loop through binomial process (Bernoulli) to get sample across entire study area
for(i in 1:100){
  y <- predic_raster
  y <- setValues(y, as.vector(rbinom(length(y), 1, getValues(predic_raster))))
  y_1 <- do.call(cbind,lapply(1:6, function(i){
    rasterstack_norm[[i]][y==1]
    }))
  y_1 <- y_1[base::sample(1:nrow(y_1),2000),]
  out <-  nlm(lik, c(rep(0,13)), print.level = 2, hessian=TRUE)
  diff <- out$estimate-b
  paramsout_sim_sampled[[i]] <- diff
}


for(i in 1:13){
  hist(do.call(rbind,paramsout_sim_sampled)[,i], breaks = 15, main = paste("Beta",0:12,sep="")[i])
}
```



```{r}
# generate presence-absence data
y <- predic_raster
y <- setValues(y, as.vector(rbinom(length(y), 1, getValues(predic_raster))))

# keep presence-only data
plot(y)
data <- lapply(1:6, function(x) rasterstack_norm[[x]][y==1])

# minimize it with nlm() non-linear minimization
out <- nlm(lik, c(rep(0,13)), hessian=TRUE)
out$estimate
b


# minimize with sampling
outsamp <- nlm(lik, c(rep(0,13)), hessian=TRUE)
outsamp$estimate
b

# nlm seems to cause the "NA/Inf replaced by maximum positive value" warnings
# <http://people.missouristate.edu/songfengzheng/Teaching/MTH541/MLE-R.pdf> says the warning does not matter. Why? 
paramsout <- lapply(1:10, function(x){
  out <- nlm(lik, c(rep(0,13)), hessian=TRUE)
  out$estimate
})

paste("Beta",0:12,sep="")
b
diffs <- lapply(paramsout, function(x) x-b)

for(i in 1:13){
  hist(do.call(rbind,diffs)[,i], breaks = 15)
}
hist(do.call(rbind,diffs)[,1], breaks = 15)


# difference from model fit and true value, historgram of differences - centered on zero non-biased. 

```




```{r}
jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1a_abioticvars.jpg",
     width=173, height=105,units='mm', res=300)
        par(mar=c(2.1,2.1,1.1,2.1))
        plot(predic_raster_quad)
        mtext("a)", side=3, line=0, adj=0)
 
dev.off()

jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1b_abioticvars.jpg",
     width=225, height=105,units='mm', res=300)

        plot(rasterstack_norm)
        mtext("b)", side=3, line=3, adj=-.10)

dev.off()
        

```


# Sampling functions
Sampling methods
```{r}
# test
# suitability.raster <- predic_raster_quad
# samplesize <- 20
# studyarea <- 150

### systematic 
systematic_sampling <- function(suitability.raster,  samplesize, studyarea = 100){
  # systematic sampling to allow for Bernoulli process to reject some cells with high suitability and sometimes take cells with low suitability
  sampleout <- c()
  studyareabox <- c()
  b <- 0
  while(any(is.null(sampleout), nrow(sampleout) < samplesize)){
    b <- b+1
      randcellRowCol <- c(ceiling(runif(1, min = 1, max = nrow(suitability.raster))),
                          ceiling(runif(1, min = 1, max = ncol(suitability.raster)))) # Row Col
      if(randcellRowCol[1] > (nrow(suitability.raster)-(studyarea))) randcellRowCol[1] <- (nrow(suitability.raster))-(studyarea+1)
      if(randcellRowCol[2] > (ncol(suitability.raster)-(studyarea))) randcellRowCol[2] <- (ncol(suitability.raster))-(studyarea+1)
      
      # Study area box
      studyareabox1 <- data.frame(do.call(rbind, 
                              list(xyFromCell(suitability.raster, 
                                 cellFromRowCol(suitability.raster, randcellRowCol[1],
                                                randcellRowCol[2])),
                            xyFromCell(suitability.raster, 
                                 cellFromRowCol(suitability.raster, randcellRowCol[1]+studyarea,
                                                randcellRowCol[2])),
                            xyFromCell(suitability.raster, 
                                 cellFromRowCol(suitability.raster, randcellRowCol[1],
                                                randcellRowCol[2]+studyarea)),
                            xyFromCell(suitability.raster, 
                                 cellFromRowCol(suitability.raster, randcellRowCol[1]+studyarea,
                                                randcellRowCol[2]+studyarea))
                            )), BoxNum = b)
      names(studyareabox1)[1:2] <- c("BoxX","BoxY")
      
      studyareabox <- rbind(studyareabox, studyareabox1)
    
      # systematically sample the study area, row by row within the study area col
      evenstep <- 10 # floor(studyarea/(sqrt(samplesize) ))*10
      # set the 'study area' as one side of square number of cells (i.e. study area of 50 x 50 cells so study area == 50)
      # Systematically sample and then use thinmax to get the samplesize
      # Find cell, move alnog row, move to cell of next selected row
      
      # r <- c <- cellFromRowCol(suitability.raster,randcellRowCol[1],randcellRowCol[2]) # cell number that matches the top left corner of the study area
      randcellRow <- randcellRowCol[1]
      while(all(randcellRow < c(randcellRowCol[1]+studyarea, nrow(suitability.raster)))){ # while still in a row that is in the study area
          # Start at the first column each time
          randcellCol <- randcellRowCol[2] 
          # sample by row
          while(all(randcellCol < c((randcellRowCol[2] + studyarea),ncol(suitability.raster)))){ # While in this row, still within a column of the study area length, assumes cells are numbered by row
            c <- cellFromRowCol(suitability.raster, randcellRow, randcellCol) 
            if(rbinom(1, 1, getValues(suitability.raster)[c]) == 1){
                sampleout <- rbind(sampleout,xyFromCell(suitability.raster, c))
              }
              randcellCol <- randcellCol + evenstep
          } # once get to the last column in the study area, end and move to the next row
        
        randcellRow <- randcellRow + evenstep
      }
  }
  
  sampleout <- thin.max(sampleout, c("x","y"), samplesize) # spatial thinning
  list(sampleout,studyareabox)
}


# suitability.raster <- clru1[[3]]
# rm(suitability.raster)
### random
random_sampling <- function(suitability.raster, samplesize){
  sampleout <- c()
  i <- 1
  while(i <= samplesize){
    randcell <- ceiling(runif(1, min = 1, max = length(suitability.raster)))
    
    if(rbinom(1, 1, getValues(suitability.raster)[randcell]) == 1){
      sampleout <- rbind(sampleout,xyFromCell(suitability.raster, randcell))
      i <- i+1
    } 
  }
  sampleout
}



### adaptive cluster sampling  
# function to find the surrounding cells
# check if on edge is it 1 or max row or col then select the remaining ones
surroundingcells <- function(cellRowCol, suitability.raster){
  out <- data.frame(Cells = c(cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]), # center cell; 1
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]+1), # row above; 4
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]+1), # either side, same row; 6
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]+1)), # next row; 9
                    Rows = c(cellRowCol[1], # center row
                          cellRowCol[1]-1,
                          cellRowCol[1]-1,
                          cellRowCol[1]-1, # row above
                          cellRowCol[1], 
                          cellRowCol[1],  # either side, same row
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1),
                    Cols = c(cellRowCol[2], # center col
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1, # row above
                          cellRowCol[2]-1,
                          cellRowCol[2]+1, # either side, same row
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1))
  if(cellRowCol[1]==1){
    out <- out[c(1,5:9),]
  }
  if(cellRowCol[2]==1){
    out <- out[c(1,3:4,6,8:9),]
  }
  if(cellRowCol[1]==nrow(suitability.raster)){
    out <- out[c(1,1:6),]
  }
  if(cellRowCol[2]==ncol(suitability.raster)){
    out <- out[c(1:3,5,7:8),]
  }
  out
}

# Test
# suitability.raster <- predic_raster_quad
# samplesize <- 20
# rm(suitability.raster);rm(samplesize)

adaptive_cluster <- function(suitability.raster, samplesize){
  n <- 0
  samples <- c()
  while(n <= samplesize){
    if(n == samplesize) break
    # Select a random cell (name row, col, and cell number)
    randcellRowCol <- c(ceiling(runif(1, min = 1, max = nrow(suitability.raster))),
                    ceiling(runif(1, min = 1, max = ncol(suitability.raster)))) # Row Col
    randcell <- cellFromRowCol(suitability.raster,randcellRowCol[1],randcellRowCol[2]) # cell number that matches
    # start a cluster
    # Bernoulli trial for setting a presence at the cell with probabiliity of suitability score to start a cluster
    if(rbinom(1,1,getValues(suitability.raster)[randcell]) == 1){
      n <- n+1
      samples <- rbind(samples, xyFromCell(suitability.raster, randcell))
      if(n == samplesize) break
      surrcells <- surroundingcells(randcellRowCol, suitability.raster)
      
      # check surrounding cells of a 'presence' and start this cluster
      j <- 2 # move down the table of cells, adding non-duplicated if cell occupied, first row is center of cluster, included to check for dupliates
      # Could hit sample size in the middle of checking all the surrounding cells
      while(j < nrow(surrcells)){
        if(rbinom(1,1,getValues(suitability.raster)[surrcells$Cells[j]]) == 1){
          samples <- rbind(samples, xyFromCell(suitability.raster, surrcells$Cells[j]))
          surrcells <- rbind(surrcells, surroundingcells(as.numeric(paste(surrcells[j,c(2:3)])),suitability.raster) )
          surrcells <- surrcells[!duplicated(surrcells$Cells),] # only add the surrounding cells not already included
          j <- j+1 # move down surrounding cells of original cluster 
          n <- n+1 # added to sample
          if(n == samplesize) break
          if(j == samplesize) break
        } else {# surrounding the surrounding cells, clusters
          j <- j+1 # last one not added, check the next row
          }
      }  # end while statement j is number of surrounding cells to be checked
    } # end if cell selected, when not occupied, select a new random cell
    
  } # end while sample size n <= samplesize so don't start a new cluster
  samples
}


### Convenience sampling, weighted by sampled areas
# 1. set each convenience raster cell to the proportion of SEINet herb collections per cell as convenience

# convenience_raster <- predic_raster
# # All samples from 1990, 2000, 2010 to represent samples over time over 21 years
# xy_1990 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_1990/occurrences.csv")
# xy_2000 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_2000/occurrences.csv")
# xy_2010 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_2010/occurrences.csv")
# xys <- do.call(rbind,lapply(list(xy_1990,xy_2000,xy_2010), function(xys){
#   i <- sapply(xys, is.factor)
#   xys[i] <- lapply(xys[i], as.character)
#   xys    
# }))
# 
# xys$decimalLatitude <- as.numeric(xys$decimalLatitude)
# xys$decimalLongitude <- as.numeric(xys$decimalLongitude)
# xys <- xys[!is.na(xys$decimalLongitude),c("decimalLongitude","decimalLatitude")]
# convenience_raster <- rasterize(xys, convenience_raster, fun='count')

## Interpolate <https://www.rdocumentation.org/packages/raster/versions/3.0-12/topics/interpolate> 
# library(fields)
# tps <- Tps(xyFromCell(convenience_raster, 1:ncell(convenience_raster)), getValues(convenience_raster))
# p <- raster(convenience_raster)
# p <- interpolate(p, tps)
# writeRaster(p, "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/convenience_raster_interpolated.tif", overwrite=TRUE)
p <- raster("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/convenience_raster_interpolated.tif")

p_norm <- calc(p, function(x){
    (x-min(getValues(p)))/(max(getValues(p))-min(getValues(p)))
})
# plot(p)
plot(p_norm)

# # contour lines for the convenience map
# cont <- rasterToContour(p_norm)
# plot(cont)
# plot(predic_raster)
# plot(cont, add=TRUE)


# 2. have rbinom selection of that cell probability based on convenience weights
# 3. follow the random selection 
convenience_sampling <- function(suitability.raster, convenience.raster, samplesize){
  sampleout <- c()
  i <- 1
  while(i <= samplesize){
    randcell <- ceiling(runif(1, min = 1, max = length(suitability.raster)))
      if(rbinom(1,1,getValues(convenience.raster)[randcell]) == 1){
        
        if(rbinom(1, 1, getValues(suitability.raster)[randcell]) == 1){
          sampleout <- rbind(sampleout,xyFromCell(suitability.raster, randcell))
          i <- i+1
        }
      }
    
  }
  sampleout
  
}

 

```

Sampling images 
```{r}

for(x in 1:10){
  samplesys1 <- systematic_sampling(predic_raster_quad, samplesize = 100, studyarea = 100)
      
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Fig2_NewSystematicTrue",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster_quad, main = ""); points(samplesys1[[1]], pch=16, cex=0.25)
  boxes <- split(samplesys1[[2]], samplesys1[[2]]$BoxNum)
  sur <- 0.1
  for(i in 1:length(boxes)){
    # x1, x2 and y1, y2
    lines(c(boxes[[i]]$BoxX[1], boxes[[i]]$BoxX[2]), c(boxes[[i]]$BoxY[1], boxes[[i]]$BoxY[2]), type='l', col="grey50")
    lines(c(boxes[[i]]$BoxX[1], boxes[[i]]$BoxX[3]), c(boxes[[i]]$BoxY[1], boxes[[i]]$BoxY[3]), type='l', col="grey50")
    lines(c(boxes[[i]]$BoxX[3], boxes[[i]]$BoxX[4]), c(boxes[[i]]$BoxY[3], boxes[[i]]$BoxY[4]), type='l', col="grey50")
    lines(c(boxes[[i]]$BoxX[2], boxes[[i]]$BoxX[4]), c(boxes[[i]]$BoxY[2], boxes[[i]]$BoxY[4]), type='l', col="grey50")
  }
  
  mtext("d)", side=3, line=0.5, adj=-0.1)
  mtext("Systematic sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
  }
```

```{r}

############ Simple random ###############

for(x in 1:10){
  random1 <- random_sampling(predic_raster_quad, 100)
  
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Fig2_simplerandom",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster_quad, main = ""); points(random1, pch=16, cex=0.25)
  mtext("a)", side=3, line=0.5, adj=-0.1)
  mtext("Simple random sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
  }

############ Adaptive Cluster ###############
for(x in 1:10){
  cluster1 <- adaptive_cluster(predic_raster_quad, 100)
  print(nrow(cluster1))
    
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Fig2_AdaptiveCluster",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster_quad, main = ""); points(cluster1, pch=16, cex=0.25)
    mtext("b)", side=3, line=0.5, adj=-0.1)
  mtext("Adaptive cluster sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
}   

############ Convenience (based on samples from 1990, 2000, and 2010) ##########
for(x in 1:10){
  conve1 <- convenience_sampling(predic_raster_quad, p_norm, 100)
  print(nrow(conve1))
    
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Fig2_Convenience",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster_quad, main = ""); points(conve1, pch=16, cex=0.25)
  contour(p_norm, levels = c(0,0.25,0.5,0.75,1), add=TRUE)
  mtext("c)", side=3, line=0.5, adj=-0.1)

  mtext("Convenience sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
}  
```


Maxent compared to the 'true' distribution
# Maxent function  
  Background points are sampled randomly from the cells that are not NA in the first predictor variable, unless background points are specified with argument a. 
  If keep maxent old, through dismo, use this
  ## found in one rhistory from one drive, will attempt to update!
```{r}

#test 
rasterstack <- rasterstack_norm
# sampling_type <- "system"
true.dist <- predic_raster_quad
# reps <- 2
# rm(rasterstack); rm(true.dist); rm(sampling_type)

# rasterstack = predictor variables; true.dist = true distribution of suitability; conveience.raster = the raster of density of all herbarium collections sampled from three different years, could be roads or trails

maxent_sampling <- function(rasterstack, true.dist, 
                            sampling_type = c("random","systematic","convenience","cluster"),
                            convenience.raster = p_norm,
                            reps = 100, samplesize, pathstart = pathstart){

for(r in 1:reps){
  if(grepl(sampling_type, "random")){
    sample_xy <- random_sampling(suitability.raster = true.dist,samplesize = samplesize)
  } 
  if(grepl(sampling_type, "systematic")){
    sample_xy <- systematic_sampling(suitability.raster = true.dist, samplesize = samplesize, studyarea = 100)[[1]]
  }
  if(grepl(sampling_type, "convenience")){
    sample_xy <- convenience_sampling(suitability.raster = true.dist, convenience.raster, samplesize = samplesize)
  }
  if(grepl(sampling_type, "cluster")){
    sample_xy <- adaptive_cluster(suitability.raster = true.dist, samplesize = samplesize)
  }
  
  xm <- dismo::maxent(x = rasterstack, p = sample_xy, args=c(
    "outputformat=logistic",
    # 'maximumbackground=10000',
    'defaultprevalence=1.00',
    'betamultiplier=0',
    # 'pictures=true',
    # 'randomtestpoints=30',
    'linear=true',
    'quadratic=true',
    'product=false',
    'threshold=false',
    'hinge=false',
    # 'threads=2',
    'responsecurves=true',
    'jackknife=true',
    'askoverwrite=false'
    ))
  write.csv(sample_xy, paste(pathstart,"maxentOcc",sampling_type,"rep",r,"SampleSize",samplesize,".csv",sep=""))
    save(xm, file = paste(pathstart, "maxent",sampling_type,"rep",r,"SampleSize",samplesize,".Rda",sep=""))
    
    
    gc()
    ncores <- detectCores()-1
    cl = parallel::makeCluster(ncores)
    doParallel::registerDoParallel(cl,ncores)
    
    # compute indices for data splitting
    rows <- 1:nrow(rasterstack)
    split <- sort(rows%%ncores)+1
    outname <- paste(pathstart,"SampleSize",samplesize,"maxent",sampling_type,"rep",r,sep="")

    #predict with subsets of predictor dataset
    foreach(i = unique(split), .combine = c) %dopar% {
      rows_sub <- rows[split==i]
      sub <- raster::crop(rasterstack, raster::extent(rasterstack, min(rows_sub), max(rows_sub),
                                                      1, ncol(rasterstack)))
      raster::predict(sub, xm, filename = paste(outname, i, sep="_"), overwrite = TRUE)
    }
    
    rm(xm)
    gc()
    stopCluster(cl)
  }
  
   lapply(1:reps, function(k){
    resultpath <- list.files(path = pathstart,
                             pattern = paste("SampleSize",samplesize,"maxent",sampling_type,"rep",k,"_",sep=""),
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiff",sampling_type,"Rep",k,"SampleSize",samplesize,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
   
   # delete the .gri and .grd files 
    finished <- list.files(path = pathstart, pattern = "\\.gri$", full.names = TRUE)
    finished
    lapply(finished, function(x) unlink(x))
    finished2 <- list.files(path = pathstart, pattern = "\\.grd$", full.names = TRUE)
    lapply(finished2, function(x) unlink(x))
   
}

```


If use the new Maxent from maxnet::maxnet where Phillips addresses complaints of Royle et al 2012, use this:    
```{r}

# testing
rasterstack <- rasterstack_norm
rasterstack_noBio1 <- dropLayer(rasterstack_norm, 2)
rasterstack_noBio11 <- dropLayer(rasterstack_norm, 3)

sampling_type <- "system"
samplesize <- 100
true.dist <- predic_raster_quad
reps <- 2

maxnet_sampling <- function(rasterstack, true.dist, sampling_type = c("random","systematic","convenience","cluster"),
                            convenience.raster = p_norm,
                            reps = 100, samplesize, pathstart = pathstart){
  for(r in 1:reps){  
    
    if(grepl(sampling_type, "random")){
      sample_xy <- random_sampling(suitability.raster = true.dist,samplesize = samplesize)
    } 
    if(grepl(sampling_type, "systematic")){
      sample_xy <- systematic_sampling(suitability.raster = true.dist, samplesize = samplesize, studyarea = 100)[[1]]
    }
    if(grepl(sampling_type, "convenience")){
      sample_xy <- convenience_sampling(suitability.raster = true.dist, convenience.raster, samplesize = samplesize)
    }
    if(grepl(sampling_type, "cluster")){
      sample_xy <- adaptive_cluster(suitability.raster = true.dist, samplesize = samplesize)
    }
    
    # Maxnet
    p <- c(rep(1, samplesize),rep(0, 300))
    data <- data.frame(raster::extract(rasterstack, sample_xy))
    background <- sampleRandom(rasterstack,300)
    data <- rbind(data,background)
    mod <- maxnet(p, data, regmult = 0.001, # yelled at zero but set small to keep all features
                  maxnet.formula(p, data, classes = "lq")) # using glmnet for fit; where do I put type = "link",
    # plot(mod, "Elevation")
    mod$betas  # nonzero coefficients of the fitted model; selects model$beta[,200] and then of those, the ones that are not zero (this is where model reduction happens!!)
    mod$alpha # might be the LN linear normalizer discussed in Royle et al. 2012
    mod$penalty.factor
    LN <- -1*max(mod$betas*sapply(names(mod$betas), function(n){ 
      mod$featuremaxs[names(mod$featuremaxs)==n]
      }))
    # + mod$betas*sapply(names(mod$betas), function(n){
    #       mod$penalty.factor[names(mod$penalty.factor)==n]
    #     })) # not like this, aren't the same. 
    LN
    
    #MIAmaxent
    
    
    # write.csv(data.frame(sample_xy, fold), paste(pathstart,"maxentOcc",sampling_type,"rep",r,".csv",sep=""))    
    write.csv(sample_xy, paste(pathstart,"maxentOcc",sampling_type,"rep",r,"SampleSize",samplesize,".csv",sep=""))
    save(xm, file = paste(pathstart, "maxent",sampling_type,"rep",r,"SampleSize",samplesize,".Rda",sep=""))
    
    gc()
    ncores <- detectCores()-1
    cl = parallel::makeCluster(ncores)
    doParallel::registerDoParallel(cl,ncores)
    
    # compute indices for data splitting
    rows <- 1:nrow(rasterstack)
    split <- sort(rows%%ncores)+1
    outname <- paste(pathstart,"Predictmaxent",sampling_type,"rep",r,sep="")
    
    #predict with subsets of predictor dataset
    foreach(i = unique(split), .combine = c) %dopar% {
      rows_sub <- rows[split==i]
      sub <- raster::crop(rasterstack, raster::extent(rasterstack, min(rows_sub), max(rows_sub),
                                                      1, ncol(rasterstack)))
      raster::predict(sub, xm, filename = paste(outname, i, sep="_"), overwrite = TRUE)
    }
    rm(xm)
    gc()
    stopCluster(cl)
  }
  
   lapply(1:reps, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste("Predictmaxent",sampling_type,"rep",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiff",sampling_type,"Rep",k,"SampleSize",samplesize,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
}


# 
# plot(xm)
# r <- response(xm, var="bio19_12")
# plot(r, type = "l")
# response(xm)


```




<https://www.youtube.com/watch?v=42rSg60Rk-k&feature=youtu.be&fbclid=IwAR3AvdVB3U7YVI0aVrfm-KYGQm0Vnfo_W3udZL2YoH0h5vQjxQjp6_Kde7A>
<https://www.youtube.com/watch?v=2vgX7QoyPJo&feature=youtu.be&fbclid=IwAR2mvP8p9LDdz1LefrCwjcMMXi1l_uLI1m-1-sIiydHqHyIpMwcooSoz6lc> 

```{r}
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200501/"

# 3:10, 
lapply(c(3,10, 20, 30, 50, 100), function(sampsize){
    # maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster_quad, sampling_type = "random", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster_quad, sampling_type = "system", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    # maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster_quad, sampling_type = "convenience", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    # maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster_quad, sampling_type = "cluster", reps = 100, samplesize = sampsize, pathstart = pathstart)
})


```

# Check if spatial autocorrelation between explainatory variables causes problems 
But! Should still treat true.dist as the predic_raster_quad with bio1, it's just that I'm not using.
true.dist should still be predic_raster_quad, not predic_raster_quad_noBio1
```{r}

pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200501_noBio1/"

lapply(c(3,10, 20, 30, 50, 100), function(sampsize){
    maxent_sampling(rasterstack = rasterstack_noBio1, true.dist = predic_raster_quad, sampling_type = "random", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    maxent_sampling(rasterstack = rasterstack_noBio1, true.dist = predic_raster_quad, sampling_type = "system", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    maxent_sampling(rasterstack = rasterstack_noBio1, true.dist = predic_raster_quad, sampling_type = "convenience", reps = 100, samplesize = sampsize, pathstart = pathstart)
    
    maxent_sampling(rasterstack = rasterstack_noBio1, true.dist = predic_raster_quad, sampling_type = "cluster", reps = 100, samplesize = sampsize, pathstart = pathstart)
})

```


 

# Pattern: correlation to the true distribution  
```{r}

# testing
# ss <-30
# pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200428/"
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200501/"

lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffrandomRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 50000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

ggsave(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/samplesize",ss,"randompoints.jpg", sep=""),
       ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+
  # geom_smooth(se = FALSE)+
        geom_point(alpha = 0.5, size = 0.5)+
          geom_abline(slope = 1, intercept = 0, colour = "black")+
          guides(color=FALSE)+
          theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
          ggtitle(paste("Random sampling (n = ", ss,")", sep="")),
       width=170, height=175,units='mm', dpi=300)
})

# systematic
lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffsystemRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

ggsave(filename = paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/samplesize",ss,"systempoints.jpg", sep=""),
  ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+
  # geom_smooth(se = FALSE)+
        geom_point(alpha = 0.5, size = 0.5)+
  geom_abline(slope = 1, intercept = 0, colour = "black")+
  guides(color=FALSE)+
  theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
  ggtitle(paste("Systematic sampling (n = ", ss,")", sep="")),
  width=170, height=175,units='mm', dpi=300)
})

# adaptive cluster   c(3,10, 20,30,50,100)
lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffclusterRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

ggsave(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/samplesize",ss,"clusterpoints.jpg", sep=""),
       ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+  
        # geom_smooth(se = FALSE, method = "gam" )+
        geom_point(alpha = 0.5, size = 0.5)+
        geom_abline(slope = 1, intercept = 0, colour = "black")+
        guides(color=FALSE)+
        theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
        ggtitle(paste("Adaptive cluster sampling (n = ", ss,")", sep=""))
       ,width=170, height=175,units='mm', dpi=300)
})

# Convenience
lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffconvenienceRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

ggsave(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/samplesize",ss,"conveniencepoints.jpg", sep=""), 
       ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+  
        # geom_smooth(se = FALSE, method = "gam" )+
        geom_point(alpha = 0.5, size = 0.5)+
          geom_abline(slope = 1, intercept = 0, colour = "black")+
          guides(color=FALSE)+
          theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
          ggtitle(paste("Convenience sampling (n = ", ss,")", sep="")), 
       width=170, height=175,units='mm', dpi=300)
})

```


# Pattern: correlation to the true distribution  All one plot
```{r}

pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200501/"

plots_random <- lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffrandomRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

   p <- ggplotGrob(ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+
  # geom_smooth(se = FALSE)+
        geom_point(alpha = 0.5, size = 0.5)+
          geom_abline(slope = 1, intercept = 0, colour = "black")+
          guides(color=FALSE)+
          theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
          ggtitle(paste("Random sampling (n = ", ss,")", sep="")))

   p
})


# systematic
plots_system <- lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffsystemRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

  p <- ggplotGrob(ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+
        geom_point(alpha = 0.5, size = 0.5)+
  geom_abline(slope = 1, intercept = 0, colour = "black")+
  guides(color=FALSE)+
  theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
  ggtitle(paste("Systematic sampling (n = ", ss,")", sep="")))
  p
})

# adaptive cluster  
plots_cluster <- lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffclusterRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

p <- ggplotGrob(ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+  
        # geom_smooth(se = FALSE, method = "gam" )+
        geom_point(alpha = 0.5, size = 0.5)+
        geom_abline(slope = 1, intercept = 0, colour = "black")+
        guides(color=FALSE)+
        theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
        ggtitle(paste("Adaptive cluster sampling (n = ", ss,")", sep="")))
p
})

# Convenience
plots_conven <- lapply(c(3,10, 20,30,50,100), function(ss){
  df_plots <- do.call(rbind, lapply(1:100, function(i){
    r_random <- raster(list.files(path = pathstart, 
                                      pattern = paste("ProbTiffconvenienceRep",
                                                      i,"SampleSize",ss, ".tif", sep=""),
                                      full.names = TRUE))
    sampleof <- sample(1:length(r_random), 10000)
    out <- data.frame(Y=getValues(predic_raster_quad)[sampleof],
                      X=getValues(r_random)[sampleof], Rep = i)
    out
  }))

p <- ggplotGrob(ggplot(df_plots, aes(X, Y, colour = as.factor(Rep)))+  
        # geom_smooth(se = FALSE, method = "gam" )+
        geom_point(alpha = 0.5, size = 0.5)+
          geom_abline(slope = 1, intercept = 0, colour = "black")+
          guides(color=FALSE)+
          theme_bw()+
         xlab("Maxent suitability index")+
         ylab("True probability of occurrence")+
          ggtitle(paste("Convenience sampling (n = ", ss,")", sep="")))
p
})

 ggsave("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/AlldesignsAllss.jpg",
        grid.arrange(plots_random[[1]], plots_random[[2]],plots_random[[3]],
                     plots_random[[4]],plots_random[[5]],plots_random[[6]],
                     plots_conven[[1]], plots_conven[[2]],plots_conven[[3]],
                     plots_conven[[4]],plots_conven[[5]],plots_conven[[6]],
                     plots_cluster[[1]], plots_cluster[[2]],plots_cluster[[3]],
                     plots_cluster[[4]],plots_cluster[[5]],plots_cluster[[6]],
                     plots_system[[1]], plots_system[[2]],plots_system[[3]],
                     plots_system[[4]],plots_system[[5]],plots_system[[6]],nrow = 4),
       width=870, height=450,units='mm', dpi=300)
        


```


Pattern distilled down to a correlation
```{r}
true_random <- lapply(1:100, function(x){
      
          ## Correlation to true
          rtrue <- raster(list.files(path = pathstart, 
                                  pattern = paste("ProbTiffsystemRep", sample(1:10,1), ".tif", sep=""),
                                  full.names = TRUE))
          rsimprand <- raster(list.files(path = pathstart, 
                                  pattern = paste("ProbTiffrandomRep", sample(1:10,1), ".tif", sep=""),
                                  full.names = TRUE))
          cor_True_System <- layerStats(stack(rtrue, predic_raster_quad), 'pearson', na.rm=TRUE)  # non-parameteric, not normally distributied and relationship not linear
          cor_True_System$`pearson correlation coefficient`
          
          cor_True_Random <- layerStats(stack(rsimprand, predic_raster_quad), 'pearson', na.rm=TRUE)  # non-parameteric, not normally distributied and relationship not linear
          cor_True_Random$`pearson correlation coefficient`
          
          out <- data.frame(True = cor_True_System$`pearson correlation coefficient`[2], Random = cor_True_Random$`pearson correlation coefficient`[2])
          out
})

true_random <- do.call(rbind, true_random)

ggplot(true_random, aes(True, Random))+
  geom_point()+
  theme_bw()+
  geom_abline(slope = 1, intercept = 0)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))


cluster_random <- do.call(rbind,lapply(1:100, function(x){
      
          ## Correlation to true
          rclust <- raster(list.files(path = pathstart, 
                                  pattern = paste("ProbTiffclusterRep", sample(1:10,1), ".tif", sep=""),
                                  full.names = TRUE))
          rsimprand <- raster(list.files(path = pathstart, 
                                  pattern = paste("ProbTiffrandomRep", sample(1:10,1), ".tif", sep=""),
                                  full.names = TRUE))
          cor_True_System <- layerStats(stack(rclust, predic_raster_quad), 'pearson', na.rm=TRUE)  # non-parameteric, not normally distributied and relationship not linear
          cor_True_System$`pearson correlation coefficient`
          
          cor_True_Random <- layerStats(stack(rsimprand, predic_raster_quad), 'pearson', na.rm=TRUE)  # non-parameteric, not normally distributied and relationship not linear
          cor_True_Random$`pearson correlation coefficient`
          
          out <- data.frame(Cluster = cor_True_System$`pearson correlation coefficient`[2], Random = cor_True_Random$`pearson correlation coefficient`[2])
          out
}))


ggplot(cluster_random, aes(Cluster, Random))+
  geom_point()+
  theme_bw()+
  geom_abline(slope = 1, intercept = 0)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))
```




# Process: variable importance
```{r}

modelstoload <- unlist(lapply(c("random","system","cluster","convenience"), function(x){
  resultpath <- list.files(path = pathstart,
                           pattern = paste("^","maxent",x,"rep",sep=""),
                           full.names = TRUE)
  }))

maxModels <- do.call(rbind,lapply(modelstoload, function(x){ 
  load(x)
  contribs <- xm@results[grep("contribution", dimnames(xm@results)[[1]])]
  names(contribs) <- dimnames(xm@results)[[1]][grep("contribution",
                                                    dimnames(xm@results)[[1]])]
  out <- data.frame(SampleDesign =  strsplit(gsub("maxent", "",
                                                  strsplit(x,"rep")[[1]][1]),"/")[[1]][11],
                    SampleSize = gsub("[^[:digit:],]", "", strsplit(x, "SampleSize")[[1]][2]),
                    feature = names(contribs), contribs)
  out
  }))

maxModels$SampleSize <- factor(maxModels$SampleSize, levels = c("3","10","20","30","50","100"))
maxModels$feature <- factor(maxModels$feature, levels = c("Bio11.contribution","Bio19.contribution",
                                                          "Bio12.contribution","Bio1.contribution",
                                                          "Elevation.contribution","dummy.contribution"))
p1 <- ggplot(maxModels, aes(x = feature, y = contribs, colour = SampleDesign))+
  facet_wrap(~SampleSize)+
  geom_jitter(width = 0.02)+
  geom_boxplot()+
  ylab("")+
  xlab("")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

ggsave("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Process_fig.jpg",
       p1, width = 400, height=100, units="mm", dpi=300)
```


# Process: variable importance when remove Bio1
```{r}
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations_20200501_noBio1/"

modelstoload <- unlist(lapply(c("random","system","cluster","convenience"), function(x){
  resultpath <- list.files(path = pathstart,
                           pattern = paste("^","maxent",x,"rep",sep=""),
                           full.names = TRUE)
  }))

maxModels <- do.call(rbind,lapply(modelstoload, function(x){ 
  load(x)
  contribs <- xm@results[grep("contribution", dimnames(xm@results)[[1]])]
  names(contribs) <- dimnames(xm@results)[[1]][grep("contribution",
                                                    dimnames(xm@results)[[1]])]
  out <- data.frame(SampleDesign =  strsplit(gsub("maxent", "",
                                                  strsplit(x,"rep")[[1]][1]),"/")[[1]][11],
                    SampleSize = gsub("[^[:digit:],]", "", strsplit(x, "SampleSize")[[1]][2]),
                    feature = names(contribs), contribs)
  out
  }))

maxModels$SampleSize <- factor(maxModels$SampleSize, levels = c("3","10","20","30","50","100"))
maxModels$feature <- factor(maxModels$feature, levels = c("Bio11.contribution","Bio19.contribution",
                                                          "Bio12.contribution", # "Bio1.contribution",
                                                          "Elevation.contribution","dummy.contribution"))
p1 <- ggplot(maxModels, aes(x = feature, y = contribs, colour = SampleDesign))+
  facet_wrap(~SampleSize)+
  geom_jitter(width = 0.02)+
  geom_boxplot()+
  ylab("")+
  xlab("")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

ggsave("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Figures/Process_noBio1_fig.jpg",
       p1, width = 400, height=100, units="mm", dpi=300)
```














```{r}
# install.packages("remotes")
# remotes::install_github("rspatial/disdat")
library(disdat)


```


```{r}

# install.packages("maxlike")
library(maxlike)

```





