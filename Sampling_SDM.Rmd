---
title: "Sampling impacts on SDM"
author: "Michelle DePrenger-Levin"
date: "2/15/2020"
output: html_document
---

```{r}
# install.packages("installr")
# require(installr)
# updateR()

rm(list=ls())

# maybe a mix of R3.5.X and R3.6.X so need to update everything?
# update.packages(ask = FALSE, checkBuilt = TRUE)

# maybe broken and not getting updated or removed, need to find and delete
# find.package("knitr")
# remove.packages("knitr")


# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jre1.8.0_241/lib/") # for 64-bit version
# install.packages("rJava")
# library(rJava)

library(diagram)

require(DiagrammeR)

# simmulations
library(virtualspecies)

# SDM
# Sys.getenv("JAVA_HOME")
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jre1.8.0_241/lib/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Common Files/Oracle/Java/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Users/DePrengm/R/R-3.6.2/library/rJava/libs") # for 64-bit version
library(rJava)

library(dismo)

# Mapping
library(rgeos)
library(raster)
library(maptools)
library(dismo)
library(RNRCS)
library(rgdal)


# Parallelization
library(foreach)
library(parallel)
library(doParallel)
library(magrittr) # pipe %>%


thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}
```



DiagrammeR 
```{r}
grViz("digraph flowchart {

      graph [rankdir = TB, fontsize = 14]
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = box]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']
      tab8 [label = '@@8-1']
      tab9 [label = '@@8-2']
      tab10 [label = '@@8-3']
      tab11 [label = '@@8-4']
      tab12 [label = <Bio<SUB>1</SUB>>]
      tab13 [label = <Bio<SUB>11</SUB>>]
      tab14 [label = <Bio<SUB>12</SUB>>]
      tab15 [label = <Bio<SUB>19</SUB>>]
     
      # edge definitions with the node IDs
      tab1 -> tab3;
      tab4 -> tab5;
      tab2 -> tab3;
      tab5 -> tab6;
      tab3 -> tab8;
      tab8 -> tab3; 
      tab3 -> tab9;
      tab3 -> tab10;
      tab3 -> tab11;
      tab7 -> tab2;
      tab12 -> tab2;
      tab13 -> tab2;
      tab14 -> tab2;
      tab15 -> tab2;
      tab9 -> tab4;
      tab10 -> tab4;
      tab11 -> tab4;
      {rank = same ; tab8; tab3;}
      }

      [1]: 'Model organism (Claytonia rubra)'
      [2]: 'Climate Drivers'
      [3]: 'Build true distribution (SDM)'
      [4]: 'sample'
      [5]: 'Build SDM'
      [6]: 'Compare with true'
      [7]: 'Elevation'
      [8]: c('Systematic (true sample)','Adaptive cluster','Convenience','Simple random')
      ")


```


Bayesian model to treat amount of the niche sampled as a latent vairable  
```{r}

grViz("digraph flowchart {

        
      graph [layout = dot, overlap = FALSE]
        
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = plain, rankdir = LR]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab4 [label = '@@4-1']
      tab5 [label = '@@4-2']
      tab6 [label = '@@4-3']
      tab7 [label = '@@4-4']
      tab8 [label = '@@4-5']
       node [label = '&alpha;&#x2080;&beta;&#x2080;']
      tab9 
       node [label = '&alpha;&#x2081;&beta;&#x2081;']
      tab10
       node [label = '&alpha;&#x2084;&beta;&#x2082;']
      tab11 
       node [label = '&alpha;&#x2082;&beta;&#x2083;']
      tab12 
       node [label = '&alpha;&#x2083;&beta;&#x2084;']
      tab13
       node [label = '&sigma;&#x00B2;']
      tab14
      tab3 [label = '@@3']
      tab15 [label = '@@5']
      
      # edge
      edge[ dir = forw];
      tab3 -> tab9 [style=dotted]; {rank = same tab1 tab3};
      tab3 -> tab10 [style=dotted];
      tab3 -> tab12 [style=dotted];
      tab3 -> tab13 [style=dotted];
      tab3 -> tab11 [style=dotted];
      

      
      # edge definitions with the node IDs
      edge[ dir = back];
      tab1 -> tab2; 
      tab2 -> tab15;
      tab15 -> tab4;
      tab15 -> tab5;
      tab15 -> tab6;
      tab15 -> tab7;
      tab15 -> tab8;
      tab4 -> tab9;
      tab5 -> tab10;
      tab6 -> tab12;
      tab7 -> tab13;
      tab8 -> tab11;
      tab2 -> tab14;{rank = same tab11 tab14};
      }

      [1]: '(Data) &gamma;&#x2081;'
      [2]: 'SDM (Pattern) &eta;'
      [3]: 'Sampling design'
      [4]: c('Annual Precipition','Annual Temperature','Precip of coldest quarter','Mean temp of coldest quarter','Elevation')
      [5]: 'Variable contribution (Process)'
      ")
```


```{r}
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")

colocounties <- spTransform(colocounties.UTM,CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
coElev_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/ElevationResampled_bioclim.tif")
coAspect_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/AspectResampled_bioclim.tif")
# COplus_ruggedInt50.tif
coRugged_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/RuggedResampled_bioclim.tif")
bio1_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif")
bio12_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif")
rasterstack <- stack(coElev_res,coAspect_res,coRugged_res,bio1_res,bio12_res)
```


Use maxent, and then refine the relatonships of environment and points  
```{r}
clru <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Claytonia rubra_SEINet/occurrences.csv")

plot(colocounties)
points(clru$decimalLongitude, clru$decimalLatitude, col="blue")
# points(clar$decimalLongitude, clar$decimalLatitude)


```


```{r}

clru <- clru[complete.cases(clru$decimalLatitude),]
clruspdf <- SpatialPointsDataFrame(coords = clru[,c("decimalLongitude","decimalLatitude")],
                                     data = clru,
                                     proj4string = CRS("+proj=longlat +datum=WGS84"))

```
SDMtune
```{r}
# install.packages("SDMtune")
library(SDMtune)
```
## Skip to next to load normalized layers

# Virtual species 
<http://borisleroy.com/files/virtualspecies-tutorial.html>
virtual species based on Claytonia rubra   
```{r}
# or use all worldclim data
# bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)
# save(bioclim.colorado, file = "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
load("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
bioclim.colorado

rasterstack1 <- stack(bioclim.colorado[[c(1,11,12,19)]]) 
coloElev_resam <- resample(coElev_res, bioclim.colorado[[1]], method = 'bilinear')
coloElev <- crop(coloElev_resam, extent(colocounties), snap="out")
bioclim.stack <- crop(rasterstack1,extent(colocounties), snap="out")
rasterstack2 <- stack(coloElev,bioclim.stack)


# dummy variable
sim_layer <- coloElev
sim_layer <- setValues(sim_layer, runif(length(coloElev), min = 0, max = 100))
names(sim_layer) <- "dummy"
plot(sim_layer)
rasterstack3 <- stack(coloElev,bioclim.stack, sim_layer)

# scale will center (subtract the layer mean) and scale by dividing the centered layer by SD
rasterstack3_z <- scale(rasterstack3)
plot(rasterstack3_z)

# Normalize raster layers to 0-1; linear transformations so do not change the shape of the data
# normalize
# X - min(X) /  max(X) - min(X)
# z-score; Standardized
# X - mean(X) / sd(X)

rasterstack_norm <- rasterstack3
# Normalize to 0-1; worked before and now doesn't
for(i in 1:6){
  rasterstack_norm[[i]] <- calc(rasterstack_norm[[i]], function(x){
    (x-min(getValues(rasterstack_norm[[i]])))/(max(getValues(rasterstack_norm[[i]]))-min(getValues(rasterstack_norm[[i]])))
  })
}

plot(rasterstack_norm[[1]], main="noramlized")
plot(rasterstack3[[1]], main="elevation as is")
plot(rasterstack3_z[[1]], main="scaled minus mean/sd")
plot(scale(rasterstack_norm[[1]]), main="scaled normalized")

# scale in raster::scale just give the z-score
names(rasterstack_norm) <- c("Elevation","Bio1","Bio11","Bio12","Bio19","dummy")

for(i in 1:6){
    writeRaster(rasterstack_norm[[i]], paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/rasterstack_normalized,",i,".tif",sep=""), overwrite=TRUE)
}


```


e^Xlinearmodel/1+e^linear model so that it's [0-1] of the probability of y|x 
Create my own glm for comparison with maxent
```{r}

predic_raster <- rasterstack_norm[[1]]

lm1 <- as.formula(~ getValues(rasterstack_norm[[1]])*1 + getValues(rasterstack_norm[[2]])*2.5 - getValues(rasterstack_norm[[3]])*6 +
                    getValues(rasterstack_norm[[4]])*4 - getValues(rasterstack_norm[[5]])*5 + getValues(rasterstack_norm[[6]])*.005)


identical(length(predic_raster),length(exp(as.formula(lm1[[2]]))/(1+exp(as.formula(lm1[[2]])))))

# predic_raster[] <- exp(as.formula(lm1[[2]]))/(1+exp(as.formula(lm1[[2]])))
predic_raster <- setValues(predic_raster, as.vector(exp(as.formula(lm1[[2]]))/(1+exp(as.formula(lm1[[2]])))))

plot(predic_raster)

# normalize response
predic_raster <- calc(predic_raster, function(x){
    (x-min(getValues(predic_raster)))/(max(getValues(predic_raster))-min(getValues(predic_raster)))
})
```

```{r}
jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1a_abioticvars.jpg",
     width=170, height=100,units='mm', res=300)
        par(mar=c(2.1,2.1,1.1,2.1))
        plot(predic_raster)
        mtext("a)", side=3, line=0, adj=0)
 
dev.off()

jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1b_abioticvars.jpg",
     width=225, height=105,units='mm', res=300)

        plot(rasterstack_norm)
        mtext("b)", side=3, line=3, adj=-.10)

dev.off()
        

# jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig1_abioticvars.jpg",
#      width=170, height=100,units='mm', res=300)
#   
# par(mfrow=c(1,2))      
# # layout(matrix(c(1,2), 1,2, byrow = TRUE), widths = c(3,3), heights = c(1,1))
# # layout.show(2)
# # plot 1
# par(mar=c(2.1,2.1,1.1,2.1))
# plot(predic_raster)
# mtext("a)", side=3, line=0, adj=0)
# 
# # plot 2
# plot(rasterstack_norm)
# mtext("b)", side=3, line=3, adj= -0.1)
# 
# dev.off()
        



```


Pattern and process figures
```{r}


jpeg("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Pattern_compare.jpg",
     width=175, height=175,units='mm', res=300)

plot(1:10, 1:10, type="l", lwd=2, yaxt="n", xaxt="n", ylab="Biased Sampling", xlab="True sampling (systematic)")
mtext("Pixel-based distance from true", side=1, line=0.5)
mtext("Pixel-based distance from true", side=2, line=0.5)
# polygon(c(1,1,8.9), c(2.1,9.9,9.9), col=rgb(.15,0,.5,.25))
arrows(4.9,5.1,3.5,6.5,lwd=1, col='red')
arrows(5.1,4.9,6.5,3.5, col='red')
text("sampling method outperformed systematic", x = 2, y=7.5, adj = 0, cex= 0.95)
text("sampling method underperformed systematic", x = 9, y = 2, adj = 1, cex=0.95)
# across sample size? colored dots by sample size? yes maybe

dev.off()
```



# Sampling functions
Sampling methods
```{r}
# test
# suitability.raster <- predic_raster
# samplesize <- 20

### systematic 
systematic_sampling <- function(suitability.raster,  samplesize){
  # systematic sampling to allow for Bernoulli process to reject some cells with high suitability and sometimes take cells with low suitability
  # evenstep <- ceiling(length(suitability.raster)/(samplesize*mean(suitability.raster[]))) 
  evenstep <- ceiling(length(suitability.raster)/(samplesize*100))
  i <- evenstep
  
  sampleout <- c()
  while(i < length(suitability.raster)){
      if(rbinom(1, 1, getValues(suitability.raster)[i]) == 1){
        sampleout <- rbind(sampleout,xyFromCell(suitability.raster, i))
      }
      i <- i + evenstep
    }
  
  sampleout <- thin.max(sampleout, c("x","y"), samplesize) # spatial thinning
  sampleout
}


# suitability.raster <- clru1[[3]]
# rm(suitability.raster)
### random
random_sampling <- function(suitability.raster, samplesize){
  sampleout <- c()
  i <- 1
  while(i <= samplesize){
    randcell <- ceiling(runif(1, min = 1, max = length(suitability.raster)))
    
    if(rbinom(1, 1, getValues(suitability.raster)[randcell]) == 1){
      sampleout <- rbind(sampleout,xyFromCell(suitability.raster, randcell))
      i <- i+1
    } 
  }
  sampleout
}



### adaptive cluster sampling  
# function to find the surrounding cells
# check if on edge is it 1 or max row or col then select the remaining ones
surroundingcells <- function(cellRowCol){
  out <- data.frame(Cells = c(cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]), # center cell; 1
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]+1), # row above; 4
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]+1), # either side, same row; 6
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]+1)), # next row; 9
                    Rows = c(cellRowCol[1], # center row
                          cellRowCol[1]-1,
                          cellRowCol[1]-1,
                          cellRowCol[1]-1, # row above
                          cellRowCol[1], 
                          cellRowCol[1],  # either side, same row
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1),
                    Cols = c(cellRowCol[2], # center col
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1, # row above
                          cellRowCol[2]-1,
                          cellRowCol[2]+1, # either side, same row
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1))
  if(cellRowCol[1]==1){
    out <- out[c(1,5:9),]
  }
  if(cellRowCol[2]==1){
    out <- out[c(1,3:4,6,8:9),]
  }
  if(cellRowCol[1]==nrow(suitability.raster)){
    out <- out[c(1,1:6),]
  }
  if(cellRowCol[2]==ncol(suitability.raster)){
    out <- out[c(1:3,5,7:8),]
  }
  out
}


adaptive_cluster <- function(suitability.raster, samplesize){
  n <- 0
  samples <- c()
  while(n <= samplesize){
    if(n == samplesize) break
    # Select a random cell (name row, col, and cell number)
    randcellRowCol <- c(ceiling(runif(1, min = 1, max = nrow(suitability.raster))),
                    ceiling(runif(1, min = 1, max = ncol(suitability.raster)))) # Row Col
    randcell <- cellFromRowCol(suitability.raster,randcellRowCol[1],randcellRowCol[2]) # cell number that matches
    # start a cluster
    # Bernoulli trial for setting a presence at the cell with probabiliity of suitability score to start a cluster
    if(rbinom(1,1,getValues(suitability.raster)[randcell]) == 1){
      n <- n+1
      samples <- rbind(samples, xyFromCell(suitability.raster, randcell))
      if(n == samplesize) break
      surrcells <- surroundingcells(randcellRowCol)
      
      # check surrounding cells of a 'presence' and start this cluster
      j <- 2 # move down the table of cells, adding non-duplicated if cell occupied, first row is center of cluster, included to check for dupliates
      # Could hit sample size in the middle of checking all the surrounding cells
      while(j < nrow(surrcells)){
        if(rbinom(1,1,getValues(suitability.raster)[surrcells$Cells[j]]) == 1){
          samples <- rbind(samples, xyFromCell(suitability.raster, surrcells$Cells[j]))
          surrcells <- rbind(surrcells, surroundingcells(as.numeric(paste(surrcells[j,c(2:3)]))) )
          surrcells <- surrcells[!duplicated(surrcells$Cells),] # only add the surrounding cells not already included
          j <- j+1 # move down surrounding cells of original cluster 
          n <- n+1 # added to sample
          if(n == samplesize) break
          if(j == samplesize) break
        } else {# surrounding the surrounding cells, clusters
          j <- j+1 # last one not added, check the next row
          }
      }  # end while statement j is number of surrounding cells to be checked
    } # end if cell selected, when not occupied, select a new random cell
    
  } # end while sample size n <= samplesize so don't start a new cluster
  samples
}


### Convenience sampling, weighted by sampled areas
# 1. set each convenience raster cell to the proportion of SEINet herb collections per cell as convenience

# convenience_raster <- predic_raster
# # All samples from 1990, 2000, 2010 to represent samples over time over 21 years
# xy_1990 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_1990/occurrences.csv")
# xy_2000 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_2000/occurrences.csv")
# xy_2010 <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Colorado_2010/occurrences.csv")
# xys <- do.call(rbind,lapply(list(xy_1990,xy_2000,xy_2010), function(xys){
#   i <- sapply(xys, is.factor)
#   xys[i] <- lapply(xys[i], as.character)
#   xys    
# }))
# 
# xys$decimalLatitude <- as.numeric(xys$decimalLatitude)
# xys$decimalLongitude <- as.numeric(xys$decimalLongitude)
# xys <- xys[!is.na(xys$decimalLongitude),c("decimalLongitude","decimalLatitude")]
# convenience_raster <- rasterize(xys, convenience_raster, fun='count')

## Interpolate <https://www.rdocumentation.org/packages/raster/versions/3.0-12/topics/interpolate> 
# library(fields)
# tps <- Tps(xyFromCell(convenience_raster, 1:ncell(convenience_raster)), getValues(convenience_raster))
# p <- raster(convenience_raster)
# p <- interpolate(p, tps)
# writeRaster(p, "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/convenience_raster_interpolated.tif", overwrite=TRUE)
p <- raster("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/convenience_raster_interpolated.tif")

p_norm <- calc(p, function(x){
    (x-min(getValues(p)))/(max(getValues(p))-min(getValues(p)))
})
# plot(p)
plot(p_norm)

# # contour lines for the convenience map
# cont <- rasterToContour(p_norm)
# plot(cont)
# plot(predic_raster)
# plot(cont, add=TRUE)


# 2. have rbinom selection of that cell probability based on convenience weights
# 3. follow the random selection 
convenience_sampling <- function(suitability.raster, convenience.raster, samplesize){
  sampleout <- c()
  i <- 1
  while(i <= samplesize){
    randcell <- ceiling(runif(1, min = 1, max = length(suitability.raster)))
      if(rbinom(1,1,getValues(convenience.raster)[randcell]) == 1){
        
        if(rbinom(1, 1, getValues(suitability.raster)[randcell]) == 1){
          sampleout <- rbind(sampleout,xyFromCell(suitability.raster, randcell))
          i <- i+1
        }
      }
    
  }
  sampleout
  
}

 

```

Sampling images
```{r}

########### Systematic (true sample) ############
 
for(x in 1:10){
  samplesys1 <- systematic_sampling(predic_raster, 100)
      
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig2_SystematicTrue",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster, main = ""); points(samplesys1, pch=16, cex=0.25)
  mtext("d)", side=3, line=0.5, adj=-0.1)
  mtext("Systematic sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
  }


############ Simple random ###############

for(x in 1:10){
  random1 <- random_sampling(predic_raster, 100)
  
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig2_simplerandom",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster, main = ""); points(random1, pch=16, cex=0.25)
  mtext("a)", side=3, line=0.5, adj=-0.1)
  mtext("Simple random sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
  }



############ Adaptive Cluster ###############
for(x in 1:10){
  cluster1 <- adaptive_cluster(predic_raster, 100)
  print(nrow(cluster1))
    
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig2_AdaptiveCluster",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster, main = ""); points(cluster1, pch=16, cex=0.25)
    mtext("b)", side=3, line=0.5, adj=-0.1)
  mtext("Adaptive cluster sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
}   

############ Convenience (based on samples from 1990, 2000, and 2010) ##########
for(x in 1:10){
  conve1 <- convenience_sampling(predic_raster, p_norm, 100)
  print(nrow(conve1))
    
  jpeg(paste("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/Fig2_Convenience",x,".jpg", sep=""),
       width=145, height=100,units='mm', res=300)
  par(mar=c(2.1,2.1,2.1,1.1))
  plot(predic_raster, main = ""); points(conve1, pch=16, cex=0.25)
  contour(p_norm, levels = c(0,0.25,0.5,0.75,1), add=TRUE)
  mtext("c)", side=3, line=0.5, adj=-0.1)

  mtext("Convenience sampling (n = 100)", side=3, line=0, adj=0.5, cex=1.15)
  
  dev.off()
}  
```


Maxent compared to the 'true' distribution
# Maxent function  
  Background points are sampled randomly from the cells that are not NA in the first predictor variable, unless background points are specified with argument a.
```{r}
#test 
rasterstack <- rasterstack_norm
# sampling_type <- "system"
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations/"
true.dist <- predic_raster
# reps <- 2
# sampling_type <- "ra"
# rm(rasterstack); rm(true.dist); rm(sampling_type)

maxent_sampling <- function(rasterstack, true.dist, sampling_type = c("random","systematic","convenience","cluster"),
                            convenience.raster = p_norm,
                            reps = 100, samplesize, pathstart = pathstart){
  if(grepl(sampling_type, "random")){
    sample_xy <- random_sampling(suitability.raster = true.dist,samplesize = samplesize)
  } 
  if(grepl(sampling_type, "systematic")){
    sample_xy <- systematic_sampling(suitability.raster = true.dist, samplesize = samplesize)
  }
  if(grepl(sampling_type, "convenience")){
    sample_xy <- convenience_sampling(suitability.raster = true.dist, convenience.raster, samplesize = samplesize)
  }
  if(grepl(sampling_type, "cluster")){
    sample_xy <- adaptive_cluster(suitability.raster = true.dist, samplesize = samplesize)
  }
  
  for(r in 1:reps){
    fold <- kfold(sample_xy, k=5) # withold 20% of the sample for testing; or just use whole sample and test with random background?
    occtest <- sample_xy[fold == 1,]
    occtrain <- sample_xy[fold != 1,]
    xm <- maxent(x = rasterstack, p = occtrain,  args=c(
  # 'maximumbackground=10000',
  'defaultprevalence=1.00',
  # 'betamultiplier=0.5',
  # 'pictures=true',
  # 'randomtestpoints=30',
  'linear=true',
  'quadratic=false',
  'product=false',
  'threshold=false',
  'hinge=false',
  # 'threads=2',
  'responsecurves=true',
  'jackknife=true',
  'askoverwrite=false'
))
    write.csv(data.frame(sample_xy, fold), paste(pathstart,"maxentOcc",sampling_type,"rep",r,".csv",sep=""))
    save(xm, file = paste(pathstart, "maxent",sampling_type,"rep",r,".Rda",sep=""))
    
    gc()
    ncores <- detectCores()-1
    cl = parallel::makeCluster(ncores)
    doParallel::registerDoParallel(cl,ncores)
    
    # compute indices for data splitting
    rows <- 1:nrow(rasterstack)
    split <- sort(rows%%ncores)+1
    outname <- paste(pathstart,"Predictmaxent",sampling_type,"rep",r,sep="")
    
    #predict with subsets of predictor dataset
    foreach(i = unique(split), .combine = c) %dopar% {
      rows_sub <- rows[split==i]
      sub <- raster::crop(rasterstack, raster::extent(rasterstack, min(rows_sub), max(rows_sub),
                                                      1, ncol(rasterstack)))
      raster::predict(sub, xm, filename = paste(outname, i, sep="_"), overwrite = TRUE)
    }
    
    # random background data
    bg <- randomPoints(rasterstack, 300)
    e <- evaluate(xm, p=occtest, a=bg, x=rasterstack)
    save(e, file = paste(pathstart, "evaluate", sampling_type,"rep",r,".Rda",sep=""))
    
    rm(xm)
    gc()
    stopCluster(cl)
  }
  
   lapply(1:reps, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste("Predictmaxent",sampling_type,"rep",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiff",r,sampling_type,"Rep",k,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
}

# 
# plot(xm)
# r <- response(xm, var="bio19_12")
# plot(r, type = "l")
# response(xm)

```

<https://www.youtube.com/watch?v=42rSg60Rk-k&feature=youtu.be&fbclid=IwAR3AvdVB3U7YVI0aVrfm-KYGQm0Vnfo_W3udZL2YoH0h5vQjxQjp6_Kde7A>
<https://www.youtube.com/watch?v=2vgX7QoyPJo&feature=youtu.be&fbclid=IwAR2mvP8p9LDdz1LefrCwjcMMXi1l_uLI1m-1-sIiydHqHyIpMwcooSoz6lc> 

```{r}
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations/"

maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster, sampling_type = "random", reps = 10, samplesize = 50, pathstart = pathstart)

maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster, sampling_type = "system", reps = 10, samplesize = 50, pathstart = pathstart)

maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster, sampling_type = "convenience", reps = 10, samplesize = 50, pathstart = pathstart)

maxent_sampling(rasterstack = rasterstack_norm, true.dist = predic_raster, sampling_type = "cluster", reps = 10, samplesize = 50, pathstart = pathstart)


```

# Pattern: correlation between, within, and to the systematic  
```{r}

# c("system","random","convenience","cluster")
All_nicheEquivalency <- lapply(1:10, function(i) {
      # Sample from the kfolds and repeat a few times
      sp1 <- read.csv(paste(pathstart, "maxentOccsystemrep", sample(1:10, 1),".csv", sep=""))
      sp2 <- read.csv(paste(pathstart, "maxentOccrandomrep", sample(1:10, 1),".csv", sep=""))
      sp3 <- read.csv(paste(pathstart, "maxentOccconveniencerep", sample(1:10, 1),".csv", sep=""))
      sp4 <- read.csv(paste(pathstart, "maxentOccclusterrep", sample(1:10, 1),".csv", sep=""))
      
      Sp1 <- SpatialPoints(sp1[,c("x","y")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      Sp2 <- SpatialPoints(sp2[,c("x","y")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      Sp3 <- SpatialPoints(sp3[,c("x","y")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      registerDoParallel(cl)
      trials <- 100
      foreach(icount(trials),  
              .combine='c', 
              .packages = c("dismo")) %dopar% {
                
                equiv1_2 <- nicheEquivalency(Sp1, Sp2, rasterstack, n=100)
                equiv1_3 <- nicheEquivalency(Sp1, Sp3, rasterstack, n=100)
                equiv2_3 <- nicheEquivalency(Sp2, Sp3, rasterstack, n=100)
                list(equiv1_2,equiv1_3,equiv2_3)
              }
      
      stopCluster(cl)
      })

```




# Process: variable importance
```{r}

modelstoload <- unlist(lapply(c("random","system"), function(x){
  resultpath <- list.files(path = pathstart,
                           pattern = paste("^","maxent",x,"rep",sep=""),
                           full.names = TRUE)
  }))


load(modelstoload[1])

maxModels <- lapply(modelstoload, function(x) load(x))
response(xm) # response curve shapes
plot(xm) # variable contriubtion
var.importance <- data.frame(Var = row.names(xm@results), xm@results)[7:12,]

# evaluate models
evalstoload <- list.files(path = pathstart,
                           pattern = "evaluate",
                           full.names = TRUE)
evalModels <- lapply(evalstoload, function(x) load(x))

load(evalstoload[1])
threshold(e)  # can get prevalence and other measures for thresholds
plot(e, 'ROC')
```



# Nicheoverlap function    
```{r}

```


# JAGS or make a gibbs sampler?   
Just raster stack glm with the enviornmental variable relationships. 
```{r}
# The distribution of suitable values by layer
ana_rast <- stack(rasterstack3_z, clru1_z[[3]])
# x <- calc(ana_rast, function(x) if(ana_rast[[7]]>0.3) lm(x~time))

# Distribution in suitable 
for(i in names(ana_rast)[-7]){
  hist(ana_rast[[i]][ana_rast[[7]]>0.3], main=paste(i))
}
for(i in names(ana_rast)[-7]){
  hist(ana_rast[[i]][ana_rast[[7]]>0.5], main=paste(i))
}
# Take the central tendancy as alpha and the SD as var? those are the first step to estimate those parameters from the random sample? Then take the output from those as the input 

# if beta then dbeta(ElevationResampled_bioclim, $alpha, $beta, ncp = based on derivation as a Poisson mixture of betas Johnson et al 1995)
model1 <- ElevationResampled_bioclim[]*x

r_suitability <- stackApply(rasterstack3_z, indices = 1:6, function(x) )

```




