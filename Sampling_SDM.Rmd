---
title: "Sampling impacts on SDM"
author: "Michelle DePrenger-Levin"
date: "2/15/2020"
output: html_document
---

```{r}
# install.packages("installr")
# require(installr)
# updateR()

# maybe a mix of R3.5.X and R3.6.X so need to update everything?
# update.packages(ask = FALSE, checkBuilt = TRUE)

# maybe broken and not getting updated or removed, need to find and delete
# find.package("knitr")
# remove.packages("knitr")


# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jre1.8.0_241/lib/") # for 64-bit version
# install.packages("rJava")
# library(rJava)

library(diagram)

require(DiagrammeR)

# simmulations
library(virtualspecies)

# SDM
# Sys.getenv("JAVA_HOME")
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jre1.8.0_241/lib/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Common Files/Oracle/Java/") # for 64-bit version
# Sys.setenv(JAVA_HOME="C:/Users/DePrengm/R/R-3.6.2/library/rJava/libs") # for 64-bit version
library(rJava)

library(dismo)

# Mapping
library(rgeos)
library(raster)
library(maptools)
library(dismo)
library(RNRCS)
library(rgdal)


# Parallelization
library(foreach)
library(parallel)
library(doParallel)
library(magrittr) # pipe %>%


thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}
```



DiagrammeR 
```{r}
grViz("digraph flowchart {

      graph [rankdir = TB, fontsize = 14]
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = box]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']
      tab8 [label = '@@8-1']
      tab9 [label = '@@8-2']
      tab10 [label = '@@8-3']
      tab11 [label = '@@8-4']
     
      # edge definitions with the node IDs
      tab1 -> tab3;
      tab4 -> tab5;
      tab2 -> tab3;
      tab5 -> tab6;
      tab3 -> tab8;
      tab3 -> tab9;
      tab3 -> tab10;
      tab3 -> tab11;
      tab7 -> tab2;
      tab8 -> tab4;
      tab9 -> tab4;
      tab10 -> tab4;
      tab11 -> tab4;
      }

      [1]: 'Model organism (Claytonia rubra)'
      [2]: 'Climate Drivers'
      [3]: 'Build true distribution (SDM)'
      [4]: 'sample'
      [5]: 'Build SDM'
      [6]: 'Compare with true'
      [7]: 'Bio&#x2081; Bio&#x2082; Bio&#x2083; Bio&#x2084; Elevation, dummy'
      [8]: c('Systematic','Adaptive cluster','Convenience','Simple random')
      ")


```


Bayesian model to treat amount of the niche sampled as a latent vairable  
```{r}

grViz("digraph flowchart {

        
      graph [layout = dot, overlap = FALSE]
        
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = plain, rankdir = LR]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab4 [label = '@@4-1']
      tab5 [label = '@@4-2']
      tab6 [label = '@@4-3']
      tab7 [label = '@@4-4']
      tab8 [label = '@@4-5']
       node [label = '&alpha;&#x2080;&beta;&#x2080;']
      tab9 
       node [label = '&alpha;&#x2081;&beta;&#x2081;']
      tab10
       node [label = '&alpha;&#x2084;&beta;&#x2082;']
      tab11 
       node [label = '&alpha;&#x2082;&beta;&#x2083;']
      tab12 
       node [label = '&alpha;&#x2083;&beta;&#x2084;']
      tab13
       node [label = '&sigma;&#x00B2;']
      tab14
      tab3 [label = '@@3']
      
      # edge
      edge[ dir = forw];
      tab3 -> tab9 [style=dotted]; {rank = same tab1 tab3};
      tab3 -> tab10 [style=dotted];
      tab3 -> tab12 [style=dotted];
      tab3 -> tab13 [style=dotted];
      tab3 -> tab11 [style=dotted];
      

      
      # edge definitions with the node IDs
      edge[ dir = back];
      tab1 -> tab2; 
      tab2 -> tab4;
      tab2 -> tab5;
      tab2 -> tab6;
      tab2 -> tab7;
      tab2 -> tab8;
      tab4 -> tab9;
      tab5 -> tab10;
      tab6 -> tab12;
      tab7 -> tab13;
      tab8 -> tab11;
      tab2 -> tab14;{rank = same tab11 tab14};
      }

      [1]: '(Data) &gamma;&#x2081;'
      [2]: 'SDM (Process) &eta;'
      [3]: 'Sampling design'
      [4]: c('Annual Precipition','Annual Temperature','Precip of coldest quarter','Mean temp of coldest quarter','Elevation')
      ")
```



Snow melt, temperature
```{r}
# install.packages("snotelr")
# library("snotelr")
# 
# library(snotelr)
# snotel.explorer()

# install.packages("RNRCS")


# grabNRCS.meta()
# snow1 <- grabNRCS.data(network = 'SNTL', site_id = 2229, timescale = "monthly", DayBgn = '2019-01-01', DayEnd = '2020-01-01')
# 
# 
# load("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/hackathon/Simulations/SNOWHLNDmodelavg_annual.RData") # 1920-2005
# load("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/hackathon/Simulations/SNOWHLNDmodelavg.raster.RData")
# load("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/hackathon/Simulations/SNOWHLNDmodel17_annual.RData") # 5, 17 year blocks averaged 
# load("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/hackathon/Simulations/SNOWHLNDmodel17_raster.RData")
# 
# 
# str(SNOWHLNDmodel17.raster[[1]])
# plot(SNOWHLNDmodel17.raster[[1]][[1]])
```

```{r}
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")

colocounties <- spTransform(colocounties.UTM,CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
coElev_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/ElevationResampled_bioclim.tif")
coAspect_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/AspectResampled_bioclim.tif")
# COplus_ruggedInt50.tif
coRugged_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/RuggedResampled_bioclim.tif")
bio1_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif")
bio12_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif")
rasterstack <- stack(coElev_res,coAspect_res,coRugged_res,bio1_res,bio12_res)
```


Use maxent, and then refine the relatonships of environment and points  
```{r}
# hygr <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Hymenoxys grandiflora_SEINet/occurrences.csv")
# 
# plot(hygr$decimalLongitude, hygr$decimalLatitude)
# 
# hygr <- hygr[complete.cases(hygr$decimalLatitude),]
# 
# pointsspdf <- SpatialPointsDataFrame(coords = hygr[,c("decimalLongitude","decimalLatitude")],
#                                      data = hygr,
#                                      proj4string = CRS("+proj=longlat +datum=WGS84"))
# 
# plot(bio12_res)
# # plot(SNOWHLNDmodel17.raster[[1]][[1]], add=TRUE)
# points(pointsspdf, pch = 16)
# plot(colocounties, add=TRUE)


# clar <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Claytonia arctica_SEINet/occurrences.csv")

clru <- read.csv("C:/Users/DePrengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/Claytonia rubra_SEINet/occurrences.csv")

plot(colocounties)
points(clru$decimalLongitude, clru$decimalLatitude, col="blue")
# points(clar$decimalLongitude, clar$decimalLatitude)


```


```{r}

clru <- clru[complete.cases(clru$decimalLatitude),]
clruspdf <- SpatialPointsDataFrame(coords = clru[,c("decimalLongitude","decimalLatitude")],
                                     data = clru,
                                     proj4string = CRS("+proj=longlat +datum=WGS84"))

# 
# plot(SNOWHLNDmodel17.raster[[1]][[1]])
# points(clruspdf, pch = 16)
# plot(colocounties, add=TRUE)

# snow11 <- rotate(SNOWHLNDmodel17.raster[[1]][[1]])
# plot(snow11)
# plot(SNOWHLNDmodel17.raster[[1]][[1]])
# 
# snowclip11 <- mask(snow11, colocounties)
# plot(snowclip11)
# 
# snowcrop11 <- crop(snow11, extent(colocounties), snap="out")
# plot(snowcrop11)
# points(clar$decimalLongitude, clar$decimalLatitude)
# points(hygr$decimalLongitude, hygr$decimalLatitude, col="blue")
```
SDMtune
```{r}
install.packages("SDMtune")
library(SDMtune)
```

# Virtual species 
<http://borisleroy.com/files/virtualspecies-tutorial.html>
virtual species based on Claytonia rubra   
```{r}
# or use all worldclim data
# bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)
# save(bioclim.colorado, file = "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
load("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/R/bioclim.colorado.Rda")
bioclim.colorado

rasterstack1 <- stack(bioclim.colorado[[c(1,11,12,19)]]) # Does this need to be rotated? And turned into Z scores? subtract mean and divide by SD? 

coloElev_resam <- resample(coElev_res, bioclim.colorado[[1]], method = 'bilinear')
coloElev <- crop(coloElev_resam, extent(colocounties), snap="out")
bioclim.stack <- crop(rasterstack1,extent(colocounties), snap="out")
rasterstack2 <- stack(coloElev,bioclim.stack)

# Variables bio1 (annual mean temperature in °C * 10) and bio2 (annual precipitation)
plot(bioclim.colorado[[c("bio1_12","bio12_12")]])

```

Define response functions
```{r}

# climate_clru <- raster::extract(bioclim.colorado, clru[,c("decimalLongitude","decimalLatitude")])
# 
# mu_clru <- colMeans(climate_clru, na.rm = TRUE)
# sd_clru <- apply(climate_clru, 2, function(x) sd(x, na.rm = TRUE))
# 
# # gaussian distribution functions 
# dnorm_clru <- mapply(function(x,y) dnorm(x = 150, mean = x, sd = y), mu_clru, sd_clru)
# 
# 
# # using dnorm to generate vitural species
# # my.parameters <- mapply(function(bio, x, y) formatFunctions(assign(paste(bio), 
# #                                                                    c(fun = 'dnorm', mean = x, sd = y))),
# #                         names(bioclim.colorado), mu_clru, sd_clru)
# mu_clru[11]
# sd_clru[11]
# 
# # my.parameters2 <- formatFunctions(bio1_12 = c(fun = 'dnorm', assign('mean', mu_clru[1]), 
# #                                              assign('sd', mu_clru[1])),
# #                                  bio12_12 = c(fun = 'dnorm', assign('mean', mu_clru[12]), 
# #                                               assign('sd', mu_clru[12])))
# # 
# # my.first.species2 <- generateSpFromFun(raster.stack = bioclim.colorado[[c("bio1_12", "bio12_12")]],
# #                                               parameters = my.parameters2,
# #                                               plot = TRUE)
# 
# # bio19 precip of coldest quarter to represent snow accumulation
# # bio11 mean temp of coldest quarter
# my.parameters <- formatFunctions(bio1_12 = c(fun = 'dnorm', mean = 81.5, sd = 30.4),
#                                  bio12_12 = c(fun = 'dnorm', mean = 505.9, sd = 182),
#                                  bio19_12 = c(fun = 'dnorm', mean = 198.1, sd = 117.8),
#                                  bio11_12 = c(fun = 'dnorm', mean = -1.38, sd = 37.4))
# 
# 
# my.first.species <- generateSpFromFun(raster.stack = 
#                                         bioclim.colorado[[c("bio1_12", "bio12_12", "bio19_12", "bio11_12")]],
#                                               parameters = my.parameters,
#                                               plot = TRUE)
# 
# plot(colocounties);plot(my.first.species, add=TRUE);plot(colocounties, add=TRUE); points(clruspdf)
```

Change the functions to use built in linear, quadratic, logistic, normal, and beta or make your own  
logisticFun   (alpha=7.42; beta=272.035035035035) has all 1 suitability until the beta and drops fast to 0
```{r}
# rasterstack1 <- stack(bioclim.colorado[[c(1,11,12,19)]]) # Does this need to be rotated? And turned into Z scores? subtract mean and divide by SD? 
# 
# hist(climate_clru[,12])
# 
# clru.parameters1 <- formatFunctions(bio1_12 = c(fun = 'dnorm', mean = 81.5, sd = 20),
#                                  bio12_12 = c(fun = 'custnorm', mean = 505.9, diff = 70, prob = 0.99),
#                                  bio19_12 = c(fun = 'betaFun', p1 = 0, p2 = 198.1, alpha = 0.9, gamma = 0.08), # precip coldest quarter
#                                  # bio11_12 = c(fun = 'logisticFun', beta = -1.38, alpha = 2)) # mean temp of coldest quarter, should be how fast the snow melts
#                                  bio11_12 = c(fun = 'quadraticFun', a = -1, b = 2, c = -1))
# 
# clru.species1 <- generateSpFromFun(raster.stack = 
#                                         bioclim.colorado[[c("bio1_12", "bio12_12", "bio19_12", "bio11_12")]],
#                                               parameters = clru.parameters1,
#                                    # species.type = "additive",
#                                    formula = "3 * bio19_12 + sqrt(bio11_12) + bio12_12 * bio1_12",
#                                               plot = TRUE)
# 
# plot(colocounties);plot(clru.species1, add=TRUE);plot(colocounties, add=TRUE); points(clruspdf, pch=16, cex=0.5, col='blue')

```

Plot response curves
```{r}

# plotResponse(clru.species1)


```


```{r}
# class(clru.species1)
# 
# # suitability map
# clru.species1[[3]]
# 
# colorado.clru <- crop(clru.species1[[3]], extent(colocounties), snap="out")
# plot(colorado.clru); points(clruspdf, pch=16, cex=0.5, col='blue')
# 
# # convert to presence absence, use beta to set the inflection and alpha to make stronger switch from suitable to unsuitable
# pa1 <- convertToPA(clru.species1, plot = TRUE, beta = 0.65, alpha = -0.001)
# 
# # Can set the species prevalance and let it get there with testing levels of beta that get to the desired prevalence, alpha fixed at default -0.05
# pa2 <- convertToPA(clru.species1, plot = TRUE, species.prevalence = 0.2)  
# - beta = 0.4677734375
# - alpha = -0.05
# - species prevalence =0.200104841931879
```

#########################################################################################
# attempt to get a more specific distribution  
## add a dummy predictor variable

```{r}
# dummy variable
sim_layer <- coloElev
sim_layer <- setValues(sim_layer, runif(length(coloElev), min = 0, max = 100))
names(sim_layer) <- "dummy"
plot(sim_layer)

rasterstack3 <- stack(coloElev,bioclim.stack, sim_layer)

# scale will center (subtract the layer mean) and scale by dividing the centered layer by SD
rasterstack3_z <- scale(rasterstack3)
plot(rasterstack3_z[[6]])


```

Look at coding distributions
```{r}
x <- seq(0,1,length.out = 100)
#beta
alpha <- 0.3
beta_1 <- 1 

for(alpha in seq(0,1,length.out = 10)){
  for(beta_1 in seq(0,1,length.out = 10)){
    y <- (x^(alpha-1))*((1-x)^(beta_1-1))
    plot(x,y, main=paste("alpha =", alpha, " beta =", beta_1))
}}

```

lognormal
```{r}
x <- seq(0,1,length.out = 100)

for(mu in seq(0.001,1,length.out = 10)){
  for(sig2 in seq(0.001,1,length.out = 10)){
    alpha1 <- log(mu)-(1/2)*log((sig2+mu^2)/mu^2)
    beta1 <- sqrt(log((sig2+mu^2)/mu^2))
    y <- (1/(x*sqrt(2*pi*beta1^2)))*(exp(-(log(x)-alpha1)^2)/(2*beta1^2))
    plot(x,y, main=paste("mu =",mu, "sig2 =",sig2,"alpha =", alpha, " beta =", beta_1))
}}

```

Exponential
```{r}
x <- seq(0,1,length.out = 100)


for(alpha in seq(0,1,length.out = 10)){
  for(beta_1 in seq(0,1,length.out = 10)){
    y <- (x^(alpha-1))*((1-x)^(beta_1-1))
    plot(x,y, main=paste("alpha =", alpha, " beta =", beta_1))
}}

```

Normalize raster layers to 0-1; linear transformations so do not change the shape of the data
```{r}
# normalize
# X - min(X) /  max(X) - min(X)
# z-score; Standardized
# X - min(X) / max(X)-min(X)


rasterstack_norm <- rasterstack3
# Normalize to 0-1
for(i in 1:6){
  rasterstack_norm[[i]] <- calc(rasterstack_norm[[1]], function(x){
    (x-min(getValues(rasterstack_norm[[i]])))/(max(getValues(rasterstack_norm[[i]]))-min(getValues(rasterstack_norm[[i]])))
  })
}

plot(rasterstack_norm[[1]])
plot(rasterstack3[[1]])
plot(rasterstack3_z[[1]])
plot(scale(rasterstack_norm[[1]]))

# scale in raster::scale just give the z-score
rasterstack_norm[[2]] <- scale(rasterstack_norm[[2]])
plot(rasterstack_norm[[2]])
plot(rasterstack3[[2]])

```



Create my own glm for comparison with maxent
```{r}
predic_raster <- rasterstack_norm[[1]]
predic_raster[] <- (getValues(rasterstack3_z[[1]])^2)*1 + log(getValues(rasterstack3_z[[2]]))*2 + getValues(rasterstack3_z[[3]])*3 + getValues(rasterstack3_z[[4]])*4 + getValues(rasterstack3_z[[5]])*0.001

# predic_raster[] <- exp(getValues(rasterstack3_z[[1]])*1 + getValues(rasterstack3_z[[2]])*2 + getValues(rasterstack3_z[[3]])*3 + getValues(rasterstack3_z[[4]])*4 + getValues(rasterstack3_z[[5]])*0.001)/(1-exp(getValues(rasterstack3_z[[1]])*1 + getValues(rasterstack3_z[[2]])*2 + getValues(rasterstack3_z[[3]])*3 + getValues(rasterstack3_z[[4]])*4 + getValues(rasterstack3_z[[5]])*0.001))

plot(predic_raster)
plot(rasterstack3_z[[1]])

for(i in 1:6){
  hist(getValues(rasterstack3_z[[i]]))
}

hist(predic_raster)

for(i in 1:6){
  hist(getValues(rasterstack3_z[[i]][predic_raster[predic_raster>0.5]]))
}

```

Define response functions   
 again with a dummy variable to check model
```{r}

# add in a dummy variable and scale all variables
rasterstack3_z

climate_clru3 <- raster::extract(rasterstack3_z, clru[,c("decimalLongitude","decimalLatitude")])
mu_clru3 <- colMeans(climate_clru3, na.rm = TRUE)
sd_clru3 <- apply(climate_clru3, 2, function(x) sd(x, na.rm = TRUE))
hist(climate_clru3[,4])

```

e^Xlinearmodel/1+e^linear model so that it's [0-1] of the probability of y|x 
```{r}
# Need to be beta? if I've scaled? But these aren't 0-1 like it does somehow, when I don't rescale, maybe fine? 
# bio19 precip of coldest quarter to represent snow accumulation
# bio11 mean temp of coldest quarter
# BIO12 = Annual Precipitation
# BIO1 = Annual Mean Temperature
clru.para_z <- formatFunctions(bio1_12 = c(fun = 'dnorm', mean = -0.05, sd = 0.47035730),
                             bio12_12 = c(fun = 'dnorm', mean = 0.14942025, sd = 0.1),
                             # bio19_12 = c(fun = 'linearFun', a = 1, b = 0),
                             bio19_12 = c(fun = 'betaFun', p1 = -1, p2 = 3, alpha = 0.9, gamma = 0.09),
                             bio11_12 = c(fun = 'logisticFun', beta = -1, alpha = .2),
                             ElevationResampled_bioclim = c(fun = 'betaFun', p1 = -1, p2 = 0.3, alpha = 0.9, gamma = 0.08),
                             dummy = c(fun = 'linearFun', a=0, b=0),
                             rescale = FALSE) # but let the suitability be rescaled to 0 - 1

 # - The response to each variable was rescaled between 0 and 1. To
 #            disable, set argument rescale.each.response = FALSE
 # 
 # - The final environmental suitability was rescaled between 0 and 1.
 #            To disable, set argument rescale = FALSE
clru1_z <- generateSpFromFun(raster.stack = rasterstack3_z, parameters = clru.para_z,
                             species.type = "mixed",
                             formula = "bio1_12*3 + bio12_12 *2+ (bio11_12 * bio19_12)*4 + ElevationResampled_bioclim + dummy", 
                             rescale.each.response = FALSE,
                             plot = TRUE)

plot(clru1_z);plot(colocounties, add=TRUE); points(clruspdf, pch=16, cex=0.25,col="blue")

# Plot response curves
plotResponse(clru1_z)
edit(generateSpFromFun)
```



###############################################################################################
###############################################################################################
###############################################################################################


```{r}
# suitability map
# clru.species1[[3]]

# colorado.clru <- crop(clru.species1[[3]], extent(colocounties), snap="out")
# plot(colorado.clru); points(clruspdf, pch=16, cex=0.5, col='blue')

# convert to presence absence, use beta to set the inflection and alpha to make stronger switch from suitable to unsuitable
# pa1 <- convertToPA(clru.species1, plot = TRUE, beta = 0.65, alpha = -0.001)

# Can set the species prevalance and let it get there with testing levels of beta that get to the desired prevalence, alpha fixed at default -0.05
# pa2 <- convertToPA(clru.species1, plot = TRUE, species.prevalence = 0.2)  
# - beta = 0.4677734375
# - alpha = -0.05
# - species prevalence =0.200104841931879
```





#################################################################################
Sampler testing  
```{r}

length(clru1_z) 
nrow(clru1_z) # 482
ncol(clru1_z) # 844
# use xyFromCell() to get coordinates from row, column, or cell number
xyFromCell(clru1_z[[3]], ceiling(runif(1, min = 1, max = length(clru1_z[[3]]))))

### systematic and thinning
systematic_sampling_thinning <- function(suitability.raster,  samplesize){
  evenstep <- length(suitability.raster)/stepsize
  i <- evenstep
  
  sampleout <- c()
  while(i < length(suitability.raster)){
    if(getValues(suitability.raster)[i] > 0.3 &
       rbinom(1, 1, getValues(suitability.raster)[i]) == 1){
      sampleout <- rbind(sampleout,xyFromCell(suitability.raster, i))
    }
      i <- i + evenstep
  }

  sampleout <- thin.max(sampleout, c("x","y"), samplesize)
  sampleout
}


# use prob argument in sample?
# select that cell if a runif(1, min = minValue(colorado.clru), max = maxValue(colorado.clru)) is < the cell value; cell number comes from indexing: colorado.clru[cellnumber]
getValues(colorado.clru)[ceiling(runif(1, min = 1, max = length(colorado.clru)))]

 # the first parameter of pbinom must be the number of "successes,", the second parameter is the number of trials, and the third parameter is the chance of success (not of failure). It looks like pbinom(n,m,p) is returning Pr(X≤n) when X has a Binomial(m,p) distribution. Just as a check that ≤ is meant instead of <, compute some small values

plot(pbinom(1:100,100,0.8))
rbinom(1:100, 1, 0.8)

# > runif(1, min = mean(getValue(suitability.raster), max = maxValue(colorado.clru))


# Maybe this is faster? But need to not get below 0.5
# Which are above threshold of 0.5? Oh, maybe change the values from 0.5 to 0 
colorado.clru.threshold <- colorado.clru
colorado.clru.threshold[colorado.clru.threshold<0.5] <- 0
randfaster <- xyFromCell(colorado.clru.threshold,sample(1:length(colorado.clru.threshold), 100, prob = getValues(colorado.clru.threshold)))
plot(colorado.clru); points(randfaster, pch=16, cex=0.5)
# randfaster <- xyFromCell(colorado.clru,sample(colorado.clru[colorado.clru>0.5], 100, prob = getValues(colorado.clru[colorado.clru[colorado.clru>0.5]])))


samplerand1 <- random_sampling(colorado.clru, 100)
plot(colorado.clru); points(samplerand1, pch=16, cex=0.5)
```


# Sampling functions
Sampling methods
```{r}
# test
suitability.raster <- clru1_z[[3]]
samplesize <- 20

### systematic 
systematic_sampling <- function(suitability.raster,  samplesize){
  # systematic sampling to allow for Bernoulli process to reject some cells with high suitability and sometimes take cells with low suitability
  # evenstep <- ceiling(length(suitability.raster)/(samplesize*mean(suitability.raster[]))) 
  evenstep <- ceiling(length(suitability.raster)/(samplesize*100))
  i <- evenstep
  
  sampleout <- c()
  while(i < length(suitability.raster)){
      if(rbinom(1, 1, getValues(suitability.raster)[i]) == 1){
        sampleout <- rbind(sampleout,xyFromCell(suitability.raster, i))
      }
      i <- i + evenstep
    }
  
  sampleout <- thin.max(sampleout, c("x","y"), samplesize) # spatial thinning
  sampleout
}



for(x in 1:10){
  samplesys1 <- systematic_sampling(clru1_z[[3]], 100)
  plot(clru1_z[[3]], main = "Systematic sampling (n = 100)"); points(samplesys1, pch=16, cex=0.5)
  }

# suitability.raster <- clru1[[3]]
# rm(suitability.raster)
### random
random_sampling <- function(suitability.raster, samplesize){
  sampleout <- c()
  i <- 1
  while(i <= samplesize){
    randcell <- ceiling(runif(1, min = 1, max = length(suitability.raster)))
    
    if(rbinom(1, 1, getValues(suitability.raster)[randcell]) == 1){
      sampleout <- rbind(sampleout,xyFromCell(suitability.raster, randcell))
      i <- i+1
    } 
  }
  sampleout
}


for(x in 1:10){
  random1 <- random_sampling(clru1_z[[3]], 100)
  plot(clru1_z[[3]], main = "Simple random sampling (n = 100)"); points(random1, pch=16, cex=0.5)
  }



### adaptive cluster sampling  
# function to find the surrounding cells
# check if on edge is it 1 or max row or col then select the remaining ones
surroundingcells <- function(cellRowCol){
  out <- data.frame(Cells = c(cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]), # center cell; 1
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]-1, cellRowCol[2]+1), # row above; 4
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1], cellRowCol[2]+1), # either side, same row; 6
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]-1),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]),
                          cellFromRowCol(suitability.raster, cellRowCol[1]+1, cellRowCol[2]+1)), # next row; 9
                    Rows = c(cellRowCol[1], # center row
                          cellRowCol[1]-1,
                          cellRowCol[1]-1,
                          cellRowCol[1]-1, # row above
                          cellRowCol[1], 
                          cellRowCol[1],  # either side, same row
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1, 
                          cellRowCol[1]+1),
                    Cols = c(cellRowCol[2], # center col
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1, # row above
                          cellRowCol[2]-1,
                          cellRowCol[2]+1, # either side, same row
                          cellRowCol[2]-1,
                          cellRowCol[2],
                          cellRowCol[2]+1))
  if(cellRowCol[1]==1){
    out <- out[c(1,5:9),]
  }
  if(cellRowCol[2]==1){
    out <- out[c(1,3:4,6,8:9),]
  }
  if(cellRowCol[1]==nrow(suitability.raster)){
    out <- out[c(1,1:6),]
  }
  if(cellRowCol[2]==ncol(suitability.raster)){
    out <- out[c(1:3,5,7:8),]
  }
  out
}


adaptive_cluster <- function(suitability.raster, samplesize){
  n <- 0
  samples <- c()
  while(n <= samplesize){
    if(n == samplesize) break
    # Select a random cell (name row, col, and cell number)
    randcellRowCol <- c(ceiling(runif(1, min = 1, max = nrow(suitability.raster))),
                    ceiling(runif(1, min = 1, max = ncol(suitability.raster)))) # Row Col
    randcell <- cellFromRowCol(suitability.raster,randcellRowCol[1],randcellRowCol[2]) # cell number that matches
    # start a cluster
    # Bernoulli trial for setting a presence at the cell with probabiliity of suitability score to start a cluster
    if(rbinom(1,1,getValues(suitability.raster)[randcell]) == 1){
      n <- n+1
      samples <- rbind(samples, xyFromCell(suitability.raster, randcell))
      if(n == samplesize) break
      surrcells <- surroundingcells(randcellRowCol)
      
      # check surrounding cells of a 'presence' and start this cluster
      j <- 2 # move down the table of cells, adding non-duplicated if cell occupied, first row is center of cluster, included to check for dupliates
      # Could hit sample size in the middle of checking all the surrounding cells
      while(j < nrow(surrcells)){
        if(rbinom(1,1,getValues(suitability.raster)[surrcells$Cells[j]]) == 1){
          samples <- rbind(samples, xyFromCell(suitability.raster, surrcells$Cells[j]))
          surrcells <- rbind(surrcells, surroundingcells(as.numeric(paste(surrcells[j,c(2:3)]))) )
          surrcells <- surrcells[!duplicated(surrcells$Cells),] # only add the surrounding cells not already included
          j <- j+1 # move down surrounding cells of original cluster 
          n <- n+1 # added to sample
          if(n == samplesize) break
          if(j == samplesize) break
        } else {# surrounding the surrounding cells, clusters
          j <- j+1 # last one not added, check the next row
          }
      }  # end while statement j is number of surrounding cells to be checked
    } # end if cell selected, when not occupied, select a new random cell
    
  } # end while sample size n <= samplesize so don't start a new cluster
  samples
}

for(x in 1:10){
cluster1 <- adaptive_cluster(clru1_z[[3]], 100)
print(nrow(cluster1))
plot(clru1_z[[3]], main = "Adaptive cluster sampling (n = 100)"); points(cluster1, pch=16, cex=0.5)
}   

### Convenience sampling, weighted by sampled areas
# 1. set each convenience raster cell to the proportion of SEINet herb collections per cell as convenience
# 2. have rbinom selection of that cell probability based on convenience weights
# 3. follow the random selection 
convenience_sampling <- function(suitability.raster, samplesize){
  
}


rasterstack
```


Maxent compared to the 'true' distribution
# Maxent function  
  Background points are sampled randomly from the cells that are not NA in the first predictor variable, unless background points are specified with argument a.
```{r}
#test 
rasterstack <- rasterstack3_z
sampling_type <- "random"
pathstart <- "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Chapter1/maxent_simulations/"
true.dist <- clru1_z[[3]]
reps <- 2
# sampling_type <- "ra"
rm(rasterstack); rm(true.dist); rm(sampling_type)

maxent_sampling <- function(rasterstack, true.dist, sampling_type = c("random","systematic","convenience","cluster"),
                            reps = 100, samplesize, pathstart = pathstart){
  if(grepl(sampling_type, "random")){
    sample_xy <- random_sampling(suitability.raster = true.dist,samplesize = samplesize)
  } 
  if(grepl(sampling_type, "systematic")){
    sample_xy <- systematic_sampling(suitability.raster = true.dist, samplesize = samplesize)
  }
  if(grepl(sampling_type, "convenience")){
    sample_xy <- convenience_sampling()
  }
  if(grepl(sampling_type, "cluster")){
    sample_xy <- cluster_sampling()
  }
  
  for(r in 1:reps){
    fold <- kfold(sample_xy, k=5) # withold 20% of the sample for testing
    occtest <- sample_xy[fold == 1,]
    occtrain <- sample_xy[fold != 1,]
    xm <- maxent(x = rasterstack, p = occtrain)
    write.csv(data.frame(sample_xy, fold), paste(pathstart,"maxentOcc",sampling_type,"rep",r,".csv",sep=""))
    save(xm, file = paste(pathstart, "maxent",sampling_type,"rep",r,".Rda",sep=""))
    
    gc()
    ncores <- detectCores()-1
    cl = parallel::makeCluster(ncores)
    doParallel::registerDoParallel(cl,ncores)
    
    # compute indices for data splitting
    rows <- 1:nrow(rasterstack)
    split <- sort(rows%%ncores)+1
    outname <- paste(pathstart,"Predictmaxent",sampling_type,"rep",r,sep="")
    
    #predict with subsets of predictor dataset
    foreach(i = unique(split), .combine = c) %dopar% {
      rows_sub <- rows[split==i]
      sub <- raster::crop(rasterstack, raster::extent(rasterstack, min(rows_sub), max(rows_sub),
                                                      1, ncol(rasterstack)))
      raster::predict(sub, xm, filename = paste(outname, i, sep="_"), overwrite = TRUE)
    }
    
    # random background data
    bg <- randomPoints(rasterstack, 300)
    e <- evaluate(xm, p=occtest, a=bg, x=rasterstack)
    save(e, file = paste(pathstart, "evaluate", sampling_type,"rep",r,".Rda",sep=""))
    
    rm(xm)
    gc()
    stopCluster(cl)
  }
  
   lapply(1:reps, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste("Predictmaxent",sampling_type,"rep",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiff",r,sampling_type,"Rep",k,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
}


plot(xm)
r <- response(xm, var="bio19_12")
plot(r, type = "l")
response(xm)

```

<https://www.youtube.com/watch?v=42rSg60Rk-k&feature=youtu.be&fbclid=IwAR3AvdVB3U7YVI0aVrfm-KYGQm0Vnfo_W3udZL2YoH0h5vQjxQjp6_Kde7A>
<https://www.youtube.com/watch?v=2vgX7QoyPJo&feature=youtu.be&fbclid=IwAR2mvP8p9LDdz1LefrCwjcMMXi1l_uLI1m-1-sIiydHqHyIpMwcooSoz6lc> 

```{r}
maxent_sampling(rasterstack = rasterstack2, true.dist = clru1[[3]], sampling_type = "random", reps = 20, samplesize = 50, pathstart = pathstart)


maxent_sampling(rasterstack = rasterstack2, true.dist = clru1[[3]], sampling_type = "system", reps = 20, samplesize = 50, pathstart = pathstart, stepsize = 1000)

```

# Look at response curves 
```{r}

modelstoload <- unlist(lapply(c("random","system"), function(x){
  resultpath <- list.files(path = pathstart,
                           pattern = paste("^","maxent",x,"rep",sep=""),
                           full.names = TRUE)
  }))


load(modelstoload[1])

maxModels <- lapply(modelstoload, function(x) load(x))
response(xm) # response curve shapes
plot(xm) # variable contriubtion
```



# Nicheoverlap function    
```{r}

```


# JAGS or make a gibbs sampler?   
Just raster stack glm with the enviornmental variable relationships. 
```{r}
# The distribution of suitable values by layer
ana_rast <- stack(rasterstack3_z, clru1_z[[3]])
# x <- calc(ana_rast, function(x) if(ana_rast[[7]]>0.3) lm(x~time))

# Distribution in suitable 
for(i in names(ana_rast)[-7]){
  hist(ana_rast[[i]][ana_rast[[7]]>0.3], main=paste(i))
}
for(i in names(ana_rast)[-7]){
  hist(ana_rast[[i]][ana_rast[[7]]>0.5], main=paste(i))
}
# Take the central tendancy as alpha and the SD as var? those are the first step to estimate those parameters from the random sample? Then take the output from those as the input 

# if beta then dbeta(ElevationResampled_bioclim, $alpha, $beta, ncp = based on derivation as a Poisson mixture of betas Johnson et al 1995)
model1 <- ElevationResampled_bioclim[]*x

r_suitability <- stackApply(rasterstack3_z, indices = 1:6, function(x) )

```




