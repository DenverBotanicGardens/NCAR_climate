---
title: "Chapter1_EORtoHerbtoHerbwitherror"
author: "Michelle DePrenger-Levin"
date: "June 4, 2019"
output: html_document
---

```{r}
rm(list=ls())

```

Packages
```{r}
# Plotting
library(gridExtra)
library(grid)
library(lattice)
library(ggplot2)
library(grid) # for rectGrob
library(RCurl)

# Models
require(lme4)
require(AICcmodavg)
library(splancs)
library(Rmisc)
library(rgdal) # errors with readOGR
library(sf)
library(ENMeval)
 # devtools::install_github("ropensci/prism")
library(prism)
options(java.parameters = "Xmx26000m")
# library(rJava)

# Mapping
library(rgeos)
library(raster)
library(maptools)
library(dismo)

# Parallelization
library(foreach)
library(parallel)
library(doParallel)
library(magrittr) # pipe %>%

# Taxonomy
library(Taxonstand)

```

How many SEINet records collected?, how this was reduced to get to only colorado G1/G2
```{r}
seinet <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/seinet_colorado_with_coordinates_ALLG1G2.csv") #I must have made this one sometime, need to check again that matched all names

coloradosps <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/occurrences.csv")

nrow(coloradosps)
length(unique(coloradosps$scientificName))

nrow(seinet)
min(seinet$year, na.rm = TRUE)

```


Functions
```{r}

ScaleBar <- function(reference_raster_utm, round_to_nearest_km, width_percent, y_percent_from_bottom, x_percent_from_left, y_text_percent_from_bottom, ...) {
    # Round by max to nearest... e.g. 5 km 
    mround <- function(x,base){ 
        base*round(x/base) 
    }   
    # scale bar size adjustment to avoid decimals
        scale_size <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*width_percent)/1000
        scale_size_adj <- mround(scale_size, round_to_nearest_km)
        scale_size_adj_plot <- (scale_size_adj*1000)/2
    # Horizontal percent position (x) for scale bar
        x_position <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*x_percent_from_left)+xmin(reference_raster_utm)
    # Vertical percent position y for scale bar
        y_position <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_percent_from_bottom)+ymin(reference_raster_utm)
        y_position_text <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_text_percent_from_bottom)+ymin(reference_raster_utm)
    # Draw line on plot
        library(sp)
        x_ends <- c((x_position-scale_size_adj_plot), (x_position+scale_size_adj_plot))
        y_ends <- c((y_position), (y_position))
        scale_bar_line <- SpatialLines(list(Lines(Line(cbind(x_ends, y_ends)), ID="length")))
        projection(scale_bar_line) <- projection(reference_raster_utm)
        plot(scale_bar_line, add=TRUE, ...)
        text(x_position, y_position_text, paste0(scale_size_adj, "km"))
}

thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}
```

Raw data
```{r}
# This shapefile got corrupted
# colocounties <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties_wgs84")
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")
colocounties <- spTransform(colocounties.UTM,CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

# colocounties <- st_read("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties/counties_wgs84.shp")
# colocounties.UTM <- st_read("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties/counties.shp")


load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2namesall68.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/coloradosps.g1g2_168.Rda")
l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")
l1G1G2and <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2","G1G2"),])
l1G1G2and$PolyID <- do.call(rbind, lapply(split(l1G1G2and,l1G1G2and$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))

# Predictor layers (DEM layers resampled to match at Bioclim 30 arc second scale)
coElev_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/ElevationResampled_bioclim.tif")
coAspect_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/AspectResampled_bioclim.tif")
# COplus_ruggedInt50.tif
coRugged_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/RuggedResampled_bioclim.tif")
bio1_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio1Resampled_bioclim.tif")
bio12_res <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/Bio12Resampled_bioclim.tif")
rasterstack <- stack(coElev_res,coAspect_res,coRugged_res,bio1_res,bio12_res)
rm(bio1_res,bio12_res,coAspect_res,coElev_res,coRugged_res)
gc()
```

Grid cells are around 1km anyway so round to about 1.1 km
```{r}
coloradosps.g1g2_168$decimalLatitude <- as.numeric(as.character(coloradosps.g1g2_168$decimalLatitude))
coloradosps.g1g2_168$decimalLongitude <- as.numeric(as.character(coloradosps.g1g2_168$decimalLongitude))
coloradosps.g1g2_168$year <- as.numeric(as.character(coloradosps.g1g2_168$year))

coloradosps.g1g2_168$lon2 <- round(coloradosps.g1g2_168$decimalLongitude, 2)
coloradosps.g1g2_168$lat2 <- round(coloradosps.g1g2_168$decimalLatitude, 2)

length(table(coloradosps.g1g2_168$lon2)) # 325 
length(table(coloradosps.g1g2_168$decimalLongitude)) # 870

# Number of herbaria
length(unique(coloradosps.g1g2_168$institutionCode))
```



Points for SDMs    
       1. EOs sample size matching Herbarium specimens sample size, use thin.max
       2. Herbarium specimens as-is
       3. Herbarium specimens with additional error (to be created from distance distribution) 
       4. Combined EOR and herbarium records of same sample size
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_EOR.Rda")
```
or make new: 
```{r}

distXsp_EOR <- lapply(1:nrow(g1g2namesall68), function(i){
  gc()
  g1g2now <- coloradosps.g1g2_168[coloradosps.g1g2_168$scientificName %in%
                                c(g1g2namesall68$Taxon[i],g1g2namesall68$AcceptedName[i])&
                                !is.na(coloradosps.g1g2_168$decimalLatitude),]
  g1g2now$year <- as.numeric(as.character(substring(g1g2now$year,1,4)))
   # remove points with errors in the year field
  g1g2now <- g1g2now[g1g2now$year>1800,]
  
  g1g2now$decimalLatitude <- as.numeric(as.character(g1g2now$decimalLatitude))
  g1g2now$decimalLongitude <- as.numeric(as.character(g1g2now$decimalLongitude))
  # remove points not in Colorado
  g1g2now <- g1g2now[g1g2now$decimalLatitude>35,]
  g1g2now <- g1g2now[g1g2now$decimalLongitude>(-113),]
  # discard observations that lack specimens
  g1g2now <- g1g2now[grepl("Specimen",g1g2now$basisOfRecord),]
  g1g2now <- g1g2now[,c("scientificName","scientificNameAuthorship","institutionCode", 
                        "recordedBy","year","decimalLatitude","decimalLongitude")] # ,"lat2","lon2"
  # remove duplicate locations
  g1g2now$lonRounded <- round(g1g2now$decimalLongitude,4) # from stackexchange "The fourth decimal place is worth up to 11 m: it can identify a parcel of land. It is comparable to the typical accuracy of an uncorrected GPS unit with no interference."
  g1g2now$latRounded <- round(g1g2now$decimalLatitude,4)
  # reverse sort so the newest is kept when duplicates, not dinged for one duplicate being before 1980
  g1g2now <- g1g2now[order(g1g2now$year, decreasing = TRUE),]
  g1g2now <- g1g2now[!duplicated(g1g2now[c("latRounded","lonRounded")]),]
  gc()
  
  
  if(nrow(g1g2now)>0){
  # put grid points across colorado, sample from it for each polygons
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                        g1g2namesall68$Taxon[i]),] 
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
    totalarea <- sum(area(disaggregate(polys)))
  # area of each polygon
    # areanow <- sapply(slot(polys,"polygons"), slot, "area")
  

  # Convert polys to match rasterstack; only those mapped after 1980
    polys@data$LASTOBS <- as.numeric(substring(polys@data$LASTOBS,1,4))
    polys <- polys[polys@data$LASTOBS < 2020,] # when keeping all polys, need to remove unknown year
    polys_after1980 <- polys[polys$LASTOBS > 1980 & polys$LASTOBS < 2020,] # and not unknown 9999
    polys_latlon <- spTransform(polys, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

    # Get cell number of where the herbarium specimens fall; for records after 1980; sample size reduced if fall within same raster cell based on the ca. 1 km size of the climate variables
      Herbcell <- cellFromXY(rasterstack, g1g2now[g1g2now$year>1980
                                                  ,c("decimalLongitude","decimalLatitude")])

    # Get cell number
      cell <- cellFromPolygon(rasterstack, polys_latlon, weights =TRUE)
      EORpnts <- rasterToPoints(rasterstack[[1]])[unique(as.vector(do.call(rbind,cell)[,1])),c(1:2)]
    # Thin to same number of points available in Herbarium specimens
      EORpnts <- thin.max(EORpnts,c("x","y"),length(unique(Herbcell)))
        
    gc()
    
    
    # Combine the two and thin to Herbcell, make sure no duplicate points
    combined <- rbind(EORpnts, data.frame(x = g1g2now$lonRounded, y = g1g2now$latRounded))
    combined <- combined[!duplicated(combined[,c("x","y")]),]
    combined <- thin.max(combined, c("x","y"),length(unique(Herbcell)))
        
    
  #turn into spatialpointsdataframe for individual species
    coordinates(g1g2now) <- ~decimalLongitude+decimalLatitude
    proj4string(g1g2now) <- CRS("+init=epsg:4326")  # CRS("+proj=longlat +datum=WGS84")
    g1g2now <- spTransform(g1g2now, CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
  # The minimum distance from each point to the nearest polygon
    distnow <- apply(gDistance(g1g2now,polys, byid=TRUE),2,min)
    gc()
    
    g1g2distarea <- data.frame(Species = g1g2namesall68$AcceptedName[i], 
                               g1g2now,Dist = distnow, Area = totalarea,
                               EORdate = polys$LASTOBS[apply(gDistance(g1g2now,polys,
                                                                       byid=TRUE),2,which.min)],
                               Yeardiff = g1g2now$year- as.numeric(substring(as.character(polys$LASTOBS[apply(gDistance(g1g2now,polys, byid=TRUE),2,which.min)]),1,4)))
    gc()
    
    out <- list(g1g2distarea, EORpnts, combined)
    gc()
    out
    }
  })
save(distXsp_EOR, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_EOR.Rda") 


# Have now made a list of species each has a list of (1) the herbarium specimens for that species 
head(distXsp_EOR[[1]][[1]])

#  distXsp_EOR[[1]][[1]]$decimalLongitude,
checkyears <- colSums(data.frame(matrix(table(distXsp_EOR[[1]][[1]]$decimalLatitude, distXsp_EOR[[1]][[1]]$year), 
                          ncol = length(unique(distXsp_EOR[[1]][[1]]$year)) ), check.names = FALSE) )   # 
names(checkyears) <-  sort(unique(distXsp_EOR[[1]][[1]]$year))

# and (2) a list of points within the EORs that approximatly matches the number of herbarium records after 1980. or matches after get rid of duplicates, and only has one per raster cell
head(distXsp_EOR[[1]][[2]])
nrow(distXsp_EOR[[1]][[2]])


head(distXsp_EOR[[1]][[3]])
nrow(distXsp_EOR[[1]][[3]])
```



#Distribution of error in herbarium specimens 
    
    1) Habitat Specificity
        i.  Table 1 of distance, area, year, year differences
        ii. AIC to compare habitt specificity, range, year, difference in years EOR to herbarium on distance from nearest EOR     
        
```{r}
notnull <- which(!unlist(lapply(distXsp_EOR, is.null)))
length(notnull)
match(1:68,notnull)
atleast12 <- unlist(lapply(notnull, function(l){
  if(nrow(distXsp_EOR[[l]][[2]])>11) l
  }))

df_dist <- do.call(rbind,Map(cbind, mapply('[[', distXsp_EOR, 1)[notnull], SpNum = notnull))
df_dist$EORYear <- as.numeric(substring(df_dist$EORdate,1,4))

out <- Rmisc::summarySE(df_dist, "Dist", "SpNum", na.rm=TRUE)
tab1 <- do.call(rbind, lapply(split(df_dist, df_dist$SpNum), function(x){
  data.frame(SpNum1 = unique(x$SpNum), Species = x$Species[1], DistAvg = summarySE(x, "Dist"), 
             Area = unique(x$Area), minYearDiff = min(x$Yeardiff), maxYearDiff = max(x$Yeardiff),
             SpNum= unique(x$SpNum), YearDiffAvg = summarySE(x, "Yeardiff"),
             MinEORyear = min(x$EORYear), MaxEORyear = max(x$EORYear), 
             MinYear = min(x$year), MaxYear = max(x$year), AvgLat = summarySE(x, "decimalLatitude"))
})) 
tab1$SDM[tab1$SpNum %in% atleast12] <- "x"

tab1[!is.na(tab1$SDM), c("SpNum1","Species","SDM")]

# write.csv(tab1, "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/table1_20190804.csv")
write.csv(tab1, "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Uncertainty data source Chapter XX/table1_20190216.csv")


```


#Maxent function
```{r}
maxentrun <- function(whichones, spatialpointsdataframe_herb, 
                      Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
                      numberofReps = 10, 
                      maxentarguments = FALSE, predictorvariables = rasterstack, 
                      pathstart, filenames, kfoldnum = 4, 
                      error = FALSE, distdistribution = NULL, backgroundscale = 5000){

        for(x in whichones){
          pointsspdf <- SpatialPointsDataFrame(coords = spatialpointsdataframe_herb[[x]][,c("decimalLongitude",
                                                                                            "decimalLatitude")],
                                               data = spatialpointsdataframe_herb[[x]],
                                               proj4string = Whichproj4string)
          
          if(error == TRUE){
               # for each point I will draw a circle of size (drawn from the distribution of error seen distXspall$Dist) and then pick a random point along the circle.
              errorpointsout <- do.call(rbind,lapply(1:nrow(pointsspdf), function(r){
                errordist <- sample(distdistribution, 1)
                if(errordist>0){
                  erroraround <- gBuffer(pointsspdf[r,], width=errordist)
                  newpoint <- erroraround@polygons[[1]]@Polygons[[1]]@coords
                  out <- newpoint[sample(1:nrow(newpoint),1),]
                } else {
                  out <- pointsspdf@coords[r,]
                  }
                out
                }))
                
                df <- SpatialPointsDataFrame(coords = errorpointsout,
                                             data = pointsspdf@data,
                                             proj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
                circlesout <- circles(df, d = backgroundscale) #Should be 5km around
                polygns <- polygons(circlesout)
                bgpnts <- spsample(polygns, 300, "stratified") # one single random location in each 'cell' 
                
                convertxy <- spTransform(df, CRS("+proj=longlat +datum=WGS84"))
                proj4string(bgpnts) <- Whichproj4string
                bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
          } else {
                convertxy <- spTransform(pointsspdf, CRS("+proj=longlat +datum=WGS84"))
                circlesout <- circles(pointsspdf, d = backgroundscale)
                polygns <- polygons(circlesout)
                bgpnts <- spsample(polygns, 300, "stratified")
                proj4string(bgpnts) <- Whichproj4string
                bgpnts <- spTransform(bgpnts, CRS("+init=epsg:4326") )
            
          }
            
             for(rep in 1:numberofReps){
                convertxy$kfold <- kfold(convertxy, k=kfoldnum) # to have 75:25%
                if(maxentarguments == TRUE){
                  xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords,
                           args=c("noautofeature","noproduct","nothreshold"))
                } else {
                  xm <- maxent(x = predictorvariables,p = convertxy[convertxy$kfold!=1,], a = bgpnts@coords)
                }
                
                write.csv(data.frame(convertxy@coords,convertxy@data),
                          paste(pathstart,"presenceHerb",filenames,"Sp",x,"kfold",rep,".csv", sep=""))
                # write.csv(data.frame(bgpnts@coords, convertxy@data[1,c("Species","Area")]),
                #           paste(pathstart,"presenceHerb",filenames,"Sp",x,"kfold",rep,".csv", sep=""))
                save(xm, file= paste(pathstart,"maxentHerb",filenames,"Sp",x,"kfold",rep,".Rda", sep=""))        
        
                gc()
                #register parallel computing backend
                ncores <- detectCores()-1
                cl = parallel::makeCluster(ncores)
                doParallel::registerDoParallel(cl,ncores)
                #compute indices for data splitting
                rows = 1:nrow(predictorvariables)
                split = sort(rows%%ncores)+1
                outname = paste(pathstart,"PredictHerb",filenames,"Sp", x,"kfold",rep, sep="")
                #perform the prediction on subsets of the predictor dataset
                foreach(i=unique(split), .combine=c)%dopar%{
                  rows_sub = rows[split==i]
                  sub = raster::crop(predictorvariables,raster::extent(predictorvariables, min(rows_sub), max(rows_sub), 
                                                                1, ncol(predictorvariables)))
                  raster::predict(sub, xm, filename=paste(outname, i, sep="_"), overwrite=TRUE)
                }
        
                e <- evaluate(convertxy[convertxy$kfold==1,], bgpnts, xm, predictorvariables)
                save(e, file= paste(pathstart,"evaluateHerb",filenames,"Sp",x,"kfold",rep,".Rda", sep=""))
        
                rm(xm)
                gc()
                stopCluster(cl)
             }
                e
        }
}


stitchtogether <- function(whichones, pathstart, patternmatch, rasternames){
  lapply(whichones, function(i){
  gc()
  lapply(1:10, function(k){
    resultpath <- list.files(path = pathstart, 
                             pattern = paste(patternmatch,i,"kfold",k,"_",sep=""), 
                             full.names=TRUE)
    rastout <- lapply(resultpath, function(x){
      raster(x)
      })
    rastout$filename <- paste(pathstart,"ProbTiffSp",i,rasternames,k,".tif", sep="")
    rastout$overwrite <- TRUE
    m <- do.call(merge, rastout)
  })
})
}

forhistofaverage <- function(whichones = atleast12, pathstart, patternmatch){
  stacktoaverage <- lapply(whichones, function(i){
    rasterstoavg <- list.files(path = pathstart, 
                               pattern = paste("ProbTiffSp",
                                               i,
                                               patternmatch,sep=""), 
                               full.names=TRUE)
    ras <- stack(lapply(rasterstoavg, function(y){
      raster(y)
    }))
    beginCluster(10)
    ras.mean <- clusterR(ras, calc, args=list(mean, na.rm=T))
    writeRaster(ras.mean, paste(pathstart,
                                "AvgTiffSp",i,patternmatch,g1g2namesall68$AcceptedName[i],
                                ".tif", sep=""),overwrite=TRUE)
    gc()
    endCluster()
    ras.mean
  })
  stacktoaverage
}


forhistofsd <- function(whichones = atleast12, pathstart, patternmatch){
  stacktoaverage <- lapply(whichones, function(i){
    rasterstoavg <- list.files(path = pathstart, 
                               pattern = paste("ProbTiffSp",
                                               i,
                                               patternmatch,sep=""), 
                               full.names=TRUE)
    ras <- stack(lapply(rasterstoavg, function(y){
      raster(y)
    }))
    beginCluster(10)
    ras.mean <- clusterR(ras, calc, args=list(sd, na.rm=T))
    writeRaster(ras.mean, paste(pathstart, "SDTiffSp",i,patternmatch,
                              g1g2namesall68$AcceptedName[i],
                              ".tif", sep=""),overwrite=TRUE)
    gc()
    endCluster()
    ras.mean
  })
  stacktoaverage
}


habitatSpecificity <- function(whichones, pathstart, replicates = 10, filenames, rasterstack){
  
  # collect all the presence points used in the SDM
  habsp <- do.call(rbind,lapply(whichones, function(x){
    out <- do.call(rbind,lapply(filenames, function(nams){
      outno <- do.call(rbind,lapply(1:replicates, function(rep){
        load(paste(pathstart,"maxentHerb", nams, "Sp", x,"kfold",rep, ".Rda",sep=""))
        outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = nams, xm@presence)
        outinner
        }))
      outno      
    }))
    out
  }))

  prPCA <- princomp(habsp[,-c(1:3)])
  allsphabsp <- data.frame(habsp[,1:3],prPCA$scores)
  allsphabsp$HerbType <- as.character(allsphabsp$HerbType)
  
  # for all species and both error and no and EOR, dropping parameters (kfolds in EOR; only 1)
  habspecificity <- do.call(rbind,
                            lapply(split(allsphabsp, 
                                         list(allsphabsp$SpeciesNum,
                                              allsphabsp$kfold,
                                              allsphabsp$HerbType), drop=TRUE), function(x){
      p <- ggplot(x, aes(Comp.1,Comp.2))+
      geom_point()+
      stat_ellipse(segments=201) #default is to draw 51 line segments to make the ellipse
    # get ellipse coordinates
    pb <- ggplot_build(p)
    table(pb$data[[2]]$group)
    el <- pb$data[[2]][c("x","y")]
    
    # Center of ellipse
    ctr <- MASS::cov.trob(el)$center
    
    # Distance to center from each point on ellipse
    dist2center <- sqrt(rowSums((t(t(el)-ctr))^2))
    
    # Area of ellipse from semi-major and semi-minor axes which are largest and smallest of dist to center
    habitatspecificity <- pi*min(dist2center)*max(dist2center)
    ellipseoutfold <- data.frame(SpeciesNum = unique(x$SpeciesNum), 
                                 kfold = unique(x$kfold), 
                                 HerbType = unique(x$HerbType), habspec = habitatspecificity)
    ellipseoutfold
        }))
  habspecificity
}

# probmapnames = "Herb", filename, "kfold" ; i.e. 'rasternames' below
accuracyhists <- function(pathstart, probmapnames, whichones, reps = 1:10){
  lapply(whichones, function(i){
    lapply(1:length(probmapnames), function(p){
  lrhist <- lapply(reps, function(k){
    r <- raster(paste(pathstart,"ProbTiffSp",i,probmapnames[p],k,".tif", sep=""))
    
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                    g1g2namesall68$Taxon[i]),] 
    # Only want ones after 1980
    years <- as.numeric(substring(polys$LASTOBS,1,4))
    yearlater1980 <- c()
    for(l in 1:length(years)){
      yearlater1980[l] <- if(years[l]>1980 & years[l]<2020){ # there are 4 unknown year 9999
                                      l } else {
                                        NA
                                      }
    }
    yearlater1980 <- yearlater1980[!is.na(yearlater1980)]
    polys <- polys[yearlater1980,]
    
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
    f <- spTransform(polys, CRS("+proj=longlat +datum=WGS84"))   
    fr <- extract(r, f, small = TRUE, weights = TRUE)
    # Want the value [,1] for each polygon but use the weight [,2] to get the pro-rated cells 
    out <- do.call(rbind,fr)
    out
    })
  lrhist
  })
  })
}

# Need to label type 
accuracyhistsOneperSp <- function(pathstart, whichones = atleast12, SDorAVG = "Avg", filenames){
  lapply(whichones, function(i){
    datatype <- do.call(rbind,lapply(filenames, function(file){
    resultpath <- list.files(path = pathstart, 
                         pattern = paste(SDorAVG,"TiffSp",i,"Herb",file,
                                         g1g2namesall68$AcceptedName[i], sep=""), 
                         full.names=TRUE)
    
    r <- raster(resultpath)
    polys <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[i],
                                              g1g2namesall68$Taxon[i]),] 
    # Only want ones after 1980
    years <- as.numeric(substring(polys$LASTOBS,1,4))
    yearlater1980 <- c()
    for(l in 1:length(years)){
      yearlater1980[l] <- if(years[l]>1980 & years[l]<2020){
                                      l } else {
                                        NA
                                      }
    }
    yearlater1980 <- yearlater1980[!is.na(yearlater1980)]
    polys <- polys[yearlater1980,]
    
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
    f <- spTransform(polys, CRS("+proj=longlat +datum=WGS84"))   
    fr <- extract(r, f, small = TRUE, weights = TRUE)
    # Want the value [,1] for each polygon but use the weight [,2] to get the pro-rated cells 
    out <- do.call(rbind,fr)
    out <- as.data.frame(out)
    out$DataType <- file
    out$SpNum <- i
    out$AcceptedName <- g1g2namesall68$AcceptedName[i]
    out
    }))
    datatype
  })
}
```

1. Select background points trying to account for biased sampling (would have sampled within area where presence points were)  
2. Select background points from larger area   
3. Select background points ramdomly from entire possible area    
```{r}
# Selected polygons mapped after 1980, found overlapping raster cells for points; thinned to matching number of points from herbarium specimens (the number of raster cells covered by herbarium specimens collected after 1980). May lead to repeated points from EOR records. got rid of duplicated rows in combined
distXsp_justEOR_forspdf <- mapply('[[', distXsp_EOR, 2)
distXsp_justEOR_spdf <- lapply(1:68, function(x) NA)
for(i in atleast12){
  out <- data.frame(distXsp_justEOR_forspdf[[i]])
  colnames(out) <- c("decimalLongitude","decimalLatitude")
  distXsp_justEOR_spdf[[i]] <- out
}

distdistribution <- do.call(rbind, mapply('[[', distXsp_EOR, 1))
hist(distdistribution$Dist/1000, breaks=50,
     # main="Error distribution (km)",
     main = "",
     xlab = "km") 

# make list of the combined datasets
distXsp_combined_forspdf <- mapply('[[', distXsp_EOR, 3)
distXsp_combined_spdf <- lapply(1:68, function(x) NA)
for(i in atleast12){
  out <- data.frame(distXsp_combined_forspdf[[i]])
  colnames(out) <- c("decimalLongitude","decimalLatitude")
  distXsp_combined_spdf[[i]] <- out
}

# Herbarium specimens after 1980
distXsp_justHerb <- mapply('[[', distXsp_EOR, 1)
distXsp_justHerb <- lapply(distXsp_justHerb, function(x){
  out <- x[x$year > 1980,]
  out
})
```


```{r}
 Sp <- 8
# Make a plot for the presentation
polySp2 <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[Sp],
                                        g1g2namesall68$Taxon[Sp]),]
proj4string(polySp2) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
toplot <- spTransform(polySp2,
                      CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

g1g2now <- coloradosps.g1g2_168[coloradosps.g1g2_168$scientificName %in%
                                c(g1g2namesall68$Taxon[Sp],g1g2namesall68$AcceptedName[Sp])&
                                !is.na(coloradosps.g1g2_168$decimalLatitude),]
g1g2now$year <- as.numeric(as.character(substring(g1g2now$year,1,4)))
 # remove points with errors in the year field
g1g2now <- g1g2now[g1g2now$year>1800,]

g1g2now$decimalLatitude <- as.numeric(as.character(g1g2now$decimalLatitude))
g1g2now$decimalLongitude <- as.numeric(as.character(g1g2now$decimalLongitude))
# remove points not in Colorado
g1g2now <- g1g2now[g1g2now$decimalLatitude>35,]
g1g2now <- g1g2now[g1g2now$decimalLongitude>(-113),]
# discard observations that lack specimens
g1g2now <- g1g2now[grepl("Specimen",g1g2now$basisOfRecord),]
g1g2now <- g1g2now[,c("scientificName","scientificNameAuthorship","institutionCode", 
                        "recordedBy","year","decimalLatitude","decimalLongitude")] # ,"lat2","lon2"

Sp <- 8
coloradosps.g1g2_168[coloradosps.g1g2_168$scientificName %in%
                                c(g1g2namesall68$Taxon[Sp],g1g2namesall68$AcceptedName[Sp])&
                                !is.na(coloradosps.g1g2_168$decimalLatitude),]
```


# (1) AIC for distance distribution   
```{r}
f1 <- lm(Dist ~ year + Area, data=df_dist) # range and collection year
f2 <- lm(Dist ~ year, data= df_dist) # when herbarium collected
f3 <- lm(Dist ~ Area, data= df_dist) #x is the area of the species known range
f4 <- lm(Dist ~ 1, data= df_dist) # null model
f5 <- lm(Dist ~ year*Area, data= df_dist) #year given the range
f6 <- lm(Dist ~ Yeardiff, data= df_dist) # range shift as years between herbarium and field mapped
f7 <- lm(Dist ~ Yeardiff*Area, data= df_dist) # range shift as years between herbarium and field mapped given species range size

aictab(list(f1))

formulas <- list(f1,f2,f3,f4,f5,f6,f7)

(lmresults <- aictab(formulas,
       modnames=as.character(unlist(lapply(formulas,formula)))))
evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))))

evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[1])


evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[3])

sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})
```

Figure 2    
```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure2_v3_20190805.jpg",
     width=225, height=105,units='mm', res=300)
layout(matrix(c(1,2,3), 1,3))
plot(df_dist$Area,
     log(df_dist$Dist+1),
     xlab=expression(paste("Species Range km" ^2)),
     ylab=expression(paste(km["log"])),
     pch=16, cex= .5, xaxt="n") # , yaxt="n"
     # axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
     axis(1, at=c(0,10,20,30,40,50,60,70,80,90)*(1000^2), labels=c(0,10,20,30,40,50,60,70,80,90))
abline(lm(log(Dist+1)~Area, data=df_dist))
mtext("a)", side=3, line=0, adj=0)
plot(jitter(df_dist$year ),
     log(df_dist$Dist+1),
     xlab="Year",
     ylab="",
     pch=16, cex= .25) # , yaxt="n")
     # axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
abline(lm(log(Dist+1)~year, data=df_dist))
mtext("b)", side=3, line=0, adj=0)
plot(jitter(abs(df_dist$Yeardiff) ),
     log(df_dist$Dist),
     xlab="Difference in collection year and mapped year",
     ylab="",
     pch=16, cex= .25) # , yaxt="n")
     # axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
abline(lm(log(Dist+1)~abs(Yeardiff), data=df_dist))
mtext("c)", side=3, line=0, adj=0)
dev.off()

summary(lm(df_dist$Dist ~ df_dist$Area))

summary(lm(df_dist$Dist ~ df_dist$year))

summary(lm(df_dist$Dist ~ abs(df_dist$Yeardiff)))
```



# (2) Maxent (with stitchtogether(), should be able to delete all the partial files (i.e. "PredictHerbAs-Is_bg5000Sp30kfold4_7.gri" and "PredictHerbAs-Is_bg5000Sp30kfold4_7.grd"))

Run maxent for the three datasets - EOR, As-IS, and Error; all with background of 5000 meters
```{r}
filenames <-  c("EOR_bg5000","As-Is_bg5000","Error_bg5000","Combined_bg5000")
pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error/")

# Sample size will be at least 12 except for some where there were not enough mapped polygons to cover 12 different raster cells, in this case there are repeated points. 
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"),
          spatialpointsdataframe_herb = distXsp_justEOR_spdf,
          maxentarguments = FALSE, filenames = filenames[1], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 5000)

# Herbarium as-is
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenames[2], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 5000)


# Herbarium Error
maxentrun(whichones = atleast12, numberofReps = 10, error = TRUE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          distdistribution = distdistribution$Dist,
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenames[3], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 5000)


# Combined herb and error distXsp_EOR[[]][[3]]
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"),
          spatialpointsdataframe_herb = distXsp_combined_spdf,
          maxentarguments = FALSE, filenames = filenames[4], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 5000)
```

The patternmatch needs to be "PredictHerb" filename from maxentrun() and "Sp" 
The rasternames are "Herb" and filenames
```{r}
patternmatch <- paste("PredictHerb", filenames, "Sp", sep="")
rasternames <- paste("Herb", filenames, "kfold", sep="")

lapply(c(1:3), function(x) stitchtogether(atleast12, pathstart=pathstart, patternmatch = patternmatch[x], rasternames = rasternames[x]))

# add in the combined dataset
stitchtogether(atleast12, pathstart = pathstart, patternmatch = patternmatch[4], rasternames = rasternames[4])

```


```{r}

# There are too few points, they overlap in the raster cells for the EOR points for species 8
for(i in atleast12){
  for(k in 1:10){
    load(paste(pathstart,"maxentHerb", filenames[1], "Sp",i,"kfold",k,".Rda",sep=""))
    if(nrow(xm@presence)<10){
        print(paste("Species is number:", i, "and kfold", k))
        print(nrow(xm@presence))
    }
    rm(xm)
    }
}

# i<-1;k<-1;f<-filenames[2]
# rm(i);rm(k);rm(f)

# Get the area as defined by the number of cell occupied for each dataset
# maxentrun does kfold = 4, takes out k=1 for p
CellsOccupied <- do.call(rbind,lapply(atleast12, function(i){
  
  numCells <- do.call(rbind,lapply(1:10, function(k){
    
    spFile <- do.call(rbind,lapply(filenames[c(1:2,4)], function(f){
      pr <- read.csv(paste(pathstart,"presenceHerb", f, "Sp",i,"kfold",k,".csv",sep=""))
      cells <- length(unique(cellFromXY(rasterstack, pr[pr$kfold!=1,c("decimalLongitude","decimalLatitude")]))) # lat.1 are UTM in herbarium records
      out <- data.frame(CellsOccupied = cells, Dataset = f, rep = k, SpNum = i, Species = g1g2namesall68$AcceptedName[i])
      out
      }))
    
    spFile
    }))
  
  numCells
}))

ggplot(CellsOccupied, aes(Dataset, CellsOccupied))+
  geom_boxplot()
```

## Somehow forgot how to read lm output, help!
```{r}
tab1_cells <- merge(tab1, CellsOccupied, by.x = "SpNum1", by.y = "SpNum")

table(tab1_cells$Dataset)
tab1_cells[tab1_cells$Dataset=="EOR_bg5000",]

# The cells covered does not change much by dataset and only slightly declines with range covered. 
ggplot(tab1_cells, aes(log(Area),CellsOccupied, colour = Dataset))+
  geom_jitter(width = 0.2)+
  stat_smooth(method="lm")+
  theme_bw()

cellslm1 <- lm(CellsOccupied~log(Area)*Dataset, tab1_cells)
# (1) intercept, (2) intercept for EOR 1+2 and slope of EOR 2 (3) intercept Herb 1+3, (4) intercept of combined 1+4, (5) slope effect offset of herb 2+3+ 5  (6)  slope effect offset of combined (From EOR) 1+2+4+6
summary(cellslm1) 

# Matters when unique cells are added. 
# Call:
# lm(formula = CellsOccupied ~ log(Area) * Dataset, data = tab1_cells)
# 
# Residuals:
#     Min      1Q  Median      3Q     Max 
# -14.022  -7.771  -4.120   6.307  50.510 
# 
# Coefficients:
#                                  Estimate Std. Error t value Pr(>|t|)  
# (Intercept)                       12.2372     7.0603   1.733   0.0835 .   <- interecpt of EOR
# log(Area)                          0.4349     0.4440   0.980   0.3276     <- the slope of EOR
# DatasetAs-Is_bg5000               23.0910     9.9848   2.313   0.0211 *   <- intercept of Herb (EOR + Herb)
# DatasetCombined_bg5000            20.8482     9.9848   2.088   0.0372 *   <- intercept of Comb (EOR + comb)
# log(Area):DatasetAs-Is_bg5000     -1.3578     0.6279  -2.163   0.0309 *   <- the slope of Herb (EOR + log(Area):DatasetAs-Is...)
# log(Area):DatasetCombined_bg5000  -1.2717     0.6279  -2.025   0.0432 *   <- the slope of combined (EOR + comb)
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 11.84 on 654 degrees of freedom
# Multiple R-squared:  0.01645,	Adjusted R-squared:  0.00893 
# F-statistic: 2.188 on 5 and 654 DF,  p-value: 0.05398


# visualize with a model.matrix
areas <- unique(tab1_cells$Area)
Datasets <- factor(unique(tab1_cells$Dataset))
model.df <- expand.grid(areas,Datasets)
model.matrix(~Var1*Var2, data=model.df) # var1 is area, var2 is datasets

slopeEOR <- cellslm1$coefficients[1:2] # ~0x + 19
slopeHerb <- sum(cellslm1$coefficients[c(2:3,5)]) # 2.4
slopecomb <- sum(cellslm1$coefficients[c(2,4,6)]) # 1.33
interEOR <- cellslm1$coefficients[1] # 19.06`
# interHerb <- (cellslm1$coefficients[1:2]) # no no no

# habitat specificity with combined instead of error 
# Did have "habSp_EOR_uncleaned_error" but now have habsp_smallbg for the small background and habsp_bigbg for big
habsp_smallbg <- habitatSpecificity(whichones = atleast12[atleast12 != 8], pathstart = pathstart, 
                                    replicates = 10, filenames = filenames[-3])
save(habsp_smallbg, file= paste(pathstart,"habsp_smallbg.Rda", sep=""))

ggplot(data = habsp_smallbg, aes(HerbType, habspec))+
  geom_boxplot()+
  theme_bw()

```

Run maxent for the three datasets - EOR, As-IS, and Error; all with background of 500000 meters
```{r}
filenamesbig <-  c("EOR_bg500000","As-Is_bg500000","Error_bg500000","Combined_bg500000")
pathstart <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error/")

# Sample size will be at least 12 except for some where there were not enough mapped polygons to cover 12 different raster cells, in this case there are repeated points. 
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"),
          spatialpointsdataframe_herb = distXsp_justEOR_spdf,
          maxentarguments = FALSE, filenames = filenamesbig[1], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 500000)

# Herbarium as-is
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenamesbig[2], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 500000)


# Herbarium Error
maxentrun(whichones = atleast12, numberofReps = 10, error = TRUE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          distdistribution = distdistribution$Dist,
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenamesbig[3], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 500000)

# Combined herb and error distXsp_EOR[[]][[3]] and turned into distXsp_combined_spdf renaming x and y to decimalLat...
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"),
          spatialpointsdataframe_herb = distXsp_combined_spdf,
          maxentarguments = FALSE, filenames = filenamesbig[4], 
          predictorvariables = rasterstack, pathstart = pathstart, backgroundscale = 500000)
```

The patternmatch needs to be "PredictHerb" filename from maxentrun() and "Sp" 
The rasternames are "Herb" and filenames
```{r}
patternmatchbig <- paste("PredictHerb", filenamesbig, "Sp", sep="")
rasternamesbig <- paste("Herb", filenamesbig, "kfold", sep="")

lapply(1:3, function(x) stitchtogether(atleast12, pathstart=pathstart, patternmatch = patternmatchbig[x], rasternames = rasternames[x]))

stitchtogether(atleast12, pathstart=pathstart, patternmatch = patternmatchbig[4], rasternames = rasternamesbig[4])

# habitat specificity

# habsp_bigbg <- habitatSpecificity(atleast12[atleast12 != 8], pathstart, replicates = 10, filenames = filenamesbig[-3])
# save(habsp_bigbg, file= paste(pathstart,"habsp_bigbg.Rda", sep=""))

# these only have error, nothing else, need the 21 species one that actually is 22 species
# load(paste(pathstart,"habsp_smallbg.Rda", sep=""))
# habsp_bigbg
# habspebig
# habsp_smallbg
load(paste(pathstart, "habsp_21species.Rda", sep=""))
habsp_22species

# Identical!
identical(habsp_smallbg, habsp_22species)

```



# (4) Habitat Specificity      
        a) add to table 1 for AIC with habitat specificity   
        
        
Merge habitat specificity and tab 1 to look at the impact of actual habitt specificity, not just the range of the polygons.
```{r}
# CellsOccupied from cellsfromXY of the samples so does reflect area of sample
table(habsp_22species$HerbType) # same as the three datasets and 5000 background, background doesn't matter for these measures
habspebig <- merge(habsp_22species, tab1_cells, by.x = c("SpeciesNum","HerbType"), by.y = c("SpNum1","Dataset"))
# habspesmall <- merge(habsp_smallbg, tab1, by.x = "SpeciesNum", by.y = "SpNum1")

# Not fair since area is based on EOR, of course it will better reflect 
ggplot(habspebig, aes(y = habspec, x =(Area/(1000^2)), colour = HerbType))+
  geom_point()+
  stat_smooth(method="lm")+
  theme_bw()

# So test by cells occupied by each dataset
ggplot(habspebig, aes(y = habspec, x = CellsOccupied, colour = HerbType))+
  geom_jitter(height = 5000, width = 0.5, shape=1, size=0.75)+
  stat_smooth(method="lm")+
  theme_bw()+
  scale_color_manual(values=c("slateblue", "#E69F00", "orangered"), 
                       name="Dataset",
                       breaks=c("As-Is_bg5000", "Combined_bg5000", "EOR_bg5000"),
                       labels=c("Herbarium", "Combined", "Field Survey"))
  
table(habspebig$HerbType)

ggplot(habspebig, aes(y = habspec, x = HerbType, colour=HerbType))+
  geom_boxplot()

summary(lm(habspec ~ Area*HerbType, data=habspebig))
```


```{r}

# how many points are there for each? Need enough points to make ellipse
# collect all the presence points used in the SDM
howmany <- do.call(rbind,lapply(atleast12, function(x){
  out <- do.call(rbind,lapply(filenames[-3], function(nams){
    outno <- do.call(rbind,lapply(1:10, function(rep){
      load(paste(pathstart,"maxentHerb", nams, "Sp", x,"kfold",rep, ".Rda",sep=""))
      outinner <- data.frame(SpeciesNum = x, kfold = rep, HerbType = nams, xm@presence)
      outinner
      }))
    outno      
    }))
  out
  }))
table(howmany$SpeciesNum, howmany$HerbType, howmany$kfold)

# Average sample size per species
samplesize <- apply(table(howmany$SpeciesNum, howmany$HerbType, howmany$kfold), c(1,2), mean)
samplesize <- data.frame(samplesize)
samplesize$SpNum <- row.names(samplesize)
```

Animated PCAs of all species    
```{r}
prPCA <- princomp(howmany[,-c(1:3)])
allsphabsp <- data.frame(howmany[,1:3],prPCA$scores)
allsphabsp$HerbType <- as.character(allsphabsp$HerbType)
x.Pov <- prPCA$sdev/sum(prPCA$sdev^2) # Percent of variance explained by the components
  
  plots <- lapply(split(allsphabsp,
               list(allsphabsp$SpeciesNum), #,allsphabsp$kfold), 
               drop=TRUE), function(x){
                      outplot <- ggplot(x, aes(Comp.1,Comp.2, colour=HerbType))+
                        geom_point()+
                        stat_ellipse()+
                        theme_bw()
                      outplot
                      })

length(plots)
length(unique(howmany$SpeciesNum))
plots[[1]]

library(animation)
saveGIF(
  {lapply(plots, print)}
  , "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/PCA_animate_20190805.gif"
)
```

Merge habitat specificity and tab 1 to look at the impact of actual habitt specificity, not just the range of the polygons.
Should turn all into the number of cells occupied instead of a sum of the polygon space of the EORs, of course EORs will have a relationship, the area measure comes from EORs only, does not change by the dataset used. 
```{r}
# habspe <- merge(habSp_EOR_uncleaned_error, tab1, by.x = "SpeciesNum", by.y = "SpNum1")

# update 2020.03.02
habspe <- merge(habsp_22species, tab1, by.x = "SpeciesNum", by.y = "SpNum1")

summaryHabSp <- summarySE(habsp_smallbg, "habspec", "SpeciesNum")
habspe_aic <- merge(summaryHabSp, df_dist, by.x = "SpeciesNum", by.y = "SpNum")

ggplot(habspe, aes(as.factor(round(Area/(1000^2),0)), habspec, colour=HerbType))+
  geom_jitter()+
  geom_boxplot(position=position_dodge(width=-0.5))+
  theme_bw()+
  # theme(legend.position = "none")+
  xlab(expression("Area km"^2))+
  geom_boxplot()

summarySE(habsp_smallbg, "habspec", "HerbType")
# Upper and lower CI
rowSums(summarySE(habsp_smallbg, "habspec", "HerbType")[,c(3,5)])
summarySE(habsp_smallbg, "habspec", "HerbType")[,3]-summarySE(habsp_smallbg, "habspec", "HerbType")[,5]
```

```{r}
g <- ggplotGrob(ggplot(habspe[habspe$Area<max(habspe$Area),], 
          aes(Area/(1000^2), habspec/1000, shape=HerbType, colour=HerbType))+
      geom_point(position = position_dodge(width=.25), size = 0.25)+
      stat_smooth(method="lm", se=FALSE, lwd = 0.5)+
      theme_bw()+
      scale_colour_manual(values = c("green","red","blue"))+ #  "EOR"  "NoError"   "WithError"==grey80
      # scale_colour_manual(values = c("black","grey50","goldenrod"))+ #  "EOR"  "NoError"   "WithError"==grey80
      theme(legend.position = "none")+
      xlab("")+
      ylab(""))

jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure3_v3_20190805.jpg",
     width=225, height=150,units='mm', res=300)   
ggplot(habspe, aes(Area/(1000^2), habspec/1000, shape=HerbType, colour=HerbType))+
            geom_point(position = position_dodge(width=.25))+
            stat_smooth(method="lm", se=FALSE)+
            theme_bw()+
            scale_colour_manual(values = c("green","red","blue"))+ #  "EOR"   "NoError"   "WithError"
            # scale_colour_manual(values = c("black","grey50","goldenrod"))+ #  "EOR"   "NoError"   "WithError"
            theme(legend.position = "none")+
            xlab(expression("Area km"^2))+
            ylab("Habitat specificity (size of 95% CI)")+
  annotation_custom(
    grob = g,
    xmin = 42,
    xmax = 99,
    ymin = 900,
    ymax = 2510
  )
dev.off()

```


# With habitat specificity (another measure of range, niche size as opposed to space covered/range size)
AIC
```{r} 

habspe_aic <- merge(summaryHabSp, df_dist, by.x = "SpeciesNum", by.y = "SpNum")

# Area is the best measure of how much distance error will be, is this because of environmental 'area'??
lm1 <- lm(Dist ~ year + Area, data=habspe_aic) # range and collection year
lm2 <- lm(Dist ~ year, data= habspe_aic) # when herbarium collected
lm3 <- lm(Dist ~ Area, data= habspe_aic) #x is the area of the species known range
lm4 <- lm(Dist ~ 1, data= habspe_aic) # null model
lm5 <- lm(Dist ~ year*Area, data= habspe_aic) #year given the range
lm6 <- lm(Dist ~ Yeardiff, data= habspe_aic) # range shift as years between herbarium and field mapped
lm7 <- lm(Dist ~ Yeardiff*Area, data= habspe_aic) # range shift as years between herbarium and field mapped given species range size

lm8 <- lm(Dist ~ year + habspec, data=habspe_aic) # range and collection year
lm9 <- lm(Dist ~ habspec, data= habspe_aic) #x is the area of the species known range
lm10 <- lm(Dist ~ year*habspec, data= habspe_aic) #year given the range
lm11 <- lm(Dist ~ habspec+Yeardiff, data= habspe_aic) # range shift as years between herbarium and field mapped
lm12 <- lm(Dist ~ Yeardiff*habspec, data= habspe_aic) # range shift as years between herbarium and field mapped 
lm13 <- lm(Dist ~ habspec + Area, data= habspe_aic) # range shift as years between herbarium and field mapped
lm14 <- lm(Dist ~ habspec*Area, data= habspe_aic) # range shift as years between herbarium and field mapped 

formulas <- list(lm1,lm2,lm3,lm4,lm5,lm6,lm7,lm8,lm9,lm10,lm11,lm12,lm13,lm14)
(lmresults <- aictab(formulas,
       modnames=as.character(unlist(lapply(formulas,formula)))))
evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))))

# Should be the evidence ratio, but it's something else
sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})
```
Best model is habitat specificty given area
```{r}
ggplot(habspe_aic, aes(habspec, Dist))+
  geom_point()+
  theme_bw()+
  stat_smooth(method="lm")


```

# (5) Accuracy and Precision    
        i. forhistofavg()     
        ii. forhistofsd() {This is just the SD of the kfolds, not the same as a MESS map, should just make that instead, ignore sd among kfolds for now}     
        iii. accuracyhists??
        iv.    
```{r}
lapply(filenames, function(x){
  forhistofaverage(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", x, sep=""))
  })

forhistofaverage(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", filenames[4], sep=""))
forhistofaverage(whichones = 1, pathstart = pathstart, patternmatch = paste("Herb", filenamesbig[1], sep=""))

lapply(filenames, function(x){
  forhistofsd(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", x, sep=""))
  })

forhistofsd(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", filenames, sep=""))
```

Also average and SD for the larger background area ones
```{r}
lapply(filenamesbig[-3], function(x){
  forhistofaverage(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", x, sep=""))
  })

# forhistofaverage(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", filenamesbig[4], sep=""))


lapply(filenamesbig, function(x){
  forhistofsd(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", x, sep=""))
  })

forhistofsd(whichones = atleast12, pathstart = pathstart, patternmatch = paste("Herb", filenamesbig, sep=""))
```

Compare to AUC and how AUC changes with differences and someway look at precision again  
Herb, EOR, and combined
```{r}
maxevaluates <- do.call(rbind,lapply(atleast12, function(x){
  outnow <- do.call(rbind,lapply(c(filenames[-3],filenamesbig[-3]), function(filenames){
    aucs <- do.call(rbind,lapply(1:10, function(rep){
    load(paste(pathstart,"evaluateHerb", filenames, "Sp", x, "kfold", rep, ".Rda", sep=""))
      out <- data.frame(filename = filenames, SpNum = x, AUC = e@auc)
      out
  }))
    aucs
      }))
  outnow
  }))

levels(maxevaluates$filename)
aggregate(AUC ~ filename, mean, data=maxevaluates)
aggregate(AUC ~ filename, sd, data=maxevaluates)

maxevaluates$background <- as.character("small (5 km)")
maxevaluates$background[grep("bg500000",maxevaluates$filename)] <- as.character("large (500 km)")




fig3 <- 
  ggplot(maxevaluates, aes(y = AUC, x = background, fill = filename))+
  geom_boxplot()+
  theme_bw()+
  xlab("Background extent")+
  scale_fill_manual(values = c("grey","white","grey30","grey","white","grey30"),
                    name = "Dataset / background",
                    breaks = c(paste(filenames[-3]), paste(filenamesbig[-3])),
                    labels = c("EOR / 500 km","herbarium / 500 km","combined / 500 km",
                               "EOR / 5 km",  "herbarium / 5 km",  "combined / 5 km"))
# +
#   scale_x_discrete(breaks = c(paste(filenames[-3]), paste(filenamesbig[-3])), 
#                    labels =c("","small (5 km)","","","large (500 km)",""))
#  limits = c(paste(filenames[-3]), paste(filenamesbig[-3])), 


ggsave(filename = "C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/Uncertainty data source Chapter XX/fig3_AUC.png", fig3, width = 150, height = 80, dpi = 300, units = "mm", device='png')

# effect size as the estimate and standard error? 
summary(lm(AUC~filename, data=maxevaluates))

# effect size as bootstrap difference and 95% CI
reps <- 5000
n <- nrow(maxevaluates)
set.seed(100)
boot.diff <- unlist(lapply(1:reps, function(i){
  idx <- sample(1:n, n, T) # with replacement
  boot.diff <- c(dist(tapply(maxevaluates$AUC[idx], maxevaluates$filename[idx], mean))) # difference of one before it; I need all combinations differences
  c(boot.diff)
}))

# the differences
dist(tapply(maxevaluates$AUC, maxevaluates$filename, mean))

boot.diff <- data.frame(matrix(boot.diff, ncol = 15, byrow = TRUE)) # fill by row
names(boot.diff) <- sapply(1:15, function(x){
  paste(matrix(combn(unique(maxevaluates$filename),2),nrow = 2)[,x], collapse="-")
  })

# quantiles
boot.95hdi <- apply(boot.diff, 2, function(x) quantile(x, p=c(0.025,0.95)))

# the differences
dist(tapply(maxevaluates$AUC, maxevaluates$filename, mean))
boot.95hdi


# Eta-squared effect size 
# sum of squares between/sum of squares total
summary(aov(AUC~filename, data=maxevaluates))

# distribution normal?
boot.diff.long <- melt(boot.diff)
ggplot(boot.diff.long, aes(value, colour=variable))+
  geom_density(stat="density")

ggplot(maxevaluates, aes(AUC, colour=filename))+
  geom_density(stat="density")
```

```{r}
# habsp_allbg is just repeated, all gives is the ellipse size around the points for the kfold rep, not dependant on background size

AUC_habsp <- merge(maxevaluates, habsp_smallbg, by.x = c("filename", "SpNum"), by.y = c("HerbType","SpeciesNum"))

ggplot(AUC_habsp, aes(habspec, AUC))+
  geom_point()+
  theme_bw()+
  stat_smooth(method = "lm")

ggplot(AUC_habsp, aes(habspec, AUC, colour=filename))+
  geom_point()+
  theme_bw()+
  stat_smooth(method = "lm")

summary(lm(AUC~habspec*SpNum, data = AUC_habsp))

summary(lm(AUC~habspec*filename, data = AUC_habsp))

ggplot(AUC_habsp, aes(x=filename, y=habspec))+
  geom_boxplot()+
  geom_jitter()

```

#  (6) Niche Overlap (and correlation)    
      
How correlated within EOR and how correlated across entire range  
Or really how similar are the niches derived from the different data?
```{r}

# dismo::nicheEquivalency
# dismo::nicheOverlap

correlations <- function(pathstart, filenames, rasterstack, whichones){
  lrhist <- lapply(whichones, function(i){

    for(i in atleast12){
          # How correlated are the averages? 
      r1 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[1],
                         g1g2namesall68$AcceptedName[i],".tif", sep=""))
      r2 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[2],
                         g1g2namesall68$AcceptedName[i],".tif", sep=""))
      r3 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[3],
                         g1g2namesall68$AcceptedName[i],".tif", sep=""))
      
      plot(r1)
      plot(r2)
      plot(r3)
  }
      
      # Pearson correlation measures the strength of the linear relationship between normally distriubted variables
      # Spearman rank correlation for variables not normally distributed or when the relationship is not linear. 
      # cor_1$mean should just be the average of all pixels in each layer
      cor_1 <- layerStats(stack(r1,r2,r3), 'pearson', na.rm=TRUE)
      out_cor <- cor_1$`pearson correlation coefficient`
      
      overlaps1_2 <- nicheOverlap(r1,r2,mask=FALSE, checkNegatives = FALSE)
      overlaps1_3 <- nicheOverlap(r1,r3, mask=FALSE, checkNegatives=FALSE)
      overlaps2_3 <- nicheOverlap(r2,r3, mask=FALSE, checkNegatives=FALSE)
      
      list(cor_1, out_cor, data.frame(OverlapEOR_AsIs = overlaps1_2, 
                                      OverlapEOR_comb = overlaps1_3, 
                                      OverlapAsIs_comb = overlaps2_3))
  })
  lrhist
}


```

# Create correlations and overlap values or   
Three so swap out error for combined
```{r}
smallbackgroundcor <- correlations(pathstart = pathstart, filenames = filenames[-3],
                                  whichones = atleast12)
smallbackgroundcor[[1]]

bigbackgroundcor <- correlations(pathstart = pathstart, filenames = filenamesbig[-3], 
                                 whichones = atleast12)

save(smallbackgroundcor, file= paste(pathstart,"smallbackgroundcor.Rda", sep=""))
save(bigbackgroundcor, file= paste(pathstart,"bigbackgroundcor.Rda", sep=""))
```

# load
```{r}
load(paste(pathstart,"smallbackgroundcor.Rda", sep=""))
load(paste(pathstart,"bigbackgroundcor.Rda", sep=""))

OverlapSmall <- mapply('[[', smallbackgroundcor, 3)  
OverlapSmall.df <- data.frame(t(matrix(unlist(OverlapSmall),3,22)))
colnames(OverlapSmall.df) <- dimnames(OverlapSmall)[[1]]
OverlapSmall.df$SpNum <- atleast12

OverlapBig <- mapply('[[', bigbackgroundcor, 3)
OverlapBig.df <- data.frame(t(matrix(unlist(OverlapBig),3,22)))
colnames(OverlapBig.df) <- dimnames(OverlapBig)[[1]]
OverlapBig.df$SpNum <- atleast12

OverlapSmall.df$Background <- "Small"
OverlapBig.df$Background <- "Big"

library(reshape2)
overlap_values <- melt(rbind(OverlapBig.df,OverlapSmall.df), id.vars = list("Background","SpNum"))

```

```{r}
levels(overlap_values$Background)

png("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure5_nicheOverlap.jpg",
     width=150, height=80,units='mm', res=300)


ggplot(overlap_values, aes(variable, value, fill = Background))+
  geom_boxplot()+
  theme_bw()+
  ylab("Niche Overlap")+
  xlab("")+
  scale_fill_manual(values = c("white","grey30"),
                  name = "Background",
                  breaks = c("Big","Small"),
                  labels = c("Large","Small"))+
  scale_x_discrete(breaks =c("OverlapEOR_AsIs","OverlapEOR_comb","OverlapAsIs_comb"),
                   labels =c("EOR - herbarium", "EOR - Combined", "herbarium - Combined"))
dev.off()
```

Niche overlap and correlations - 22 speices, only 21 got ellipses!  8 got dropped!
```{r}
str(smallbackgroundcor)

CorrelationSmall <- mapply('[[', smallbackgroundcor, 2) # the pearson correlation coefficients  
# pearsonSmall <- mapply('[[', mapply('[[', smallbackgroundcor, 1), 1)

CorrelationSmall # stacked the columns of the pearson correlations for each species

smallbackgroundcor[[1]][[1]]
# reconstructs them, but want the correlation by comparison
# lapply(1:22, function(x){
#   matrix(CorrelationSmall[,x], nrow=3, byrow = FALSE) # break into 3x3 matrix, fill by column
#   })

correfilenames <- unlist(lapply(filenames[-3], function(x){
  paste(x,filenames[-3])
}))

CorrelationSmall <- data.frame(CorrelationSmall)
colnames(CorrelationSmall) <- paste("spNum", atleast12)
CorrelationSmall$PearsonComparison <- correfilenames

CorrelSmallMelt <- melt(CorrelationSmall)
ggplot(CorrelSmallMelt[CorrelSmallMelt$value != 1,], aes(value, colour=PearsonComparison))+
  geom_density(stat="density")


# This is three of them the upper and lower triangle of the comparions
ggplot(CorrelSmallMelt[CorrelSmallMelt$PearsonComparison %in% correfilenames[c(2,3,6)],], aes(PearsonComparison,value))+
  geom_boxplot()+
  theme_bw()+
  # theme(axis.text.x=element_text(angle=90, hjust=1))+
  scale_x_discrete(labels =c("Herbarium - Combined","EOR - Herbarium","EOR - Combined"))
  
tapply( CorrelSmallMelt$value,CorrelSmallMelt$PearsonComparison, mean)
tapply( CorrelSmallMelt$value,CorrelSmallMelt$PearsonComparison, sd)
```



# (6) Niche overlap    
      ii. AIC     

Match with the difference in habitat specificity, area doesn't make as much sence since it will change with error, points would mean it covers a bigger area. 
Sample size? then want the summary one, habspe not habspe_aic   
summary(lm(value ~ Area*habspec, data = overvals_aic))    
summary(lm(value ~ habspec*variable, data = overvals_aic))    
summary(lm(value ~ variable*Background, data = overvals_aic))    

o1 <- lm(value ~ Area*habspec, data = overvals_aic)    
o2 <- lm(value ~ habspec*variable, data = overvals_aic)   
o3 <- lm(value ~ variable*Background, data = overvals_aic)    
o4 <- lm()     

# want to merge with habitat specificity, in overlap Background is "Big" or "Small" while habitat specificity changes with each data type and the overlap is the overlap so two of them... so depending on the starting specificity? 
```{r}
# Doesn't matter the background for this, can use only small
# difference in habspec by dataset and species, averaged
# avghabspec <- aggregate(habspec~SpeciesNum + HerbType, mean, data = habSp_EOR_uncleaned_error)
# habsp_allbg <- rbind(habsp_smallbg, habsp_bigbg)
avghabspec <- aggregate(habspec~SpeciesNum + HerbType, mean, data = habsp_smallbg)

# herbtypes <- levels(habsp_allbg$HerbType)

# took out filenames[3] that was error and swapped with filenames[4] that is combined
habspedifferences <- do.call(rbind,lapply(split(avghabspec, avghabspec$SpeciesNum), function(num){
  out <- data.frame(SpNum = unique(num$SpeciesNum),
                    OverlapEOR_AsIs = num$habspec[num$HerbType == paste(filenames)[2]]-
                      num$habspec[num$HerbType == paste(filenames)[1]],
                    OverlapAsIs_comb = num$habspec[num$HerbType == paste(filenames)[2]]-
                      num$habspec[num$HerbType == paste(filenames)[4]],
                    OverlapEOR_comb = num$habspec[num$HerbType == paste(filenames)[1]]-
                      num$habspec[num$HerbType == paste(filenames)[4]])
  out
  }))

library(reshape2)
habspediff <- melt(habspedifferences, id.vars = c("SpNum"))
names(habspediff) <- c("SpNum","variable","HabSpeDiff")

overvals <- merge(habspediff, overlap_values, by = c("SpNum","variable"))
overvals <- merge(overvals, samplesize, by = c("SpNum"))         
# EOR sample size an indiciation of how many raster cells covered by polygons

overvals <- merge(overvals, tab1, by.x = "SpNum", by.y = "SpNum1")


```

```{r}
# overvals_aic <- merge(overlap_values, habspe_aic, by.x = "SpNum", by.y = "SpeciesNum")
# overvals <- merge(overlap_values, tab1, by.x = "SpNum", by.y = "SpNum1")
# overvals <- merge(overvals, habspe)

# overvals <- overvals[!duplicated(overvals[,c("DistAvg.N","DistAvg.Dist","Area","habspec")]),]

o1 <- lm(value ~ EOR_bg5000, data = overvals) # sample size of herbarium specimens was DistAvg.N, not what was used in the SDM; that is EOR_bg5000
o2 <- lm(value ~ DistAvg.Dist, data = overvals) # the average distance herb to eor
o3 <- lm(value ~ Area, data= overvals)
o4 <- lm(value ~ HabSpeDiff, data= overvals)
o5 <- lm(value ~ Area * Background, data=overvals)
o6 <- lm(value ~ Background, data = overvals)

o7 <- lm(value ~ EOR_bg5000* Background, data = overvals) # sample size of herbarium specimens
o8 <- lm(value ~ DistAvg.Dist* Background, data = overvals) # the average distance herb to eor
o9 <- lm(value ~ EOR_bg5000*DistAvg.Dist , data= overvals)
o10 <- lm(value ~ HabSpeDiff* Background, data= overvals)
# o11 <- lm(value ~ EOR_bg5000* Background, data = overvals)
o12 <- lm(value ~ 1, data=overvals)

formulas <- list(o1,o2,o3,o4,o5,o6,o7,o8,o9,o10,o12) #  o11,
(lmresults <- aictab(formulas,
       modnames=as.character(unlist(lapply(formulas,formula)))))
evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))))

evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[5])

evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[8])

evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[10])


evidence(aictab(cand.set = formulas,
                modnames = as.character(unlist(lapply(formulas,formula)))),
         model.low = as.character(unlist(lapply(formulas,formula)))[7])


# no species #8 becuase too small for EOR

# Should be the evidence ratio, but it's something else
# sapply(1:length(lmresults$Delta_AICc), function(i){
#   exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
# })

```


```{r}
levels(overvals$variable)
overvals$HabSpeDiff

ggplot(overvals, aes(HabSpeDiff, value))+
  geom_point()+
  theme_bw()+
  stat_smooth(method = "lm")

table(overvals$variable, overvals$value)

overvals

ggplot(overvals, aes(Background, value, fill=variable))+
  geom_boxplot()+
  theme_bw()
```




# (6) Overlap
      iii. Accuracy and Precision 
      
Make new
```{r}
smallbg_accuracyoneper <- accuracyhistsOneperSp(pathstart = pathstart, whichones = atleast12,
                                                filenames = filenames)

bigbg_accuracyoneper <- accuracyhistsOneperSp(pathstart = pathstart, whichones = atleast12,
                                                filenames = filenamesbig)

save(smallbg_accuracyoneper, file=paste(pathstart,"smallbg_accuracyoneper_accuracy.Rda", sep=""))
save(bigbg_accuracyoneper, file=paste(pathstart,"bigbg_accuracyoneper_accuracy.Rda", sep=""))

# smallbg_acc <- accuracyhists(pathstart = pathstart, probmapnames = rasternames, whichones = atleast12)
# bigbg_acc <- accuracyhists(pathstart = pathstart, probmapnames = rasternamesbig, whichones = atleast12)
# 
# save(smallbg_acc, file=paste(pathstart,"smallbackground_accuracy.Rda", sep=""))
# save(bigbg_acc, file=paste(pathstart,"bigbackground_accuracy.Rda", sep=""))

smallbg_accuracyoneper[[1]]
```


 or load 
```{r}
load(paste(pathstart,"smallbg_accuracyoneper_accuracy.Rda", sep=""))
load(paste(pathstart,"bigbg_accuracyoneper_accuracy.Rda", sep=""))

smallbg_accuracyoneper[[1]]

ks.outs_sm <- 
  # for each species
  lapply(1:length(smallbg_accuracyoneper), function(i){
    # Compare each datatype (i.e. filenames or filenamesbig)
    # Compare filenames[1] (EOR) to filenames[2] (uncleaned)
    EOR_Uncleaned <- ks.test(log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[1]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[1]]),
          log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[2]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[2]]))
    
    EOR_Error <- ks.test(log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[1]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[1]]),
          log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[3]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[3]]))
    
    Uncleaned_Error <- ks.test(log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[2]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[2]]),
          log(data.frame(smallbg_accuracyoneper[[i]])$value[smallbg_accuracyoneper[[i]]$DataType == filenames[3]])+
             log(data.frame(smallbg_accuracyoneper[[i]])$weight[smallbg_accuracyoneper[[i]]$DataType == filenames[3]]))
    
    list(EOR_Uncleaned,EOR_Error,Uncleaned_Error)
    })

# different distributions?
whichdiff_EOR2Uncleaned <- sapply(1:22, function(s){
  if(ks.outs_sm[[s]][[1]]$p.value < 0.05){
    1
  } else {
    NA
  }
})

which(!unlist(lapply(whichdiff_EOR2Uncleaned, is.na))) #  [1]  4  5  6  7  8  9 10 14 15 17 20 21
length(which(!unlist(lapply(whichdiff_EOR2Uncleaned, is.na)))) # 12


whichdiff_EOR2Error <- sapply(1:22, function(s){
  if(ks.outs_sm[[s]][[2]]$p.value < 0.05){
    1
  } else {
    NA
  }
})

which(!unlist(lapply(whichdiff_EOR2Error, is.na))) # [1]  2  4  5  6  7  8  9 10 14 15 20 21
length(which(!unlist(lapply(whichdiff_EOR2Error, is.na)))) # 12


whichdiff_Uncleaned2Error <- sapply(1:22, function(s){
  if(ks.outs_sm[[s]][[3]]$p.value < 0.05){
    1
  } else {
    NA
  }
})

which(!unlist(lapply(whichdiff_Uncleaned2Error, is.na))) #  [1]  4  5  6  7  8  9 10 14 15 17 21
length(which(!unlist(lapply(whichdiff_Uncleaned2Error, is.na)))) # 11

```

main=bquote(.(g1g2namesall68$AcceptedName[i]) ~
                     .(round(unique(df_dist$Area[df_dist$SpNum==i])/(1000^2),2)) ~
                     km^2)

```{r}
# Plot the Cummulative distriubtion curves
# jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure4_accuracy_20190809.jpg",
#      width=300, height=400,units='mm', res=300)
# 
# 
# layout(matrix(c(1:12),nrow=4, byrow = TRUE))
# par(mar=c(2.1,3.1,1.1,0.1))
# for(i in which(!unlist(lapply(whichdiff_Uncleaned2Error, is.na)))){
#   dfaccuracy <- data.frame(smallbg_accuracyoneper[[i]])
# 
#   EOR <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[1]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[1]]))
#   Uncleaned <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[2]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[2]]))
#   Error <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[3]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[3]]))
#   plot(EOR, col = rgb(0,0,1,1),main = g1g2namesall68$AcceptedName[i],cex.main=2)  
#   lines(Uncleaned, col = rgb(0,1,0,0.5))
#   lines(Error, col = rgb(1,0,0,0.5))
# }
# 
# dev.off()

# Plot the Cummulative distriubtion curves for the ones where EOR differs from Error
# jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure4_accuracy_20190809_EOR2Error.jpg",
#      width=300, height=400,units='mm', res=300)


# layout(matrix(c(1:12),nrow=4, byrow = TRUE))
# par(mar=c(2.1,3.1,1.1,0.1))
# for(i in which(!unlist(lapply(whichdiff_EOR2Error, is.na)))){
#   dfaccuracy <- data.frame(smallbg_accuracyoneper[[i]])
# 
#   EOR <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[1]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[1]]))
#   Uncleaned <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[2]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[2]]))
#   Error <- ecdf(log(dfaccuracy$value[dfaccuracy$DataType == filenames[3]])+
#                   log(dfaccuracy$weight[dfaccuracy$DataType == filenames[3]]))
#   plot(EOR, col = "blue",main = g1g2namesall68$AcceptedName[i],cex.main=2)  
#   lines(Uncleaned, col = rgb(0,1,0,0.5))
#   lines(Error, col = rgb(1,0,0,0.5))
# }
# 
# dev.off()

```

Instead of an empirical cumulative distribution function plot, more of a histogram by values? 
```{r}
# weights are 0 to 1


library(data.table)
dfaccuracy <- rbindlist(smallbg_accuracyoneper)

ggplot(dfaccuracy, aes(value, weight = weight, fill = DataType))+
  geom_histogram(position = "dodge2")+
  theme_bw()+
  facet_wrap(~AcceptedName)

ggplot(dfaccuracy, aes(value, weight = weight, colour = DataType))+
  geom_freqpoly(binwidth=0.1)+
  theme_bw()+
  facet_wrap(~AcceptedName)

```

```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure4_accuracy_histograms_20190901.jpg",
     width=300, height=200,units='mm', res=300)

  dfaccuracy <- do.call(rbind,lapply(which(!unlist(lapply(whichdiff_Uncleaned2Error, is.na))), function(i){
    out <- data.frame(smallbg_accuracyoneper[[i]])
    out
  }))

p <- ggplot(dfaccuracy, aes(value, weight = weight, colour = DataType, linetype = DataType))+
  geom_freqpoly(binwidth=0.1)+
  # geom_histogram(position = "dodge2")+
  theme_bw()+
  facet_wrap(~AcceptedName, scales = "free")+
  scale_color_manual(values = c("red","gray30","black"),
                    name = "Dataset / background",
                    breaks = paste(filenames),
                    labels = c("EOR / small",
                               "uncleaned / small",
                               "error / small"))+
  scale_linetype_manual(values = c("twodash","dotted","solid"),
                        name = "Dataset / background",
                        breaks = paste(filenames),
                        labels = c("EOR / small",
                               "uncleaned / small",
                               "error / small"))

p
dev.off()

```


Which are signficantly different when the background is big   
```{r}


ks.outs_big <- 
  # for each species
  lapply(1:length(bigbg_accuracyoneper), function(i){
    # Compare each datatype (i.e. filenames or filenamesbig)
    # Compare filenames[1] (EOR) to filenames[2] (uncleaned)
    EOR_Uncleaned <- ks.test(log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[1]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[1]]),
          log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[2]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[2]]))
    
    EOR_Error <- ks.test(log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[1]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[1]]),
          log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[3]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[3]]))
    
    Uncleaned_Error <- ks.test(log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[2]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[2]]),
          log(data.frame(bigbg_accuracyoneper[[i]])$value[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[3]])+
             log(data.frame(bigbg_accuracyoneper[[i]])$weight[bigbg_accuracyoneper[[i]]$DataType == filenamesbig[3]]))
    
    list(EOR_Uncleaned,EOR_Error,Uncleaned_Error)
    })
# different distributions?

whichdiff_Uncleaned2Errorbig <- sapply(1:22, function(s){
  if(ks.outs_big[[s]][[3]]$p.value < 0.05){
    1
  } else {
    NA
  }
})
which(!unlist(lapply(whichdiff_Uncleaned2Errorbig, is.na))) #  [1]  4  5  6  7  8  9 10 14 15 17 20 21
length(which(!unlist(lapply(whichdiff_Uncleaned2Errorbig, is.na)))) # 12


```


```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure4_accuracy_histograms_BIG20190901.jpg",
     width=300, height=200,units='mm', res=300)

  dfaccuracy <- do.call(rbind,lapply(which(!unlist(lapply(whichdiff_Uncleaned2Errorbig, is.na))), function(i){
    out <- data.frame(bigbg_accuracyoneper[[i]])
    out
  }))

p <- ggplot(dfaccuracy, aes(value, weight = weight, colour = DataType, linetype = DataType))+
  geom_freqpoly(binwidth=0.1)+
  # geom_histogram(position = "dodge2")+
  theme_bw()+
  facet_wrap(~AcceptedName, scales = "free")+
  scale_color_manual(values = c("red","gray30","black"),
                    name = "Dataset / background",
                    breaks = paste(filenamesbig),
                    labels = c("EOR / big",
                               "uncleaned / big",
                               "error / big"))+
  scale_linetype_manual(values = c("twodash","dotted","solid"),
                        name = "Dataset / background",
                        breaks = paste(filenamesbig),
                        labels = c("EOR / big",
                               "uncleaned / big",
                               "error / big"))

p
dev.off()

```


```{r}


```



```{r}

plots_accuracy <- lapply(1:22, function(i){
  dfaccuracy <- data.frame(smallbg_accuracyoneper[[i]])
  
  p <-ggplot(dfaccuracy, aes(value, weight = weight, fill = DataType))+
    geom_histogram(position = "dodge2", bins = 10)+
    theme_bw()
  p
})

library(animation)
saveGIF(
  {lapply(plots_accuracy, print)}
  , "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/AccuracySmallbackground_animate_20190809.gif"
)



plots_accuracy_big <- lapply(1:22, function(i){
  dfaccuracy <- data.frame(bigbg_accuracyoneper[[i]])
  
  p <-ggplot(dfaccuracy, aes(value, weight = weight, fill = DataType))+
    geom_histogram(position = "dodge2", bins = 10)+
    theme_bw()
  p
})

library(animation)
saveGIF(
  {lapply(plots_accuracy_big, print)}
  , "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/AccuracyBigbackground_animate_20190809.gif"
)

```


# Precision
```{r}
precisionsmall <- do.call(rbind,lapply(atleast12, function(i){
  datatypeprec <- do.call(rbind,lapply(filenames, function(f){
      resultpath <- list.files(path = pathstart,
                               pattern = paste("AvgTiffSp",i,"Herb",f,
                                             g1g2namesall68$AcceptedName[i], sep=""), 
                             full.names=TRUE)
      rastout <- raster(resultpath)
      out <- sample(values(rastout), 1000000) # a sample of the larger set of rasters: 1,000,000
      dfout <- data.frame(Precision = out, DataType = f, SpNum = i)
      }))
  datatypeprec
  }))

head(precisionsmall)

plots_precision_sm <- lapply(split(precisionsmall, precisionsmall$SpNum), function(x){
  p <- ggplot(x, aes(Precision, fill = DataType))+
    geom_histogram(position = "dodge2", bins = 10)+
    theme_bw()
  p
})

saveGIF(
  {lapply(plots_precision_sm, print)}
  , "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/PrecisionSmallbackground_animate_20190809.gif"
)



precisionbig <- do.call(rbind,lapply(atleast12, function(i){
  datatypeprec <- do.call(rbind,lapply(filenamesbig, function(f){
      resultpath <- list.files(path = pathstart,
                               pattern = paste("AvgTiffSp",i,"Herb",f,
                                             g1g2namesall68$AcceptedName[i], sep=""), 
                             full.names=TRUE)
      rastout <- raster(resultpath)
      out <- sample(values(rastout), 1000000) # a sample of the larger set of rasters: 1,000,000
      dfout <- data.frame(Precision = out, DataType = f, SpNum = i)
      }))
  datatypeprec
  }))

plots_precision_big <- lapply(split(precisionbig, precisionbig$SpNum), function(x){
  p <- ggplot(x, aes(Precision, fill = DataType))+
    geom_histogram(position = "dodge2", bins = 10)+
    theme_bw()
  p
})

saveGIF(
  {lapply(plots_precision_big, print)}
  , "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/PrecisionBigbackground_animate_20190809.gif"
)



```



# Check how much different is within themapped occurrences, how much within a distance of the mapped occurences and how much is over the whole study area and how this differs by sample size, area range, by background... all that
```{r}
mapdifferences <- function(pathstart, filenames, whichones){
 lapply(whichones, function(i){
   
     # How correlated are the averages? 
     r1 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[1],
                        g1g2namesall68$AcceptedName[i],".tif", sep=""))
     r2 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[2],
                        g1g2namesall68$AcceptedName[i],".tif", sep=""))
     r3 <- raster(paste(pathstart,"AvgTiffSp",i,"Herb",filenames[3],
                        g1g2namesall68$AcceptedName[i],".tif", sep=""))
     
    # Map the differences between the rasters
     # 1) EOR - As Is
     # 2) EOR - Error
     # 3) As Is - Error
     
     eor_asis <- r1-r2
     eor_error <- r1-r3
     asis_error <- r2-r3
     writeRaster(eor_asis, paste(pathstart,
                               "EOR_AsIsSp",i,
                               strsplit(filenames[1],split='_', fixed=TRUE)[[1]][2],  # The background size
                               g1g2namesall68$AcceptedName[i],
                               ".tif", sep=""),overwrite=TRUE)
     writeRaster(eor_error, paste(pathstart,
                                  "EOR_ErrorSp",i,
                                 strsplit(filenames[1],split='_', fixed=TRUE)[[1]][2],  # The background size
                                 g1g2namesall68$AcceptedName[i],
                                 ".tif", sep=""),overwrite=TRUE)
     writeRaster(asis_error, paste(pathstart,
                           "AsIs_ErrorSp",i,
                           strsplit(filenames[1],split='_', fixed=TRUE)[[1]][2],  # The background size
                           g1g2namesall68$AcceptedName[i],
                           ".tif", sep=""),overwrite=TRUE)

 })
}

mapdifferences(pathstart = pathstart, filenames = filenames, whichones = atleast12)
mapdifferences(pathstart = pathstart, filenames = filenamesbig, whichones = atleast12)

```


```{r}

for(i in atleast12){
  rastertoplot <- raster(paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error/AsIs_ErrorSp",,Sp, "bg500000Lygodesmia grandiflora.tif")

}
# Plot them
AsIsErrorLyGrbig <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error/AsIs_ErrorSp66bg500000Lygodesmia grandiflora.tif")

Sp <- 66
polySp2 <- l1G1G2and[l1G1G2and$GNAME %in% c(g1g2namesall68$AcceptedName[Sp],
                                        g1g2namesall68$Taxon[Sp]),]
proj4string(polySp2) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84")
toplot <- spTransform(polySp2,
                      CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

# Blue would be error under predicted compared to uncleaned, black is AsIs overestimated compared to error
plot(AsIsErrorLyGrbig, breaks=c(-0.6,-0.3,-0.1,0.1,0.3,0.5), col=c("blue","skyblue","white","grey50","black"))
plot(toplot, add=TRUE, border = "red", lwd=3)
```














# Niche Equivalency - need parallelization first! Took three days to run one!
```{r}
cl <- makeCluster(10)

#bootstrapping by taking 100 samples, so slow, maybe better to just go through each? The presence point are the same for all the As-Is/uncleaned but should be different for some EOR and all error

All_nicheEquivalency <- lapply(atleast12, function(i) {
      # Sample from the kfolds and repeat a few times
      sp1 <- read.csv(paste(pathstart, "presenceHerb",filenames[1],"Sp", i, "kfold",
                            sample(1:10, 1),".csv", sep=""))
      sp2 <- read.csv(paste(pathstart, "presenceHerb",filenames[2],"Sp", i, "kfold1",
                            ".csv",sep="")) # As-Is or uncleaned all the same
      sp3 <- read.csv(paste(pathstart, "presenceHerb",filenames[3],"Sp", i, "kfold",
                            sample(1:10, 1),".csv", sep=""))
      Sp1 <- SpatialPoints(sp1[,c("decimalLongitude","decimalLatitude")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      Sp2 <- SpatialPoints(sp2[,c("decimalLongitude","decimalLatitude")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
      Sp3 <- SpatialPoints(sp3[,c("decimalLongitude","decimalLatitude")],
                           proj4string =
                             CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

      registerDoParallel(cl)
      trials <- 100
      foreach(icount(trials),  
              .combine='c', 
              .packages = c("dismo")) %dopar% {
                
                equiv1_2 <- nicheEquivalency(Sp1, Sp2, rasterstack, n=100)
                equiv1_3 <- nicheEquivalency(Sp1, Sp3, rasterstack, n=100)
                equiv2_3 <- nicheEquivalency(Sp2, Sp3, rasterstack, n=100)
                list(equiv1_2,equiv1_3,equiv2_3)
              }
      
      stopCluster(cl)
      })

save(equiv1_2, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/equiv1_2_Sp1.Rda") 
```

## Errors, need more paralelling or more power to do the 30 meter resolution one
Use smaller predictor variables
Run maxent for the three datasets - EOR, As-IS, and Error; all with background of 5000 meters
```{r}
coElev <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/CO_Mosaic_Elevation_VT/co_elev_VT_WGS84.tif")
coAspect <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Aspect_VT/co_aspect_VT_WGS84.tif")
coSlope <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Slope_VT/co_slope_VT_WGS84.tif")
rasterstack_2 <- stack(list(coElev, coAspect, coSlope))
rscrs_2 <- rasterstack_2@crs@projargs # "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
```
Maybe need more slices? so each is smaller?
```{r}
filenames30mrasters <-  c("EOR_bg5000","As-Is_bg5000","Error_bg5000")
pathstart30m <- ("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error_30mrasters/")

# Sample size will be at least 12 except for some where there were not enough mapped polygons to cover 12 different raster cells, in this case there are repeated points. 
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"),
          spatialpointsdataframe_herb = distXsp_justEOR_spdf,
          maxentarguments = FALSE, filenames = filenames30mrasters[1], 
          predictorvariables = rasterstack_2, pathstart = pathstart30m, backgroundscale = 5000)
  

# Herbarium as-is
maxentrun(whichones = atleast12, numberofReps = 10, error = FALSE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenames30mrasters[2], 
          predictorvariables = rasterstack_2, pathstart = pathstart30m, backgroundscale = 5000)


# Herbarium Error
maxentrun(whichones = atleast12, numberofReps = 10, error = TRUE,
          Whichproj4string = CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"),
          distdistribution = distdistribution$Dist,
          spatialpointsdataframe_herb = distXsp_justHerb,
          maxentarguments = FALSE, filenames = filenames30mrasters[3], 
          predictorvariables = rasterstack_2, pathstart = pathstart30m, backgroundscale = 5000)
```

The patternmatch needs to be "PredictHerb" filename from maxentrun() and "Sp" 
The rasternames are "Herb" and filenames
```{r}
patternmatch <- paste("PredictHerb", filenames30mrasters, "Sp", sep="")
rasternames <- paste("Herb", filenames30mrasters, "kfold", sep="")

lapply(1:3, function(x) stitchtogether(atleast12, pathstart=pathstart30m, patternmatch = patternmatch[x], rasternames = rasternames[x]))

# habitat specificity, there are 21 that work, 8 does not
habsp_22species <- habitatSpecificity(whichones=atleast12[atleast12 != 8], pathstart = pathstart, 
                                      replicates = 10, filenames = filenames[-3])
save(habsp_22species, file= paste(pathstart,"habsp_21species.Rda", sep=""))


# habsp_smallbg <- habitatSpecificity(atleast12, pathstart, replicates = 10, filenames = filenames)
# save(habsp_smallbg, file= paste(pathstart,"habsp_smallbg.Rda", sep=""))
# load(paste(pathstart, "habsp_smallbg.Rda", sep=""))  
# habsp_smallbg

load(paste(pathstart, "habsp_21species.Rda", sep=""))

habsp_21species <- aggregate(habspec~SpeciesNum + HerbType, mean, data=habsp_22species)

ggplot(habsp_21species, aes(HerbType, habspec, group=SpeciesNum, colour=as.factor(SpeciesNum)))+
  geom_point()+
  geom_line()

summary(lm(habspec~HerbType,data=habsp_21species))

# differences
hapsp_differences <- do.call(rbind,lapply(unique(hapsp_21species$SpeciesNum), function(num){
  types <-unique(hapsp_21species$HerbType)
  setNames(data.frame(num,
                      hapsp_21species$habspec[hapsp_21species$HerbType==types[1] & 
                                                hapsp_21species$SpeciesNum==num]-
                        hapsp_21species$habspec[hapsp_21species$HerbType==types[2] & 
                                                  hapsp_21species$SpeciesNum==num],
                      
                      hapsp_21species$habspec[hapsp_21species$HerbType==types[1] & 
                                                hapsp_21species$SpeciesNum==num]-
                        hapsp_21species$habspec[hapsp_21species$HerbType==types[3] & 
                                                  hapsp_21species$SpeciesNum==num],
                      
                      hapsp_21species$habspec[hapsp_21species$HerbType==types[2] & 
                                                hapsp_21species$SpeciesNum==num]-
                        hapsp_21species$habspec[hapsp_21species$HerbType==types[3] & 
                                         hapsp_21species$SpeciesNum==num]),
           c("SpeciesNum", 
             paste0(types[1], "-", types[2]),
             paste0(types[1], "-", types[3]),
             paste0(types[2], "-", types[3])))
}))

habsp_long <- melt(hapsp_differences, id.vars = "SpeciesNum")
ggplot(habsp_long, aes(variable, value, colour= as.factor(SpeciesNum), group=SpeciesNum))+
  geom_point()+
  geom_line()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
  
summary(lm(value~variable, data = habsp_long))
```


Are all the Combined wrong?!?!
```{r}
LyGr_comb <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt_EOR_asis_error/AvgTiffSp66HerbCombined_bg500000Lygodesmia grandiflora.tif")


```




Supplemental images
```{r}
#length(atleast12)
# Will be in the order: EOR, Herbarium, Combined

lapply(atleast12, function(i){ 
  
  jpeg(paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/S1_AvgMapsSmallbg_",
             g1g2namesall68$AcceptedName[i],".jpg", sep=""),
       width=500, height=110,units='mm', res=300)

    rastout <- lapply(filenames[-3], function(f){
     # for(f in filenames[-3]){ 
      resultpath <- list.files(path = pathstart,
                               pattern = paste("AvgTiffSp",i,"Herb",f,
                                               g1g2namesall68$AcceptedName[i], sep=""),
                               full.names=TRUE)
      rastout <- raster(resultpath)
      
    # par(mar =  c(5.1, 4.1, 4.1, 2.1))
    #   plot(rastout)
      })
    
  # layout(matrix(c(1:4), 1,4), widths = c(1,5,5,5))
  # layout.show(n=4)
    
    par(mfrow=c(1,4))
  # 1  
    par(mar = c(0,0,0,0))
    plot(c(0, 1), c(0, 1), ann = F, bty = 'n', type = 'n', xaxt = 'n', yaxt = 'n')
    text(x = 1, y = 0.5, g1g2namesall68$AcceptedName[i], cex = 2.5, srt = 90)
    
  # 2:4
    par(mar =  c(5.1, 4.1, 4.1, 2.1))
    plot(rastout[[1]])
    # mtext()
    plot(rastout[[2]])
    plot(rastout[[3]])
    
  
  dev.off()
})


```



