---
title: "Herbarium Bias CNHP"
author: "Michelle DePrenger-Levin"
date: "December 4, 2017"
output: html_document
---
Use Jessie's cool mapping code: "Q:\Research\All_Herbaria\KHD\Projects\exploring_herbarium_with_maps" 

Rick says don't need quotes for table or column names

MySQL query

#There are only rows in the synonym_links table when the names are synonyms, if it's not in synonym_links, it's the accepted name; taxonomic_units is the main table, if it's a synonym it's linked by synonym_links
SELECT `synonym_links`.*, `taxonomic_units`.* , `taxon_authors_lkp`.*, `geographic_div`.*
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn` 
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
LEFT JOIN `geographic_div` ON `geographic_div`.`tsn` = `taxonomic_units`.`tsn`
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 AND `geographic_div`.`geographic_value` IN ('North America')



#The authorities are giving me trouble right now; can do NOT IN (0,3,9999) or whatever
SELECT `synonym_links`.*, `taxonomic_units`.* , `taxon_authors_lkp`.*
FROM `taxonomic_units` 
JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn` 
JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 AND `taxonomic_units`.`unaccept_reason` IS NULL AND `taxonomic_units`.`taxon_author_id` NOT IN (0) AND `taxonomic_units`.`name_usage` NOT IN ('invalid') AND `taxonomic_units`.`n_usage` IN ('accepted')



#the taxon_authors_lkp.taxon_author_id isn't the tsn, it's the taxonomic_units.taxon_author_id!!!
SELECT `synonym_links`.*, `taxon_authors_lkp`.*, `taxonomic_units`.* 
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn`
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`taxon_author_id` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 



SELECT `synonym_links`.*, `taxon_authors_lkp`.*, `taxonomic_units`.* 
FROM `taxonomic_units` 
LEFT JOIN `synonym_links` ON `synonym_links`.`tsn` = `taxonomic_units`.`tsn`
LEFT JOIN `taxon_authors_lkp` ON `taxon_authors_lkp`.`taxon_author_id` = `taxonomic_units`.`tsn` 
WHERE `taxonomic_units`.`kingdom_id` = 3 AND `taxonomic_units`.`rank_id` = 220 



cd /p/hackathon/Simulations/

```{r}
rm(list=ls())

# install.packages("NicheMapR") based on animal needs anyway, mechanistic
library(ggplot2)
library(rgeos)
library(sp)
library(spdep)
library(rgdal)
library(maptools)
library(raster)
library(Taxonstand)
library(RCurl)
library(taxize)
library(ggmap)

library(gganimate)

library(dismo)
library(ENMeval)
library(biomod2)
library(spThin)

library(tidyr)
library(stringr)
library(magrittr)
library(prism)

library(doParallel)
library(parallel)
library(foreach)

#Skip making these, takes a long time
load("P:/hackathon/Simulations/circleEOR.Rda")
load("P:/hackathon/Simulations/circleHerb.Rda")
load("P:/hackathon/Simulations/EORgrid.thin.Rda")
load("P:/hackathon/Simulations/circleEOR.Rda")
load("P:/hackathon/Simulations/envs.backgEOR.Rda")
load("P:/hackathon/Simulations/envs.backgHerb.Rda")
load("P:/hackathon/Simulations/Herb.thin1000.Rda")
load("P:/hackathon/Simulations/EORgrid.thin.Rda")
load("P:/hackathon/Simulations/coloradosps.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorsinglecell.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorandherb.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herbpost1980.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorpost1980.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data2.Rda")

ScaleBar <- function(reference_raster_utm, round_to_nearest_km, width_percent, y_percent_from_bottom, x_percent_from_left, y_text_percent_from_bottom, ...) {
    # Round by max to nearest... e.g. 5 km 
    mround <- function(x,base){ 
        base*round(x/base) 
    }   
    # scale bar size adjustment to avoid decimals
        scale_size <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*width_percent)/1000
        scale_size_adj <- mround(scale_size, round_to_nearest_km)
        scale_size_adj_plot <- (scale_size_adj*1000)/2
    # Horizontal percent position (x) for scale bar
        x_position <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*x_percent_from_left)+xmin(reference_raster_utm)
    # Vertical percent position y for scale bar
        y_position <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_percent_from_bottom)+ymin(reference_raster_utm)
        y_position_text <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_text_percent_from_bottom)+ymin(reference_raster_utm)
    # Draw line on plot
        library(sp)
        x_ends <- c((x_position-scale_size_adj_plot), (x_position+scale_size_adj_plot))
        y_ends <- c((y_position), (y_position))
        scale_bar_line <- SpatialLines(list(Lines(Line(cbind(x_ends, y_ends)), ID="length")))
        projection(scale_bar_line) <- projection(reference_raster_utm)
        plot(scale_bar_line, add=TRUE, ...)
        text(x_position, y_position_text, paste0(scale_size_adj, "km"))
}


```
#Resample elevation (and others) to the bioclim sized grid, 30km grid is way to small, computer can't handle 
```{r}

coElev <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/CO_Mosaic_Elevation_VT/co_elev_VT_WGS84.tif")

coElev_biosize <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/co_elev_WGS84_30sec.tif")


plot(coElev)
```



```{r}
#l1eor <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records", layer="L1shp")
l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")

object.size(l1eor)

colocounties <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties_wgs84")
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")

#In lat/lon WGS 84
#plot(colocounties)

#in UTM NAD83
#plot(colocounties.UTM)
#plot(l1eor, add=TRUE)

#Each polygon might be made of multiple disconnected polygons. Need to separate and label
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))

#How many names in 
#length(unique(l1G1G2$GNAME)) #60
#write.table(unique(l1G1G2$GNAME), "clipboard", sep="\t", row.names=FALSE)


# Could take all the synonyms and names of l1G1G2 to make the list of names to pull from SEINet big list
namesg1g2 <- table(l1G1G2$GNAME)
#length(namesg1g2[namesg1g2 > 0]) #60 species
g1g2names <- TPL(names(namesg1g2[namesg1g2 > 0]))
#table(g1g2names$Taxonomic.status) 
#2 not in there, "Gutierrezia elegans" "Oonopsis sp. 1", 10 synonyms, 3 unresolved

#g1g2names[g1g2names$Taxonomic.status == "Unresolved",]
#Gilia sedifolia
#Ipomopsis ramosa
#Packera mancosana


# Check when names are good or not from herbarium specimens
seinet <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/seinet_colorado_with_coordinates_ALLG1G2.csv")

seinet <- seinet[seinet$genus != "",]

#table(seinet$basisOfRec) #PreservedSpecimen: 1871 and HumanObservation: 4
seinet <- seinet[seinet$basisOfRecord != "HumanObservation",]
namesSEINet <- names(table(seinet$scientificName)) #93 names
SEINet_names <- TPL(namesSEINet)
#table(SEINet_names$Taxonomic.status) #
#             Accepted    Synonym Unresolved 
#         2         75          9          7 
SEINet_names$scientificNameAccpt <-paste(SEINet_names$New.Genus, SEINet_names$New.Species)
#write.table(SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)], "clipboard", sep="\t", row.names=FALSE)
#SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)]

seinet_TPL <- merge(seinet, SEINet_names, by.x = "scientificName", by.y = "Taxon")
#head(seinet_TPL) # all the information from herbarium collections and if they're accepted names or not

seinet_TPL$family <- toupper(seinet_TPL$family)

#seinet_TPL[seinet_TPL$institutionCode=="USU",]
#seinet_TPL$institutionCode[seinet_TPL$institutionCode=="USU"] <- "USUUB"

namesbyyear <- table(seinet_TPL$Taxonomic.status,seinet_TPL$year)
plot(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[2,],col="blue", type="l",
     xlab="Year",ylab="Specimens", ylim=c(0,max(namesbyyear))) #accepted
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[3,],col="green") #synonym
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[4,],col="orange") #Unresolved
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[1,],col="pink") #not in TPL
```
#Families collected over time
```{r}
familyxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$family)) #now changed toupper lowercase $family has 44 levels, inconsistent capitals and lowercase..
familyxyear$Var1 <- as.numeric(as.character(familyxyear$Var1))

library(lattice)
xyplot(Freq ~ Var1|Var2, familyxyear[familyxyear$Freq>0,],
       type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Year of Collection",
       ylab = "Number of Collections", aspect = "xy")

herbariumxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$institutionCode)) 
ggplot(herbariumxyear[herbariumxyear$Freq>0,],aes(Var1, Freq, colour=Var2 ))+
  geom_point()+
  theme_bw()

#unique(seinet_TPL$geodeticDatum)
#seinet_TPL[seinet_TPL$geodeticDatum == "27",]
```

Cite MODIS data: <https://lpdaac.usgs.gov/citing_our_data>   
<https://lpdaac.usgs.gov/data_access/data_pool>  - need to get data but weeky mainenance is every wednesday until noon   
Got snow data https://search.earthdata.nasa.gov/data/retrieve/5396545932 can compare to NCAR's data.   
got some veg data or getting https://search.earthdata.nasa.gov/data/retrieve/2536564325  
```{r}
library(ncdf4)
#library(maptools)
#library(extRemes)
library(fields)
library(parallel)
library(doParallel)
library(foreach)
library(abind)
library(prism)

library(raster)
library(rasterVis)
#library(measurements)
library(ENMeval)
library(RCurl)
library(dismo)

#install.packages("digest")
library(devtools)
#install_github('hadley/rvest')
library(rvest)

# add the command to bind along the 3rd dimension
abind3 <- function(...) { abind(along = 3, ...) }

```



#Michelle DePrenger-Levin      
In ArcMap do this: <https://blogs.esri.com/esri/arcgis/2010/09/16/nearbygroup/>
repeat distance analysis in R          
"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet        
Only G1G2 EORs (to limit range of species)
"colonames" == TPL synonomy for all coloradosps names
```{r}
#coloradosps <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/occurrences.csv")

#save(coloradosps, file= "P:/hackathon/Simulations/coloradosps.Rda")
#nrow(coloradosps) #567328

howlongrecordedby <- nchar(as.character(coloradosps$recordedBy))
recordedby <- unique(coloradosps$recordedBy[howlongrecordedby < 100])

#table(coloradosps$basisOfRecord) # PreservedSpecimen: 566067; preservedspecimen: 55; Preserved specimen: 143; Photograph: 1; Observation: 124; Native and Naturalized Flora of; HumanObservation; many just wrong column info in this


# both of these from OBI: California Polytechnic State University
#coloradosps[coloradosps$basisOfRecord == "Plants of Jefferson County Open",] #68 plus 1 more with some other typo
#coloradosps[coloradosps$basisOfRecord == "Plants of Colorado",] #68 plus 1 more with some other typo

#Subset by just preserved specimens  
coloradosps$basisOfRecord <- as.character(coloradosps$basisOfRecord)
coloradosps.specimens <- coloradosps[grep("specimen",coloradosps$basisOfRecord,
                                          ignore.case=TRUE),]
#table(coloradosps.specimens$basisOfRecord)

coloradosps.specimens <- coloradosps.specimens[nchar(coloradosps.specimens$basisOfRecord)<50,]

#Now only preserved specimens
coloradosps <- coloradosps.specimens
#nrow(coloradosps) #566265

g1g2names$AcceptedName <- paste(g1g2names$New.Genus,g1g2names$New.Species)
#match SEINet names to either the given or accepted names from G1/G2s
#length(unique(g1g2names$Taxon))
#length(unique(g1g2names$AcceptedName)) #60
#length(unique(c(g1g2names$Taxon,g1g2names$AcceptedName))) #71 


rowstomatch <- lapply(c("Taxon","AcceptedName"), function(x){
   coloradosps[,"scientificName"] %in%  g1g2names[,x]
})

#The rows to keep that match either an accepted or synonym or unresolved name for a G1G2 species
#coloradosps.specimens was intermediate to get rid of none specimen records
coloradosps.g1g2 <- unique(rbind(coloradosps[rowstomatch[[1]],],coloradosps[rowstomatch[[2]],]))
nrow(coloradosps.g1g2)
hist(as.numeric(as.character(coloradosps.g1g2$decimalLatitude)))
#write.table(sort(unique(coloradosps.g1g2$scientificName)), "clipboard", sep="\t", row.names=FALSE)
plot(as.numeric(as.character(coloradosps.g1g2$decimalLongitude)),
     as.numeric(as.character(coloradosps.g1g2$decimalLatitude)))
```
How many are within 0.01 decimals of each other, round to two decimals - per Soltis 
```{r}
coloradosps.g1g2$decimalLatitude <- 
  as.numeric(as.character(coloradosps.g1g2$decimalLatitude))
coloradosps.g1g2$decimalLongitude <- 
  as.numeric(as.character(coloradosps.g1g2$decimalLongitude))
# ctrl+shift+c to comment/uncomment
# coloradosps.g1g2[which(coloradosps.g1g2$decimalLatitude <37),]
# coloradosps.g1g2[which(coloradosps.g1g2$decimalLongitude < -109.2),]

coloradosps.g1g2$lon2 <- round(coloradosps.g1g2$decimalLongitude,2) # about 1.1 km 
coloradosps.g1g2$lat2 <- round(coloradosps.g1g2$decimalLatitude,2)

coloradosps.g1g2$lon3 <- round(coloradosps.g1g2$decimalLongitude,3) # about 110 m
coloradosps.g1g2$lat3 <- round(coloradosps.g1g2$decimalLatitude,3)

# 
# 
# duplicated(coloradosps.g1g2[,c("lat2","lon2")])
# 
# duplicated(coloradosps.g1g2[,c("lat3","lon3")])
# 
# plot(colocounties)
# points(coloradosps.g1g2$lon2,coloradosps.g1g2$lat2,pch=20,cex=0.5)
# points(coloradosps.g1g2$decimalLongitude,coloradosps.g1g2$decimalLatitude,col="red",
#        pch=20,cex=0.5)
# points(coloradosps.g1g2$lon3,coloradosps.g1g2$lat3,pch=21,cex=0.5, col="blue")

# 
# nrow(coloradosps.g1g2) #-
# nrow(coloradosps.g1g2[!duplicated(coloradosps.g1g2[,c("lat2","lon2")]),]) #685
# 
# nrow(coloradosps.g1g2[!duplicated(coloradosps.g1g2[,c("lat3","lon3")]),]) #791
# 
# nrow(coloradosps.g1g2[duplicated(coloradosps.g1g2[,c("lat3","lon3")]),]) #1660
# nrow(coloradosps.g1g2[duplicated(coloradosps.g1g2[,c("lat2","lon2")]),]) #1766


#which ones are in denver?
# control + shift + C 
# write.table(coloradosps.g1g2[which(coloradosps.g1g2$decimalLatitude >39.5&
#                          coloradosps.g1g2$decimalLatitude <40.1&
#                          coloradosps.g1g2$decimalLongitude>-105.5),], 
#             "clipboard", sep="\t", row.names=FALSE)
# 
# write.table(coloradosps.g1g2[grep("Den",coloradosps.g1g2$county),], 
#             "clipboard", sep="\t", row.names=FALSE)

# setdiff(sort(unique(coloradosps.g1g2$scientificName)),sort(unique(l1G1G2$GNAME))) #species from the SEINet specimens that isn't in the names given for G1G2
# setdiff(sort(unique(l1G1G2$GNAME)),sort(unique(coloradosps.g1g2$scientificName))) #the first list that's not in the second


#convert points from WGS84 to NAD83
projected <- coloradosps.g1g2[complete.cases(coloradosps.g1g2[,c("lat2","lon2")]),]
coordinates(projected) <- ~ lon2+lat2
proj4string(projected) <- CRS("+proj=longlat +datum=WGS84") 
projected <- spTransform(projected, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
# 
# head(g1g2names)
# unique(l1G1G2$PolyID$GNAME)
```


There seems to be lots of points in Denver but no EOs!
```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/HerbPntsEORs_CO.jpg",
     width=250, height=225,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="gray80", add=TRUE, lwd=5) #
plot(projected, add=TRUE, pch=20, cex=0.5)

dev.off()

```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/EORs.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="goldenrod", add=TRUE, lwd=5) #☻
#plot(projected, add=TRUE, pch=20, cex=0.5)

dev.off()

```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/herbs.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
#plot(l1G1G2, border="blue", add=TRUE, lwd=5) #☻
plot(projected, col = "blue", add=TRUE, pch=20, cex=0.5)

dev.off()

```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/both.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="goldenrod", add=TRUE, lwd=5) 
plot(projected, col="blue", add=TRUE, pch=20, cex=0.5)

dev.off()

```
       [1] WGS84                     WGS 84                    WGS 1984                      
       [4] WGS96                     WGS 72                    WGS-84                     
       [7] WGS 1984.                 WGS83                     WGS84 (UTM Datum: NAD83)   
      [10] WGS84  (UTM Datum: NAD83) WGS84, Google Earth       WGS 1983                   
      [13] WGS 1984,                 WGS98                     WGS85                       
      [16] WGS86                     WGS89                     WGS95                       
      [19] WGS87                     WGS88                     WGS97                       
      [22] WGS92                     WGS 83                    WGS93                       
      [25] WGS94                     WGS84/NAD83               WGS90                        
      [28] WGS 19                    WGS99                     WGS91      
      
      "The latest revision is WGS 84 (also known as WGS 1984, EPSG:4326), established in 1984 and last revised in 2004.[1] Earlier schemes included WGS 72, WGS 66, and WGS 60. WGS 84 is the reference coordinate system used by the Global Positioning System."


#Test to make distXsp
```{r}
us <- getData('GADM', country = 'US', level = 1)
colorado <- us[us$NAME_1 == "Colorado",]
projLL <- proj4string(colorado)

colorado<- spTransform(colorado, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
i<-1
 polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),] 
 
grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
grid <- SpatialPoints(grid, proj4string = CRS(proj4string(polys)))

length(grid[polys,]) #106 points

#map each species, map each EOR, check distance from points to nearest EORs

#l1G1G2 are in UTM zone 13
# says  projargs: chr "+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84
#transform to UTM to match l1G1G2
#60 names with the listed name and the accepted name acording to TPL

proj4string(l1G1G2) <- CRS(proj4string(l1eor))
coloradosps.dist <- coloradosps.g1g2
coloradosps.dist$distNearEOR <- NA

distXspbind <- do.call(c,mapply('[[', distXsp, 2))
 
length(distXspbind[is.na(distXspbind)]) #14 don't have any data
distXspbind <- distXspbind[!is.na(distXspbind)]
coloradosps.dist$distNearEOR[which(rownames(coloradosps.dist) %in%
                                         names(distXspbind))] <-unname(distXspbind)
```


#Already loaded! Skip
```{r}
distXsp <- lapply(1:nrow(g1g2names), function(i){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),] 
     grid <- makegrid(polys, cellsize = 100) # cellsize in map units!
     grid <- SpatialPointsDataFrame(grid, 
                                    data.frame(id = 1:nrow(grid), Species = g1g2names$AcceptedName[i]),
                                    proj4string = CRS(proj4string(polys)))
     EORpnts <- data.frame(grid[polys,]@coords)
  if(nrow(EORpnts)>0){
    coordinates(EORpnts) <- ~ x1+x2
    proj4string(EORpnts) <- CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
    EORpnts <- spTransform(EORpnts, CRS("+proj=longlat +datum=WGS84"))
  } else {
    EORpnts <- NA
  }
     
  g1g2now <- coloradosps.g1g2[coloradosps.g1g2$scientificName %in%
                                c(g1g2names$Taxon[i],g1g2names$AcceptedName[i]),]
  
  areanow <- sapply(slot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),], "polygons"), 
         slot, "area")

  g1g2now$decimalLatitude <- as.numeric(as.character(g1g2now$decimalLatitude))
  g1g2now$decimalLongitude <- as.numeric(as.character(g1g2now$decimalLongitude))
  g1g2now <- g1g2now[!is.na(g1g2now$decimalLatitude),]
  g1g2now <- g1g2now[!is.na(g1g2now$decimalLongitude),]
  if(nrow(g1g2now)>0){
    #turn into spatialpointsdataframe for individual species
    coordinates(g1g2now) <- ~decimalLongitude+decimalLatitude
    proj4string(g1g2now) <- CRS("+proj=longlat +datum=WGS84") 
    g1g2now <- spTransform(g1g2now, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
    distnow <- apply(gDistance(g1g2now,
                               l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                                          g1g2names$Taxon[i]),], byid=TRUE),
                     2,min)
    
    
    out <- list(g1g2now, distnow, areanow, EORpnts)
  } else {
      out <- list(NA,NA, areanow, EORpnts)
    }
  out
  })

  
save(distXsp, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp.Rda")
```

```{r}

plot(distNearEOR~as.factor(scientificName), coloradosps.dist[coloradosps.dist$distNearEOR<500000,], las=2)

nrow(coloradosps.dist) #2451
colo.merged <- merge(coloradosps.dist, g1g2names, by.x = "scientificName", by.y="AcceptedName")
nrow(colo.merged) #2251
colo.merged.taxon <- merge(coloradosps.dist, g1g2names, by.x = "scientificName", by.y="Taxon")
nrow(colo.merged.taxon) #2121
## Want the ones that are only in the first and only the second and one copy of the middle ones... 
colo.all <- unique(rbind(colo.merged[,-grep("^Taxon$", names(colo.merged))], 
                         colo.merged.taxon[,-grep("^AcceptedName$",names(colo.merged.taxon))]))
nrow(colo.all) #2451
#Names that aren't in TPL
unique(colo.all$scientificName[colo.all$Family==""]) #"Gutierrezia elegans"
colo.all$Family[colo.all$Family==""] <- "Asteraceae"


plot(distNearEOR~as.factor(Family), colo.all[colo.all$distNearEOR<500000,], las=2,
     xlab="",ylab="")
require(lme4)
require(lattice)


length(table(colo.merged$Taxon)) #58 species with some data


#Write table for VisTrails field data
#nrow(seinet_TPL)
seinet_TPL.cc <- seinet_TPL[seinet_TPL$decimalLongitude > -110,]
seinet_TPL.cc <- seinet_TPL.cc[seinet_TPL.cc$decimalLatitude > 35, ]

plot(seinet_TPL.cc$decimalLongitude,seinet_TPL.cc$decimalLatitude)
# 
# write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2.csv")
# 
# write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Alpine_Phenology_2/Species_SEINet/FieldData_EORG1G2.csv")

herbspecimennumxsp<-table(seinet_TPL.cc$scientificNameAccpt)


whoswho <- data.frame(table(seinet_TPL.cc$scientificNameAccpt,seinet_TPL.cc$scientificName))
whoswho[whoswho$Freq>0,]  



```



#Animate maps over each species with points and polygons, animation isn't working    

```{r}
all.g1g2now <- do.call(c,mapply('[[', distXsp, 1)) #all the transformed to UTM species points
all.g1g2now <- all.g1g2now[!is.na(all.g1g2now)]

plot(colocounties.UTM, border="grey80", main=g1g2names$AcceptedName[1])
plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[1],g1g2names$Taxon[1]),],
     border = "red", add=TRUE, lwd=4)
points(distXsp[[1]][[1]], pch=16, cex=0.25)
ScaleBar(raster(colocounties.UTM), 100, .20, .10, .75, .07, lwd=2)
```


Get the coordiates from each saved to use as csvs in VisTrails. 
```{r}
# distXsp[[1]][[2]] #The distances from each herbarium point to the nearest EOR
# distXsp[[1]][[3]] #The areas of each EOR polygon for that species
# distXsp[[1]][[4]] #The 100 meter grid points within EORs

#lat/lon WGS 84
# plot(colocounties)
# points(distXsp[[1]][[4]], pch=20, col="blue", cex=0.5)

#AsMi
# plot(colocounties)
# points(distXsp[[4]][[4]]) 


#distXsp[[i]][[1]] are the herbarium points, SpatialPointsDataFrame
speciesNoHerb <- do.call(c,mapply('[[', distXsp, 1))
want <- which(do.call(c,lapply(speciesNoHerb, function(x) !is.na(x))))
wantEOR <- which(do.call(c,lapply(mapply('[[', distXsp, 4),
                         function(x) !is.na(x))))

bboxes <- lapply(want, function(i){
  speciesNoHerb[[i]]@bbox
  })

#If there are multiple names/synonyms
# unique(speciesNoHerb[[1]]@data$scientificName)

#All the points
# speciesNoHerb[[1]]@coords

#points in the EORs
EORsxsp <- mapply('[[', distXsp, 4)

# ctrl+shift+c to uncomment out
# for(i in wantEOR){
#   write.csv((EORsxsp[[i]]@coords),
#             paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2",g1g2names$AcceptedName[i],".csv", sep=""))
# }
# 
# 
# # make them match the number of points that the herbarium specimens have
# for(i in wantEOR){
#   if(nrow(EORsxsp[[i]]@coords)>herbspecimennumxsp[i]){
#   write.csv(EORsxsp[[i]]@coords[sample(1:nrow(EORsxsp[[i]]@coords), herbspecimennumxsp[i]),],
#             paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORsamplesizematch",g1g2names$AcceptedName[i],".csv", sep=""))
#   }
# }

# spatial thinning instead of just matching the points that the herbarium specimens have. Spatially thin the herbarium specimens as well. 

```

    
    

Test a PCoA for all grid EOR points and all Herb points. Then limit by year, can we figure out which EOR are historic points? 
```{r}

options(prism.path = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Prism_Climate_Data/30Year_Normal") 

#get_prism_normals(type="tmax", resolution = "800m", keepZip = FALSE, mon = 1:12)
#get_prism_normals(type="tmin", resolution = "800m", keepZip = FALSE, mon = 1:12)
#get_prism_normals(type="ppt", resolution = "800m", keepZip = FALSE, mon = 1:12)

# stack rasters
rasterstack <- ls_prism_data() %>% prism_stack(.) # many skipping the first 6 that are provisional
#get the projection from the raster stack
rscrs <- rasterstack@crs@projargs # "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
```


# Already loaded, skip!
```{r, eval=FALSE}

pca_eorandherb <- lapply(1:nrow(g1g2names), function(sp){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[sp],
                                         g1g2names$Taxon[sp]),] 

     cellsize <- 100
     grid <- lapply(1:length(polys), function(i){
       if(sqrt((polys[i,]@bbox[3]-polys[i,]@bbox[1])*(polys[i,]@bbox[4]-polys[i,]@bbox[2]))/cellsize < 1){
         grid <- makegrid(polys[i,], cellsize = 10, pretty=FALSE) #0.1km
         gridout <- SpatialPointsDataFrame(grid, data.frame(id=1:nrow(grid),polyid=i,
                                                            polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
           gridout
         } else {
           grid <- makegrid(polys[i,], cellsize = cellsize, pretty=FALSE) #1km 
           gridout <- SpatialPointsDataFrame(grid, data.frame(id=1:nrow(grid),polyid=i,
                                                              polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
           gridout
           }
       })
     

     ov <- lapply(1:length(polys), function(i){
         out <- grid[[i]][polys[i,],]
         out
         })

     eor_data <- lapply(1:length(ov), function(x){
       if(nrow(ov[[x]]@data)==0){
         center <- gCentroid(polys[i,])
         centerout <- SpatialPointsDataFrame(center, data.frame(id=1,polyid=x,
                                                                polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
         out <- spTransform(centerout, CRS(paste(rscrs)))
         out
       } else {
         out <- spTransform(ov[[x]], CRS(paste(rscrs)))
         out
       }
     })
     
     mergedEORs <- do.call(rbind,eor_data)
     data_eor <- data.frame(coordinates(mergedEORs),id=mergedEORs$id,polyid=mergedEORs$polyid,
                            Bestcrs=mergedEORs$BESTSRC, firstobs=mergedEORs$FIRSTOBS, 
                            GRANK=mergedEORs$GRANK,
                            GNAME=mergedEORs$GNAME,ENDEMIC=mergedEORs$ENDEMIC, 
                            Lastobs=mergedEORs$LASTOBS,
                            extract(rasterstack, mergedEORs))
     data_eor
})

  
```


Get rid of data from polygons only seen before 1981
get rid of repeted values for all the climate things, do only one row for each cell
Column 4 is each polygon's id so there's only one point for each raster grid over a polygon but at least one point per polygon.   
#Already loaded, skip!
```{r}
pca_eorsinglecell <- lapply(pca_eorandherb, function(x){
  x[!duplicated(x[,c(4,11:length(x))]),]
})

pca_eorpost1980 <- lapply(pca_eorandherb, function(x){
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  out <- x[!duplicated(x[,c(4,11:length(x))]),]
  out
})

save(pca_eorsinglecell, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorsinglecell.Rda")
save(pca_eorandherb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorandherb.Rda")
save(pca_eorpost1980, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorpost1980.Rda")


```

Visual checks of data
```{r}

#make into spatialpointsdataframes, make in UTM like EORs
eorsinglecell <- lapply(pca_eorsinglecell, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})

eorlotsofcell <- lapply(pca_eorandherb, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})

eorpost1980 <- lapply(pca_eorpost1980, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})


polys <- lapply(1:nrow(g1g2names), function(i){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),]
     polys
     })

for(j in 10:13){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue",lwd=3, main=polys[[j]][i,]@data$LASTOBS)
    plot(eorsinglecell[[j]], pch=21, col="red",add=TRUE)
    plot(polys[[j]], border="yellow", add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
  }
}


for(j in 3:5){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue", main=polys[[j]][i,]@data$LASTOBS)
    plot(eorlotsofcell[[j]], pch=21, col="red",add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
  }
}


for(j in 3:5){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue", main=polys[[j]][i,]@data$LASTOBS)
    plot(eorpost1980[[j]], pch=21, col="red",add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
  }
}

for(i in 10:23){
  plot(polys[[i]], border="goldenrod", lwd=10, main=polys[[j]][i,]@data$LASTOBS)
  plot(eorsinglecell[[i]], add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
}

for(i in 1:length(polys)){
  plot(polys[[i]], border="blue")
  plot(grid[i], pch=20, cex=0.5, add=TRUE)
  ScaleBar(polys[[i]],
           0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
}

```

#Already loaded, skip
```{r}

#The first in all these are the herbarium records
(distXsp[[1]][[1]]) #+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0   Need to transform

#distXsp[[i]][[1]] are the herbarium points, SpatialPointsDataFrame
speciesNoHerb <- do.call(c,mapply('[[', distXsp, 1))
want <- which(do.call(c,lapply(speciesNoHerb, function(x) !is.na(x))))

pca_herb <- lapply(want, function(x){
  out <- spTransform(distXsp[[x]][[1]], CRS(paste(rscrs)))
  pca_herbout <- data.frame(coordinates(out),id=out$id,polyid=NA,
                            Bestcrs=out$institutionCode, firstobs=out$eventDate, 
                            GRANK=NA,
                            GNAME=out$scientificName,ENDEMIC=NA, 
                            Lastobs=out$eventDate,
                            extract(rasterstack, out))
  pca_herbout
})


# since sampling bias is known issue, only one point per climate raster value (columns 11:length) are kept; don't know if sampling density is due to bias or higher area of suitability. hummm. 
pca_herbpost1980 <- lapply(pca_herb, function(x){
  colnames(x) <- c("x1","x2",colnames(x)[3:length(x)])
  x <- x[(which(nchar(as.character(x$Lastobs)) == 10)),]
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  out <- x[!duplicated(x[,c(11:length(x))]),]
  out
})

save(pca_herbpost1980, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herbpost1980.Rda")

```

select background points that are closer to road so pick more background points around roads and most where buffer around population and roads overlap.     
Or like Wunder's student's bat paper, just use random points from the circles made earlier, just a buffer, cite Elith   
Or de-bias presence records for areas near roads. Phillips et al. 2009: design selection of background data to reflect same sample selection bias as occurrence data.   
```{r}
coroads <- shapefile("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Roads/Local Roads/Local Roads SHP/LROADS.shp")

#Planer
proj4string(coroads) #"+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
r <- raster(extent(coroads))
proj4string(r) <- proj4string(coroads)
r.smaller <- disaggregate(r, fact=2)

dd <- gDistance(coroads, as(r, "SpatialPoints"), byid=TRUE)
r[] <- apply(dd,1,min)

dd.smaller <- gDistance(coroads, as(r.smaller, "SpatialPoints"), byid=TRUE)
r.smaller[] <- apply(dd.smaller,1,min)

toraster <- raster(extent(r.smaller))
proj4string(toraster) <- rscrs

r.longlat <- projectRaster(r.smaller,crs = rscrs)

plot(r)
plot(r.smaller)

plot(r.longlat)
plot(circleHerb[[2]], add=TRUE)

plot(rasterstack[[1]], xlim=c(-109,-103), ylim=c(37,41))

plot(r.smaller[r.smaller<8000])
plot(coroads, add=TRUE)
```

Either background points are selected with more frequency closer to roads or 'background' is a buffer around roads and around points and selected randomly within that overlap - all need to match the number of presence points. 

Without quality presence–absence data, discrimination metrics such as TSS can be misleading measures of model performance
Boris Leroy  Robin Delsol  Bernard Hugueny  Christine N. Meynard  Chéïma Barhoumi Morgane Barbet‐Massin  Céline Bellard
First published: 02 July 2018    
   
1. Add up all the distances as proportions or chance of getting to these, act as contributions like fecundity but in reverse.   
2. OR, know the number of grid cells within a few of the polygon, more likely to have surveyed closer to a road and closer to the point or line of the EOR polygon.   
3. will want proportionally close number of presence and background points. so use Binomial distribution to figure out probability of placing random background points in a cell and at what frequency.. over cells? randomly across background area? Use Bernolli to select which cells to include? http://users.stat.ufl.edu/~abhisheksaha/sta4321/lect12.pdf       
4. use makegrid with n = the probability? 




 From VisTrails   
Need to test overlap amount.  
```{r}
DrEx_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

DrEx <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata/Maxent_1/maxent_bin_map.tif")

#DrExEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1
#EOR <- DrEx_EOR

plot(DrEx_EOR)
plot(DrEx)
#plot(DrExEORmatchss)

AsMi_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

AsMi <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus/Maxent_1/maxent_bin_map.tif")

#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1

AsMiEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


plot(AsMi_EOR,main="EOR")
plot(colocounties, add=TRUE)
plot(AsMi, main="Herb")
plot(colocounties, add=TRUE)
plot(AsMiEORmatchss, main="EOR sample size match herb")
plot(colocounties, add=TRUE)
points(projected[grep("micro", projected$scientificName),])


#Make a copy of each to compare overlap O1 and O2
OAsMiEormatch <- AsMiEORmatchss
Oasmi <- AsMi 

#Where there is overlap between AsMiEormatchss and AsMi assign a 2 instead of 1 to AsMiEormatchss
OAsMiEormatch[Oasmi==1] <- 2 #change to 2 where overlap
out <- AsMi+OAsMiEormatch    #When added together will be 1 if only in one, 2 if only in AsMiEormatchss, 3 if in both
outcat <- ratify(out)        #To add ID for each
levelplot(outcat, col.regions =  c("white","green","darkred"), att="ID")

#Overlap of the other
Oasmi[AsMiEORmatchss==1] <- 2
out2 <- Oasmi+AsMiEORmatchss
outcat2 <- ratify(out2)
levelplot(outcat2, col.regions =  c("white","green","darkred"), att="ID")
#to get area, easier to be in meters, UTM so convert to proj4string(colocounties.UTM)
class(outcat)
# outcat <- projectRaster(outcat, crs = (proj4string(colocounties.UTM)))
# outcat2 <- projectRaster(outcat2, crs = (proj4string(colocounties.UTM))) 

areaoutcat <- outcat
areaoutcat[areaoutcat!=3] <- NA
sum(raster::area(areaoutcat, na.rm=TRUE))

areaoutcat2 <- outcat2
areaoutcat2[areaoutcat2!=3] <- NA
#tryagain<- projectRaster(outcat, crs = "+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0", method="ngb")
raster::area(areaoutcat2, na.rm=TRUE)

areaoutcat2raster <- rasterToPolygons(areaoutcat2)
out2proj <- spTransform(areaoutcat2raster, CRS(proj4string(colocounties.UTM)))
sum(gArea(out2proj, byid=TRUE))/(1000^2) #divided by 1km squared: 1043.532 square km

#Sum rasters that are overlap ==3 and multiply by cell size, but this is in decimal seconds at this latitude so about 1km is 1 and this is 0.00083 or whatevers
sum(outcat[] == 3)*res(outcat)[1]^2
sum(outcat2[] == 3)*res(outcat2)[1]^2

```
     1. Could use the comparison of more points (appropriate density?) within EORs vs. the sparse data because of biases and practices of herbarium collections to compare overlap of niche models for herb, eor, and sample size same eor. how many background?    
     2. Can't I get the background points from VisTrails? Yes! Background points were always set to 300 for my first crack at it. Too many? Maybe, maybe fine. Or just understand the bias it creats for AUC, sensitivity, specificity and such. 



layout plot   
```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/EOR_Herb_EORthin_AsMi.jpg",
     width=700, height=170,units='mm', res=300)

nf <- layout(matrix(c(1,2,3), 1,3, byrow=FALSE),
             width=c(6,6,6), height=c(1))
layout.show(nf)

op <- par(no.readonly=TRUE)
par(mai=c(1.5,2,3.5,3)/4)

#EOR
plot(AsMi_EOR)
plot(colocounties, add=TRUE)
mtext("a)", side=3, cex=2, line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))
points(distXsp[[4]][[4]],pch=20, cex=0.5)

#Herbarium collections
plot(AsMi)
plot(colocounties, add=TRUE)
mtext("b)", side=3,cex=2,  line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))

#Thinned EOR points to match Herbarium points
plot(AsMiEORmatchss)
plot(colocounties, add=TRUE)
mtext("c)", side=3,cex=2,  line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))

dev.off()

```


http://www.earthskysea.org/!ecology/sdmShortCourseKState2012/grinnellExercise/exercise-tuning-maxent-using-beta-and-aic.html  
https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html    
Use AICc with Maxent to find best model  
```{r}



```


create a raster in R as distance from road
```{r}
localroads <- readOGR(dsn = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Roads/Local Roads/Local Roads SHP", layer="LROADS")


bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)

#Create a raster to hold distance from road in the same size and extent of the bioclim layers
r <- raster()
extent(r) <- extent(bioclim.colorado[[1]])
projection(r) <- projection(bioclim.colorado[[1]])
res(r) <- res(bioclim.colorado[[1]])

localroads.latlon <- spTransform(localroads, CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))


dd <- gDistance(localroads.latlon, as(r, "SpatialPoints"), byid=TRUE)

```


#No recollection of what I'm doing here...   
```{r}

hist(distXspbind/1000, main = "Distance (km) from points to nearest EOR",
     xlab="km", breaks=1000, xlim=c(1,500))

vals <- density(distXspbind)
pStar <- vals$x[which(vals$y==max(vals$y))]

plot(density(distXspbind), xlim=c(0,50000))
abline(v=pStar, col="red")
abline(h=max(vals$y)-qlnorm(0.95))

length(distXspbind[distXspbind>100]) #816

length(distXspbind[distXspbind<=100]) #275

median(distXspbind)

reps <- 50000
ssize <- 500
rand.diff <- unlist(lapply(1:reps, function(x) median(sample(distXspbind,ssize))))
ctrl95CI <- sd(rand.diff)/sqrt(ssize)*qt(0.975,df=ssize) 
ctrl95CI.l <- sd(rand.diff)/sqrt(ssize)*qt(0.025,df=ssize)


hist(rand.diff)
abline(v=mean(rand.diff)+ctrl95CI, col="red")
abline(v=mean(rand.diff)+ctrl95CI.l, col="red")

```

```{r}
for(i in want){
  plot(distXsp[[i]][[1]], pch=16, cex=0.5, col=rgb(0,0,0,0.75), main=g1g2names$AcceptedName[i])
  plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),],
       border = rgb(1,0,0,0.5), lwd=3, add=TRUE)
  ScaleBar(raster(l1G1G2[l1G1G2$GNAME %in%
                           c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),]), #reference raster
           0.1, .5, .15, .15, .05, lwd=2) #round to nearest km, width, % y from bottom, % x from left, y text
}
```


```{r}
for(i in want){
  plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),],
       border = rgb(1,0,0,0.5),  main=g1g2names$AcceptedName[i], lwd=3,
       xlim=c(min(distXsp[[i]][[1]]@coords[,1])-100,max(distXsp[[i]][[1]]@coords[,1]))+100,
       ylim=c(min(distXsp[[i]][[1]]@coords[,2]-100),max(distXsp[[i]][[1]]@coords[,2]))+100)
  points(distXsp[[i]][[1]], pch=16, cex=0.5, col=rgb(0,0,0,0.75))
  ScaleBar(raster(l1G1G2[l1G1G2$GNAME %in%
                           c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),]), #reference raster
           0.1, .25, .15, .15, .10, lwd=2) #round to nearest km, width, % y from bottom, % x from left, y text
}
```

```{r}
i <- sapply(colo.all, is.factor)
colo.all[i]<-lapply(colo.all[i], as.character)

colo.all$year <- as.numeric(colo.all$year)
colo.all$decimalLatitude <- as.numeric(colo.all$decimalLatitude)
colo.all$decimalLongitude <- as.numeric(colo.all$decimalLongitude)
colo.all$minimumElevationInMeters <- as.numeric(colo.all$minimumElevationInMeters)

colo.all[colo.all$recordNumber == 9088,] #year is 1999, not recorded at Fort Lewis
colo.all[colo.all$recordNumber == 1596,] #another repeat, should be 1997 not 1007
## Ok to get rid of duplicates that resulted in wrong years
colo.all <- colo.all[colo.all$year > 1800,]

colo.all$coordinateUncertaintyInMeters <-
  as.numeric(colo.all$coordinateUncertaintyInMeters)
#How many specimens give an uncertainty in meters (square meters?)
length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$coordinateUncertaintyInMeters) & !is.na(colo.all$decimalLatitude)])/
  length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$decimalLatitude)])

colo.all[is.na(colo.all$coordinateUncertaintyInMeters),]

colo.all[colo.all$distNearEOR>50000 & !is.na(colo.all$distNearEOR),c(56,57,83,61,63,66,67)] #611551meters is width of colorado about

#Can't have missing data
colo.all.ll <- colo.all[!is.na(colo.all$decimalLatitude)&
                       !is.na(colo.all$decimalLongitude),]
coordinates(colo.all.ll) <-~decimalLongitude+decimalLatitude
proj4string(colo.all.ll) <- CRS("+proj=longlat +datum=WGS84") 
colo.all.ll <- spTransform(colo.all.ll, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

all.county <- over(colo.all.ll, colocounties.UTM[,"COUNTY"])
colo.all.ll$InCounty <- all.county[,1]


table(colo.all.ll$county[colo.all.ll$county != colo.all.ll$InCounty])
colo.all.ll@data[colo.all.ll$county != colo.all.ll$InCounty &
              !is.na(colo.all.ll$county),c("scientificName","institutionCode","county","InCounty","georeferencedBy")]

#Which county are the points really in
colocounties.UTM.df <- as.data.frame(colocounties.UTM)
colocounties.UTM.df$id <- as.numeric(rownames(colocounties.UTM.df))

plot(colocounties.UTM, border="grey80")
plot(colo.all.ll, add=TRUE, pch=16, cex=0.5)

#Make a Levenshtein distance matric between names (like that paper!)
countymatch<- adist(colo.all.ll@data$county[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    colo.all.ll@data$InCounty[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    partial=TRUE,ignore.case = TRUE)
#Single linkage method
#hcfar <- hclust(as.dist(countymatch), method='single')


#errors so try clean up county column
colo.all.ll$countyclean <- gsub("Co.*","",colo.all.ll$county)
#trim trailing white space
colo.all.ll$countyclean <- sub("\\s+$", "", colo.all.ll$countyclean)
colo.all.ll$countyclean <- toupper(colo.all.ll$countyclean)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]

nrow(colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),])/
       nrow(colo.all.ll)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]


```

Total area of EOR polygons per spcies for the the 46 species with mapped herbarium records 
```{r}
speciesNoEOR <- lapply(want, function(x){
  out <- sum(distXsp[[x]][[3]])
  out
})

areaxsp <- unlist(lapply(want, function(x) sum(distXsp[[x]][[3]])))
areaxsp <- data.frame(Area = areaxsp, AccName = g1g2names$AcceptedName[want])

mediandistxsp <- unlist(lapply(want, function(x) median(distXsp[[x]][[2]])))

area_distxsp <- rep(areaxsp$Area, times= unlist(lapply(want,function(x) length(distXsp[[x]][[2]]))))
sp_distxsp <- rep(areaxsp$AccName, times= unlist(lapply(want,function(x) length(distXsp[[x]][[2]]))))

toplot <- data.frame(area_distxsp, sp_distxsp, distXspbind)

#One is factor, but they are the same! 46 species
areaxsp$AccName <- as.character(areaxsp$AccName) 

colo.all.ll$AcceptedName <- paste(colo.all.ll$New.Genus,colo.all.ll$New.Species)
colo.all.ll@data <- merge(colo.all.ll@data,areaxsp, 
                          by.x ="AcceptedName", by.y="AccName")

toplot[which(toplot$area_distxsp==max(toplot$area_distxsp)),]


ggplot(toplot[toplot$distXspbind<200000,], aes(area_distxsp, distXspbind/1000))+
  geom_point()+
  theme_bw()+
  stat_smooth(method="lm")+
  ylab("Distance (km) to nearest EOR")+
  xlab("Total EOR area")

```

```{r}
require(AICcmodavg)



f1 <- lm(distNearEOR ~ New.Genus + year + institutionCode + Family + Area, data= colo.all.ll@data)
f2 <- lm(distNearEOR ~ year + institutionCode + Family + Area, data= colo.all.ll)
f3 <- lm(distNearEOR ~ year + Area, data= colo.all.ll)
f4 <- lm(distNearEOR ~ year, data= colo.all.ll)
f5 <- lm(distNearEOR ~ Area, data= colo.all.ll)
f6 <- lm(distNearEOR ~ 1, data= colo.all.ll)

(lmresults <- aictab(list(f1,f2,f3,f4,f5,f6),
       modnames=as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6),formula)))))

evidence(aictab(cand.set = list(f1,f2,f3,f4,f5,f6),
                modnames = as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6),formula)))))

sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})

(er1 <- exp(0.5*lmresults$Delta_AICc[2]))
(er2 <- exp(0.5*lmresults$Delta_AICc[3]))
```


#how many are in the county they say they should be in? Check Jessie's code for help
```{r}
ggplot(colo.all[colo.all$distNearEOR<50000,], aes(as.numeric(year), distNearEOR))+
  geom_point()+
  stat_smooth(se=FALSE)+
  theme_bw()+
  xlab("Year")+
  ylab("Distance from nearest EOR")+
#  labs(color='Georeference Method')+
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```



#By herbarium 
Maybe regional collections are much better?   
FLD = Fort Lewis College    
DES = Dessert Arizona   
MESA = Colorado Mesa University   
MISSA = Miss State Univ   
NMC = New Mexico State University    
SJNM = San Juan College, New Mexico   
UCR = University of California Riverside    
USUUB = Utah STate University Uintah Basin (probably is USU)   
UVSC = Utah Valley University	U.S.A. Utah. Orem.      




```{r}

local <- c("COLO","CS","DBG","FLD","MESA","RM","RMBL")
Adjacent <- c("ASC","ASU","BRY","DES","NMC","RM","SJNM","UNM","USUUB","UVSC")
far <- c("CM","F","MISSA","MO","NY","RENO","UCR")

colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% local] <- "Local"
colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% Adjacent] <- "Near"
colo.all.ll@data$InstNear[colo.all.ll@data$institutionCode %in% far] <- "Far"

colo.all.ll@data$InstNear <- factor(colo.all.ll@data$InstNear, levels = c("Far","Near","Local"))

ggplot(colo.all.ll@data[colo.all.ll$distNearEOR<500000,], 
       aes(InstNear, distNearEOR/1000))+
  geom_jitter()+
  stat_summary(fun.y=mean, geom="point", shape=18,
                 size=3, color="red")+ 
  geom_violin(trim = FALSE)+
  theme_bw()+
  ylab("Distance (km) to nearest EOR")+
  xlab("Total EOR area")

```

```{r}
colo.all.ll@data$verbatimCoordinates[colo.all.ll$distNearEOR>100]

#How many gave 1/4 section
colo.all.ll@data[grep("1/4", colo.all.ll$verbatimCoordinates),]
mean(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])
sd(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])


mean(colo.all.ll@data$distNearEOR[grep("", colo.all.ll$verbatimCoordinates)])
sd(colo.all.ll@data$distNearEOR[grep("1/4", colo.all.ll$verbatimCoordinates)])
```



#Convext hull or tiny circles around eors vs herbarium vs. both together, how much change?    

The Weber example of an out of state person mapping a collection to Gray's Peak from Chicago Lake that should be Mount Evans.     
- <https://conps.org/wp-content/uploads/2015/05/Mount_Evans_Summit_Lake_4-24-2011.pdf>    

"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet      
"coloG1G2" == species matching names in L1 EORs but without any synonym corrections     
"seinet" == Mo worked on this, made distances from nearest EOR and looked at posted informaiton to figure out what might have been used to pick point on map        
      
ITISlist is synonmy pulled from ITIS into SQL    
TPL is pulling from synonym from the plant list   "colo"   
      "seinetMo"is synonymy with TPL for Mo's list of 53 species (that are G1/G2s)   
      "colonames" is synonym with TPL for all colorado specimens from SEINet   

<https://rpubs.com/spoonerf/SDM4>   
        
#ENMeval vignette
<https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html>   
```{r}
bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39) #at 0.5 minutes of a degree
##just get the datafiles lat and long from pca_eorandherb and extract the bioclim variables in addtion to the monthly precip, min, max temps

plot(bioclim.colorado$bio1_12)
plot(colocounties, add=TRUE)

# Use rasters in a RasterStack:
class(bioclim.colorado)

#Need back to latlon for bioclim layers
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[i]]][[1]], CRS(proj4string(bioclim.colorado)))
}

```


#Spatial thinning
<https://cran.r-project.org/web/packages/spThin/vignettes/spThin_vignette.html>     
Thin to create dataset in which all occurences are at least _thin.par_ distance apart. helps reduce effect of uneven, biased, species occurence collections on spatial model outcomes   
Got through 19 of them, go from 20 to length(wantEOR)   
SKIP, already loaded
```{r}
# The EOR grid points from distXsp is the forth item. Wants lat long, they are. 
#for(i in 1:length(wantEOR)){
#for(i in 20:length(wantEOR)){
#for(i in 21:length(wantEOR)){

EORgrid.thin <- list()
#for(i in c(1:18,21:47,49:length(wantEOR))){
for(i in c(20,48)){
  EORsp <- data.frame(distXsp[[wantEOR[i]]][[4]]@coords,
                      species=g1g2names$AcceptedName[wantEOR[i]])
    EORgrid.thin[[i]] <- thin(loc.data = EORsp,
                         lat.col = "x2", long.col = "x1",
                         spec.col="species",
                         thin.par = 1, reps=10,
                         locs.thinned.list.return = TRUE,
                         write.files = TRUE, max.files=5,
                         out.dir = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Thinned/",paste(out.base="EORsp_1km",i,"_",g1g2names$AcceptedName[wantEOR[i]],sep=""), write.log.file = TRUE,
                         log.file = "EORgridthinned_log.txt")
}

save(EORgrid.thin, file= "P:/hackathon/Simulations/EORgrid.thin.Rda")
```


```{r}
Herb.thin1000 <- list()
for(i in 1:length(want)){
  Herbsp <- data.frame(pointsXsp.latlon[[i]]@coords,
                       species=g1g2names$AcceptedName[want[i]])
  Herb.thin1000[[i]] <- thin(loc.data = Herbsp,
                     lat.col = "decimalLatitude", long.col = "decimalLongitude",
                     spec.col="species",
                     thin.par = 1, reps=1000,
                     locs.thinned.list.return = TRUE,
                     write.files = TRUE, max.files=5,
                     out.dir = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Thinned/",paste(out.base="Herb.thin_1km_1000",i,"_",g1g2names$AcceptedName[wantEOR[i]],sep=""), write.log.file = TRUE,
                     log.file = "EORgridthinned1000_log.txt")
}

save(Herb.thin1000, file= "P:/hackathon/Simulations/Herb.thin1000.Rda")
```


### Background points  
Can either do bounding box (plus some buffer) or circles at some diameter   
    
    - there are 60 species L1/L2 species with data   
    - 59 species have EOR data    
    - 46 species have herbarium data avalaible through SEINet    
    - wantEOR are the ones from the 60 that have EOR data so wantEOR[i] in a loop would get the ones I want    
    - want[i] would get teh 46 that have herbarium data from the list of 60 species   

##SKIP!! you already loaded these
```{r}
#Circles around herbarium points
circleHerb <- list()
for(i in 1:length(want)){
  circleHerb[[i]] <- circles(pointsXsp.latlon[[i]]@coords, d=50000, lonlat=TRUE)
}

save(circleHerb, file= "P:/hackathon/Simulations/circleHerb.Rda")

#Circles around EOR points. 
circle <- list()
for(i in 1:length(wantEOR)){
  circle[[i]] <- circles(EORsxsp[[wantEOR[i]]]@coords, d=50000, lonlat=TRUE)
}

save(circle, file= "P:/hackathon/Simulations/circleEOR.Rda")
```


```{r}
plot(colocounties)
plot(circle[[2]], add=TRUE)
points(EORsxsp[[2]]@coords, pch=16)
plot(circle[[3]], add=TRUE, border="red")
points(EORsxsp[[3]]@coords, pch=16)
plot(circle[[4]], add=TRUE, border="blue")
points(EORsxsp[[4]]@coords, pch=16)
plot(circle[[5]], add=TRUE, border="orange")
points(EORsxsp[[5]]@coords, pch=16)
```

#SKIP!!! already loaded
```{r}
# Need stack of environmental layers  
#Raster stack of just the bioclim variables
#class(bioclim.colorado)

# Get backgrounds for each species
envs.backg <- list()
for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- crop(bioclim.colorado, circle[[i]]@polygons)
}

for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- mask(envs.backg[[i]], circle[[i]]@polygons)
}


#Get backgrounds for each species according to the herbarium specimens
envs.backg.herb <- list()
for(i in 1:length(want)){
  envs.backg.herb[[i]] <- crop(bioclim.colorado, circleHerb[[i]]@polygons)
}

for(i in 1:length(want)){
  envs.backg.herb[[i]] <- mask(envs.backg.herb[[i]], circleHerb[[i]]@polygons)
}


save(envs.backg, file= "P:/hackathon/Simulations/envs.backgEOR.Rda")
save(envs.backg.herb, file= "P:/hackathon/Simulations/envs.backgHerb.Rda")
```



```{r}
#The item in the list that matches the number, i, is the the match(i, want) or match(i, wantEOR) but some are missing (want and envs.backg.herb are 46 long, wantEOR and envs.backg are 59 long )

g1g2names$Taxon[41] #Packera mancosana - new species
g1g2names$AcceptedName[41] #Packera mancosana


#need back to lat long
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[1]]][[1]], CRS("+proj=longlat +datum=WGS84"))
}

wantWOPaMa <- want[-which(want==41)] #now 45 names


#gets thrown off after Packera mancosana but can't tell why
for(i in 1:length(want)){
  if(want[i]!=41){

    plot(colocounties,
         main=g1g2names$AcceptedName[want[i]])
    plot(envs.backg.herb[[i]][[1]], #just the first bioclim, 46 of them. 
         add=TRUE, col=rgb(0,1,1,0.25),border="black")
    plot(envs.backg[[want[i]]][[1]], col=rgb(1,0,0,0.25), # there are 59 with EORs
         add=TRUE, legend=FALSE) #from the EORs
    points(pointsXsp.latlon[[i]], pch=17, col="purple") #herb points, need to skip 41...
    points(distXsp[[want[i]]][[4]]@coords, col="red", pch=16)   #this gets off one after Packera mancosana, 60 of them 

  }
}

#Penstemon degeneri and after are off, must be after Packera mancosana, got rid of the 34th one
#Now error after Oreoxis which is 41, Packera mancosana

which(g1g2names$AcceptedName == "Oreoxis humilis") #40
g1g2names$AcceptedName[41]


```

```{r}
#Plot presence points
plot(envs.backg[[wantEOR[19]]][[1]], main=g1g2names$AcceptedName[wantEOR[19]])
points(EORsxsp[[wantEOR[19]]], pch=16)

for(i in 1:length(envs.backg.herb)){
  plot(envs.backg.herb[[i]][[1]], main=g1g2names$AcceptedName[want[match(i,want)]] )
  plot(colocounties, add=TRUE)
  points(pointsXsp.latlon[[i]])
}

```


Background points for EORs
```{r}
# Background points from set area, select number of background points equal to the number of presence points
#Amend to 25% more background than presence. 

pca_eorandherb[[1]]

#bg <- list()
bg <- lapply(1:length(wantEOR), function(i){
  background <- as.data.frame(randomPoints(envs.backg[[i]][[1]], n=nrow(EORsxsp[[wantEOR[i]]]@coords)+
                                          0.25*(nrow(EORsxsp[[wantEOR[i]]]@coords))))
  out <- data.frame(x1 = background$x, x2 = background$y, id = NA, polyid=NA, Bestcrs=NA, firstobs=NA,
                           GRANK=NA, GNAME=g1g2names$AcceptedName[i], ENDEMIC=NA, Lastobs=NA,
                    raster::extract(rasterstack, background))
  out
})

save(bg, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/bg.Rda")


```


Background points for herbs
```{r}
# Background points from set area, select number of background points equal to the number of presence points
#Amend to 25% more background than presence. 

length(pointsXsp.latlon) #46

#bg <- list()
bg.herb <- lapply(1:length(want), function(i){
  background <- as.data.frame(randomPoints(envs.backg.herb[[i]][[1]], n=nrow(pointsXsp.latlon[[i]]@coords)+
                                          0.25*(nrow(pointsXsp.latlon[[i]]@coords))))
  out <- data.frame(x1 = background$x, x2 = background$y, id = NA, polyid=NA, Bestcrs=NA, firstobs=NA,
                           GRANK=NA, GNAME=g1g2names$AcceptedName[i],ENDEMIC=NA, Lastobs=NA,
                    raster::extract(rasterstack, background))
  out
})

save(bg.herb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/bg.herb.Rda")
```



Background points given roads raster, point in each grid, extract values, run the binomial distribution function probably over it. 

```{r}
#Plot presence and background points
for(i in 1:length(wantEOR)){
  plot(envs.backg[[i]][[1]])
  plot(colocounties, add=TRUE)
  points(EORsxsp[[i]], pch=16, cex=0.5)
  points(bg[[i]], pch=21, cex=0.5, col="red")
  scalebar(100, type="line", divs = 4, below="kilometers", lonlat = TRUE)
}

#Try some thinning first, for some that's lots of points that are 

```

Partitioning occurrences for evalutaiton of models to reduce overinflated performace due to biased sampling/autocorrelation among points   
```{r}
#Block partitioning spatially by dividing the data into four bins based on their position in relation to a north/south and east/west partitioning that gets the points into nearly 4 equal size groups. 

#NEED to do this for the herbarium specemins too, not just the EORS!!!

blocks <- list()
for(i in 1:length(pointsXsp.latlon)){ #46 species with points
  blocks[[i]] <- get.block(pointsXsp.latlon[[i]]@coords, bg[[want[i]]])
  
  plot(envs.backg[[i]][[1]], col="gray", legend=FALSE)
  points(EORsxsp[[i]]@coords, pch=21, bg=blocks[[i]]$occ.grp) #In CRS("+proj=longlat +datum=WGS84"), grid points every 100 meters (map units which was UTM) within EOR polygons.
  points(pointsXsp.latlon[[i]])
  
}
```



```{r}
bkgrnd <- calc(r.smaller, fun= function(X) sampleRandom(r.smaller, size= x))

samplesize <- calc(r.smaller, fun=function(x) spsample)

hist(samplesize)

layerbackgroundpoints <- sampleRandom(r.smaller, size = ) 

```


Create the full dataframes for each species   
https://www.molecularecologist.com/2013/04/species-distribution-models-in-r/     

```{r}
#Create bckground points, give them a 0 and same column names with lat long. Give eors a 1 and herb a 2 for looking at groups but then do a clustering

sort(unique(coloradosps.g1g2$scientificName)) #68 names since a few synonyms 
sort(unique(g1g2names$AcceptedName)) #60 names, why no Astragalus anisus??  
sort(unique(g1g2names$Taxon)) #60 names, why no Astragalus anisus??   

```



```{r}
pca_data <- lapply(1:length(want), function(x){
  eor <- pca_eorpost1980[[want[x]]]
  eor$pa <- 1
  herb <- pca_herbpost1980[[x]]
  herb$pa <- 2
  background.eor <- bg[[x]]
  background.eor$pa <- 3
  background.herb <- bg.herb[[x]]
  background.herb$pa <- 4 
  rbind(eor, herb, background.eor, background.herb)
})

```


```{r}
names(pca_data[[1]])

#Make seasonal precip and temps? 
# Winter: December-March; Summer: April-July; Fall: August-November
season <- pca_data[[1]]
climatenames <- do.call(c,lapply(names(season)[11:46], function(s){
                  out <- unlist(strsplit(s, split='_', fixed=TRUE))[c(2,6)]
                  paste(out[1],out[2],sep='_')
                  }))
colnames(season)[11:46] <- climatenames

pca_data2 <- lapply(pca_data, function(season){
  season$WinterPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(12,1:3)]])
  season$SummerPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(4:7)]])
  season$FallPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(8:11)]])
  
  season$WinterMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(12,1:3)]])
  season$SummerMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(4:7)]])
  season$FallMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(8:11)]])
  
  season$WinterMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(12,1:3)]])
  season$SummerMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(4:7)]])
  season$FallMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(8:11)]])
  
  season
}) 

names(pca_data2[[1]])

save(pca_data, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data.Rda")

save(pca_data2, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data2.Rda")

```

Model Fitting, AIC, GLM   
```{r}
require(AICcmodavg)
library(lme4)
library(lattice)
library(psych)

#predictor variables and eliminated one variable from each pair that was strongly correlated (Pearson or Spearman correlation, r > 0.70 — Elith et al., 2006). - Hayes et al 2015

cor.plot(cor(pca_data2[[1]][c(48, 54)]), numbers=TRUE) #after getting rid of them one by one... left with winter precip and winter max which both make sense, depending on how much winter precip and how hot so how long it might stay around and melt into the spring probably has largest impact. 
```

Can't seem to automate variable selection to get rid of one variable when more than cutoff
```{r}
cutoff <- 0.7
cornames <- colnames(pca_data2[[1]][48:56])
cor.table<-cor(pca_data2[[1]][,(48:56)])
data <- as.data.frame(as.table(cor.table))
#with how many other variables is one correlated? 
cortable <- table(data$Var1[data$Freq>=cutoff])
mostcor <- names(cortable[which(max(cortable)==cortable)])
cor.plot( cor(pca_data2[[1]][,c(grep(paste(mostcor[1:2], collapse="|"),
                            cornames, value=TRUE, invert=TRUE))]), numbers=TRUE)

```


```{r}
combins <- combn(colnames(cor.table),2,FUN=function(x) paste(x,collapse="_"))
cor.data <- data[data$Var1 != data$Var2, ]
data <- cor.data[paste(cor.data$Var1, cor.data$Var2, sep="_") %in% combins,]
data[data$Freq>=cutoff,]

#Which is better? Removing Var1 or Var2? 
sort(unique(data$Var1))
sort(unique(data$Var2))

#Even at 70, should be able to keep two varaibles! Something is off. 

model1 <- as.formula(paste("pa ~",
                           paste( grep(paste(unique(data$Var1), collapse="|"),
                                       names(pca_data2[[1]][48:56]), value=TRUE, invert=TRUE ))))

model2 <- as.formula(paste("pa ~",
                           paste( grep(paste(unique(data$Var2), collapse="|"),
                                       names(pca_data2[[1]][48:56]), value=TRUE, invert=TRUE ))))

```


```{r}
m1 <- glm(pa ~ WinterPrecip+SummerPrecip+FallPrecip+
            WinterMinTemp+SummerMinTemp+FallMinTemp+
            WinterMaxTemp+SummerMaxTemp+FallMaxTemp,  data=pca_data2[[1]])
summary(m1)

# If I want the variable with lower residual std. error from correlated pairs:

# Might be that winter precip is most important for all these and summer and winter temps
m2 <- glm(pa ~ WinterPrecip+
            WinterMinTemp+SummerMinTemp+
            WinterMaxTemp+SummerMaxTemp,  data=pca_data2[[1]])
summary(m2)

#Nothing important?
m3 <- glm(pa ~ 1,  data=pca_data2[[1]])
summary(m3)

m4 <- glm(pa ~ WinterPrecip+WinterMaxTemp,  data=pca_data2[[1]])

#not correlated for at least first species
m5 <- glm(pa ~ WinterPrecip+SummerPrecip,  data=pca_data2[[1]])

lm.list <- list(m1,m2, m3,m4,m5) #list(m4,m2, m1,m5,m3)
lm.names <- as.character(unlist(lapply(lm.list,formula)))
(lm.results <- aictab(lm.list, modnames=lm.names))

#evidence ratio 
for(i in 2:length(lm.list)){
  print(exp(0.5*lm.results$Delta_AICc[i]))
}

#Largest model most parsimonious. The uncorrelated is less parsimonious (Delta_AICc=63.86) (winterprecip and summerprecip), ~1 is worse
```

https://www.molecularecologist.com/2013/04/species-distribution-models-in-r/    
EOR presence is 1 and background is 3    
Herb presence is 2 and background is 4
```{r}
#dec-mar precip, max temp
winterPrecip <- stackApply(rasterstack[[c(12,1:3)]], 1, mean)
winterMaxtemp <- stackApply(rasterstack[[c(12,1:3)+12]], 1, mean)
winterMintemp <- stackApply(rasterstack[[c(12,1:3)+24]], 1, mean)
fallPrecip <- stackApply(rasterstack[[c(8:11)]], 1, mean)
fallMaxtemp <- stackApply(rasterstack[[c(8:11)+12]], 1, mean)
fallMintemp <- stackApply(rasterstack[[c(8:11)+24]], 1, mean)
summerPrecip <- stackApply(rasterstack[[c(4:7)]], 1, mean)
summerMaxtemp <- stackApply(rasterstack[[c(4:7)+12]], 1, mean)
summerMintemp <- stackApply(rasterstack[[c(4:7)+24]], 1, mean)

cor.plot(cor(pca_data2[[1]][c(48:56)]), numbers=TRUE)

#would lke to use scaled, colorado layers
us <- getData("GADM", country="USA", level=1)
# extract states (need to uppercase everything)
swstates <- c("Colorado", "Wyoming", "Utah", "New Mexico" ,"Arizona")

sw = us[match(toupper(swstates),toupper(us$NAME_1)),]
sw <- spTransform(sw, CRS(proj4string(winterPrecip)))

# now use the mask function
winPrecipclip <- mask(winterPrecip, sw)
winTmaxclip <- mask(winterMaxtemp, sw)

winterMintempclip <- mask(winterMintemp, sw)
fallPrecipclip <- mask(fallPrecip, sw)
fallTmaxclip <- mask(fallMaxtemp, sw)
fallMintempclip <- mask(fallMintemp,sw)
summerPrecipclip <- mask(summerPrecip,sw)
summerMaxtempclip <- mask(summerMaxtemp,sw) #error
summerMintempclip <- mask(summerMintemp, sw) #error


# winter temps and precip important to give moisture to plants in spring, melt, and to hit germination needs, fall temp is not coorelated and should impact most fruit produciton for most species, too hot and it'll all dry up too much. 
stackmini <- stack(winPrecipclip,winTmaxclip,winterMintempclip, 
                   #fallPrecipclip, 
                   fallTmaxclip) #, fallMintempclip)
                   #, summerPrecipclip)
```


```{r}
# 
# cl <- makeCluster(6)
# registerDoParallel(cl)
# stopCluster(cl)
# 
# memory.limit()
# memory.limit(size=memory.limit()+1000)
# memory.limit(size=memory.limit()-1000) #take it back down a notch? oh, can't!?


#Loop through all the species
Maxent46species <- lapply(1:length(pca_data2), function(i){
  checkrecords <- sapply(split(pca_data2[[i]], pca_data2[[i]]$pa), function(x) nrow(x))
   
  # to keep at least 5 records per group
  if(sum(checkrecords<10)>0){
    NULL
  } else {

        kfoldnum <- 10
        speciestable <- pca_data2[[i]]
        speciestable$group_pa <- unlist(lapply(split(pca_data2[[i]], pca_data2[[i]]$pa), function(x){
          kfold(x$pa, kfoldnum)
          }))
    
      out <- list()
      for(l in 1:10){     #Are 10 repeats for 10 fold enough?
          #randomly select on of the test roups to sample as the training data, 10 fold
          test <- sample(1:10, 1)
          
          train_pEOR <- speciestable[speciestable$pa == 1 & speciestable$group_pa!=test,c("x1","x2")]
          train_pHerb <- speciestable[speciestable$pa == 2 & speciestable$group_pa!=test,c("x1","x2")]
          train_bEOR <- speciestable[speciestable$pa == 3 & speciestable$group_pa!=test,c("x1","x2")]
          train_bHerb <- speciestable[speciestable$pa == 4 & speciestable$group_pa!=test,c("x1","x2")]
          
          test_pEOR <- speciestable[speciestable$pa == 1 & speciestable$group_pa==test,c("x1","x2")]
          test_pHerb <- speciestable[speciestable$pa == 2 & speciestable$group_pa==test,c("x1","x2")]
          test_bEOR <- speciestable[speciestable$pa == 3 & speciestable$group_pa==test,c("x1","x2")]
          test_bHerb <- speciestable[speciestable$pa == 4 & speciestable$group_pa==test,c("x1","x2")]
          
          me <- maxent(stackmini, p=train_pEOR, a=train_bEOR)
          meHerb <- maxent(stackmini, p=train_pHerb, a=train_bHerb)
          
          #evalidate teh model
          e <- dismo::evaluate(test_pEOR, test_bEOR, me, stackmini)
          eHerb <- dismo::evaluate(test_pHerb, test_bHerb, meHerb, stackmini)
          gc()
          
          #Visulatize predictions
          writeRaster(dismo::predict(me, stackmini),paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/EOR",g1g2names$AcceptedName[i],l,".tif", sep=""),overwrite=TRUE)
          gc()
          writeRaster(dismo::predict(meHerb, stackmini),paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/Herb",g1g2names$AcceptedName[i],l,".tif", sep=""),overwrite=TRUE)
          gc()
          thres <- dismo::threshold(e) 
          thresHerb <- dismo::threshold(eHerb)
          # 1: which group to withhold, 2: maxent for EOR, 3: maxent for Herbarium, 4: evaluate EOR, 5: evaluate Herbarium, 6: thresholds for EOR, 7: thresholds for Herbarium, 8: species
          out[[l]] <- list(test,me,meHerb,e,eHerb,thres,thresHerb,g1g2names$AcceptedName[i])#,pme,pmeHerb)
          gc()
      }
      out
  }
})

save(Maxent46species, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/Maxent46species.Rda")
```

1: which group to withhold, 2: maxent for EOR, 3: maxent for Herbarium, 4: evaluate EOR, 5: evaluate Herbarium, 6: thresholds for EOR, 7: thresholds for Herbarium, 8: species
```{r}
#Specificy=Sensitivity or Kappa??
#Make names match raster files
thres_all <- do.call(rbind,lapply(Maxent46species[lapply(Maxent46species, length)>0], function(x){
  species <- sapply(x, '[[', 8)
  EOR_th <- sapply(x, '[[', 6)
  Herb_th <- sapply(x, '[[', 7)
  EOR_ss <- unlist(EOR_th[5,]) #1:kappa, 2:spec_sens, 3:no_omission, 4:prevalence, 5:equal_sens_spec, 6:sensitivity
  HERB_ss <- unlist(Herb_th[5,])
  eorout <- data.frame(threshold=EOR_ss,Species=paste("EOR",gsub(" ","_",species),1:10,sep=""))
  herbout <- data.frame(threshold=HERB_ss,Species=paste("Herb",gsub(" ","_",species),1:10,sep=""))
  rbind(eorout,herbout)
}))

#20 species
maxentspecies <- Maxent46species[lapply(Maxent46species, length)>0]
spRasterEORvsHerb <- lapply(unlist(sapply(maxentspecies, '[[', 8)[8,]),
                            function(sp){
                              lapply(1:10, function(rep){
                                lapply(c("EOR","Herb"), function(comp){
        raster(paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/",
                     comp,sp,rep,".tif", sep=""))
                                })
                              })
                            })

  spRasterEORvsHerb[[1]][[1]][[1]]@file@name
  #Need name to match up threshold with EOR or Herb and species and which map
  spRasterEORvsHerb[[1]][[1]][[2]]@data@names   #!!! Yes!
  
  thres_all[[1]]$Species
  
  #Order of names, but can use which to match the correct threshold with its map
  mapply(function(x,y) print(unique(unlist(sapply(x, function(x1){
    sapply(x1, function(x2){
      x2@data@names
    })
  }))), unique(y$Species)),
  spRasterEORvsHerb,thres_all)

auc_all <- sapply(Maxent46species, function(x){
  species <- sapply(x, '[[', 8)
  EOR_eval <- sapply(x, '[[', 4)
  Herb_eval <- sapply(x, '[[', 5)
  EOR_auc <- sapply(EOR_eval, function(y) y@auc)
  HERB_auc <- sapply(Herb_eval, function(y) y@auc)
  data.frame(AUC_EOR=EOR_auc, AUC_Herb=HERB_auc, Species=species)
})

#make raster bricks of EOR and Herbs for each species; just species that got done 
speciesused <- unique(gsub('[[:digit:]]+', '', thres_all$Species))
unlistspRaster <- unlist(spRasterEORvsHerb)
rasterbricks <- lapply(speciesused, function(sp){
  sp1 <- unlistspRaster[grep(sp,
                               sapply(unlistspRaster, function(x) x@data@names))]
  stack(sp1)
  # eorout <- stack(sp1[grep("EOR", sapply(sp1,function(x) x@data@names))])
  # herbout <- stack(sp1[grep("Herb", sapply(sp1,function(x) x@data@names))])
  # list(eorout,herbout)
})

#Need to pick a way to make a consensus model and do same for both. 
#Should also compare niche models of both to actual distribution defined by CNHP

#need to add number and EOR or Herb to threshold extract from raster name
plot(spRasterEORvsHerb[[1]][[1]][[1]] > 
       thres_all$threshold[thres_all$Species == spRasterEORvsHerb[[1]][[1]][[1]]@data@names], 
     1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", 
     main=paste("Predicted presence of",spRasterEORvsHerb[[1]][[1]][[1]]@data@names,sep=" "))
plot(sw, add=TRUE)

```


Testing the loop for all 46 species
```{r}
group_pa <- do.call(c,sapply(1:4, function(x) kfold(pca_data2[[1]][pca_data2[[1]]$pa==x,],10))) #nope, need seperate for presence and background

#randomly select on of the test roups to sample as the training data
test <- sample(1, 1:10)

train_pEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 1 & group_pa!=test,c("x1","x2")]
train_pHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 2 & group_pa!=test,c("x1","x2")]
train_bEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 3 & group_pa!=test,c("x1","x2")]
train_bHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 4 & group_pa!=test,c("x1","x2")]

test_pEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 1 & group_pa==test,c("x1","x2")]
test_pHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 2 & group_pa==test,c("x1","x2")]
test_bEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 3 & group_pa==test,c("x1","x2")]
test_bHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 4 & group_pa==test,c("x1","x2")]

me <- maxent(stackmini, p=train_pEOR, a=train_bEOR) #put maxent.jar into the java folder of dismo and changed  regedit.exe, navigate to HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft, right click and change permissions to full operation.


#validate teh model
e <- dismo::evaluate(test_pEOR, test_bEOR, me, stackmini)

#Visulatize predictions
pred_me <- dismo::predict(me, stackmini)
thres <- dismo::threshold(e)

plot(pred_me > thres$spec_sens, 1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", 
     main=paste("Predicted presence of",g1g2names[[1]][1],sep=" "))
plot(sw, add=TRUE)
```

Steven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.1). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2018-10-7.

https://stackoverflow.com/questions/32915628/best-way-to-create-response-curves-for-a-glm-species-distribution-model-in-r 
```{r}
corout <- as.data.frame(as.table(cor(pca_data[[1]][,11:length(pca_data[[1]])])))
corout[corout$Freq<0.60,]


m1 <- glm()
```



#Random forest - classification and regression trees   
Either a formula, or (1) data.frame with predictor variables, (2) vector with response. categorical will classification, interval will do regression. 
```{r}

library(randomForest)

bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)


plot(bioclim.colorado$bio1_12)
plot(colocounties, add=TRUE)

#When loop, use "i in want" or lapply(want, function(i)) which are non-NA in the list for distXsp[[i]][[1]]@coords

#need back to lat long
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[1]]][[1]], CRS("+proj=longlat +datum=WGS84"))
}

##COME BACK TO THIS AND SEE WHAT NEEDS TO BE Done to do random forest
presvals <- extract(bioclim.colorado, )


library(dismo)

kfoldnum <- 5

#Make a training and testing set, will want to iterative over all sets, keeping one out each time
#lapply(1:kfoldnum, function(x){})

group <- kfold(distXsp[[1]][[1]]@coords, kfoldnum)
pres_train <- distXsp[[1]][[1]]@coords[group != 1,]
pres_test <- distXsp[[1]][[1]]@coords[group == 1,]

ext <- extent(-109, -102, 36, 41)
backg <- randomPoints(bioclim.colorado, n=300, ext=ext, extf=1.25)
colnames(backg) <- dimnames(pres_test)[[2]]
groupbk <- kfold(backg, kfoldnum)
backg_train <- backg[groupbk != 1,]
backg_test <- backg[groupbk == 1,]






```

