---
title: "Herbarium Bias CNHP"
author: "Michelle DePrenger-Levin"
date: "December 4, 2017"
output: html_document
---


cd /p/hackathon/Simulations/

```{r}
rm(list=ls())

# install.packages("NicheMapR") based on animal needs anyway, mechanistic
library(ggplot2)
library(rgeos)
library(sp)
# library(spdep)
library(rgdal)
library(maptools)
library(raster)
library(Taxonstand)
# library(RCurl)
# library(taxize)
# library(ggmap)

library(dismo)
library(ENMeval)
library(biomod2)
library(spThin)

library(tidyr)
library(stringr)
library(magrittr)
library(prism)

library(doParallel)
library(parallel)
library(foreach)
require(lme4)
require(lattice)
require(AICcmodavg)


     library(splancs)

# #Skip making these, takes a long time
# load("P:/hackathon/Simulations/circleEOR.Rda")
# load("P:/hackathon/Simulations/circleHerb.Rda")
# load("P:/hackathon/Simulations/EORgrid.thin.Rda")
# # load("P:/hackathon/Simulations/circleEOR.Rda")
# load("P:/hackathon/Simulations/envs.backgEOR.Rda")
# load("P:/hackathon/Simulations/envs.backgHerb.Rda")
# load("P:/hackathon/Simulations/Herb.thin1000.Rda")
# load("P:/hackathon/Simulations/EORgrid.thin.Rda")

load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2names.Rda")

load("P:/hackathon/Simulations/coloradosps.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorsinglecell.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorandherb.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp1.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herbpost1980.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorpost1980.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data2.Rda")
# load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/Maxent46species.Rda")

colocounties <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties_wgs84")
colocounties.UTM <- readOGR(dsn="Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/CO_Counties", layer="counties")


ScaleBar <- function(reference_raster_utm, round_to_nearest_km, width_percent, y_percent_from_bottom, x_percent_from_left, y_text_percent_from_bottom, ...) {
    # Round by max to nearest... e.g. 5 km 
    mround <- function(x,base){ 
        base*round(x/base) 
    }   
    # scale bar size adjustment to avoid decimals
        scale_size <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*width_percent)/1000
        scale_size_adj <- mround(scale_size, round_to_nearest_km)
        scale_size_adj_plot <- (scale_size_adj*1000)/2
    # Horizontal percent position (x) for scale bar
        x_position <- ((xmax(reference_raster_utm)-xmin(reference_raster_utm))*x_percent_from_left)+xmin(reference_raster_utm)
    # Vertical percent position y for scale bar
        y_position <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_percent_from_bottom)+ymin(reference_raster_utm)
        y_position_text <- ((ymax(reference_raster_utm)-ymin(reference_raster_utm))*y_text_percent_from_bottom)+ymin(reference_raster_utm)
    # Draw line on plot
        library(sp)
        x_ends <- c((x_position-scale_size_adj_plot), (x_position+scale_size_adj_plot))
        y_ends <- c((y_position), (y_position))
        scale_bar_line <- SpatialLines(list(Lines(Line(cbind(x_ends, y_ends)), ID="length")))
        projection(scale_bar_line) <- projection(reference_raster_utm)
        plot(scale_bar_line, add=TRUE, ...)
        text(x_position, y_position_text, paste0(scale_size_adj, "km"))
}


```
Make grid points and thin to the number of herb points right away. 
```{r}
thin.max <- function(x, cols, npoints){
  #Create empty vector for output
  inds <- vector(mode="numeric")
  
  #Create distance matrix
  this.dist <- as.matrix(dist(x[,cols], upper=TRUE))
  
  #Draw first index at random
  inds <- c(inds, as.integer(runif(1, 1, length(this.dist[,1]))))
  
  #Get second index from maximally distant point from first one
  #Necessary because apply needs at least two columns or it'll barf
  #in the next bit
  inds <- c(inds, which.max(this.dist[,inds]))
  
  while(length(inds) < npoints){
    #For each point, find its distance to the closest point that's already been selected
    min.dists <- apply(this.dist[,inds], 1, min)
    
    #Select the point that is furthest from everything we've already selected
    this.ind <- which.max(min.dists)
    
    #Get rid of ties, if they exist
    if(length(this.ind) > 1){
      print("Breaking tie...")
      this.ind <- this.ind[1]
    }
    inds <- c(inds, this.ind)
  }
  
  return(x[inds,])
}

```

Want to use elevation, slope and aspect, ruggedness
```{r}

# coElev <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/CO_Mosaic_Elevation_VT/co_elev_VT_WGS84.tif")
# 
# coElev_biosize <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Elevation_VT/co_elev_WGS84_30sec.tif")
# 
# coAspect <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Aspect_VT/co_aspect_VT_WGS84.tif")
# 
# coSlope <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Slope_VT/co_slope_VT_WGS84.tif")

coElev <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/coplus50int.tif")

coAspect <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/aspect50int.tif")

coSlope <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/COplus_slope50.tif")

coRugged <- raster("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/NationalMapUSDA_DEM_aroundcolorado/COplus_ruggedInt50.tif")


```



```{r}

l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")

#Each polygon might be made of multiple disconnected polygons. Need to separate and label
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))


# Could take all the synonyms and names of l1G1G2 to make the list of names to pull from SEINet big list
namesg1g2 <- table(l1G1G2$GNAME)
length(namesg1g2[namesg1g2 > 0]) #60 species
g1g2names <- TPL(names(namesg1g2[namesg1g2 > 0]))
table(g1g2names$Taxonomic.status)
# 2 not in there, "Gutierrezia elegans" "Oonopsis sp. 1", 10 synonyms, 3 unresolved
# save(g1g2names, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2names.Rda")

c
g1g2names[g1g2names$Taxonomic.status == "Unresolved",]
#Gilia sedifolia
#Ipomopsis ramosa
#Packera mancosana


# Check when names are good or not from herbarium specimens
seinet <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/seinet_colorado_with_coordinates_ALLG1G2.csv")

seinet <- seinet[seinet$genus != "",]

#table(seinet$basisOfRec) #PreservedSpecimen: 1871 and HumanObservation: 4
seinet <- seinet[seinet$basisOfRecord != "HumanObservation",]
namesSEINet <- names(table(seinet$scientificName)) #93 names
SEINet_names <- TPL(namesSEINet)
#table(SEINet_names$Taxonomic.status) #
#             Accepted    Synonym Unresolved 
#         2         75          9          7 
SEINet_names$scientificNameAccpt <-paste(SEINet_names$New.Genus, SEINet_names$New.Species)
#write.table(SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)], "clipboard", sep="\t", row.names=FALSE)
#SEINet_names[SEINet_names$Taxonomic.status != "Accepted",c(1,14,16,12)]

seinet_TPL <- merge(seinet, SEINet_names, by.x = "scientificName", by.y = "Taxon")
#head(seinet_TPL) # all the information from herbarium collections and if they're accepted names or not

seinet_TPL$family <- toupper(seinet_TPL$family)

#seinet_TPL[seinet_TPL$institutionCode=="USU",]
#seinet_TPL$institutionCode[seinet_TPL$institutionCode=="USU"] <- "USUUB"

namesbyyear <- table(seinet_TPL$Taxonomic.status,seinet_TPL$year)
plot(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[2,],col="blue", type="l",
     xlab="Year",ylab="Specimens", ylim=c(0,max(namesbyyear))) #accepted
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[3,],col="green") #synonym
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[4,],col="orange") #Unresolved
lines(as.numeric(dimnames(namesbyyear)[[2]]),namesbyyear[1,],col="pink") #not in TPL
```
#Families collected over time
```{r}
familyxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$family)) #now changed toupper lowercase $family has 44 levels, inconsistent capitals and lowercase..
familyxyear$Var1 <- as.numeric(as.character(familyxyear$Var1))

library(lattice)
xyplot(Freq ~ Var1|Var2, familyxyear[familyxyear$Freq>0,],
       type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Year of Collection",
       ylab = "Number of Collections", aspect = "xy")

herbariumxyear <- data.frame(table(seinet_TPL$year,seinet_TPL$institutionCode)) 
ggplot(herbariumxyear[herbariumxyear$Freq>0,],aes(Var1, Freq, colour=Var2 ))+
  geom_point()+
  theme_bw()

```

Cite MODIS data: <https://lpdaac.usgs.gov/citing_our_data>   
<https://lpdaac.usgs.gov/data_access/data_pool>  - need to get data but weeky mainenance is every wednesday until noon   
Got snow data https://search.earthdata.nasa.gov/data/retrieve/5396545932 can compare to NCAR's data.   
got some veg data or getting https://search.earthdata.nasa.gov/data/retrieve/2536564325  
```{r}
library(ncdf4)
#library(maptools)
#library(extRemes)
library(fields)
library(abind)
library(prism)

library(raster)
library(rasterVis)
#library(measurements)
library(ENMeval)
library(RCurl)
library(dismo)

#install.packages("digest")
library(devtools)
#install_github('hadley/rvest')
library(rvest)

# add the command to bind along the 3rd dimension
abind3 <- function(...) { abind(along = 3, ...) }

```



#Michelle DePrenger-Levin      
In ArcMap do this: <https://blogs.esri.com/esri/arcgis/2010/09/16/nearbygroup/>
repeat distance analysis in R          
"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet        
Only G1G2 EORs (to limit range of species)
"colonames" == TPL synonomy for all coloradosps names
```{r}
coloradosps <- read.csv("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/SEINET G1 G2 Vouchers/occurrences.csv")

#save(coloradosps, file= "P:/hackathon/Simulations/coloradosps.Rda")
#nrow(coloradosps) #567328

howlongrecordedby <- nchar(as.character(coloradosps$recordedBy))
recordedby <- unique(coloradosps$recordedBy[howlongrecordedby < 100])

#table(coloradosps$basisOfRecord) # PreservedSpecimen: 566067; preservedspecimen: 55; Preserved specimen: 143; Photograph: 1; Observation: 124; Native and Naturalized Flora of; HumanObservation; many just wrong column info in this


# both of these from OBI: California Polytechnic State University
#coloradosps[coloradosps$basisOfRecord == "Plants of Jefferson County Open",] #68 plus 1 more with some other typo
#coloradosps[coloradosps$basisOfRecord == "Plants of Colorado",] #68 plus 1 more with some other typo

#Subset by just preserved specimens  
coloradosps$basisOfRecord <- as.character(coloradosps$basisOfRecord)
coloradosps.specimens <- coloradosps[grep("specimen",coloradosps$basisOfRecord,
                                          ignore.case=TRUE),]
#table(coloradosps.specimens$basisOfRecord)

coloradosps.specimens <- coloradosps.specimens[nchar(coloradosps.specimens$basisOfRecord)<50,]

#Now only preserved specimens
coloradosps <- coloradosps.specimens
#nrow(coloradosps) #566265

g1g2names$AcceptedName <- paste(g1g2names$New.Genus,g1g2names$New.Species)
#match SEINet names to either the given or accepted names from G1/G2s
#length(unique(g1g2names$Taxon))
#length(unique(g1g2names$AcceptedName)) #60
#length(unique(c(g1g2names$Taxon,g1g2names$AcceptedName))) #71 


rowstomatch <- lapply(c("Taxon","AcceptedName"), function(x){
   coloradosps[,"scientificName"] %in%  g1g2names[,x]
})

#The rows to keep that match either an accepted or synonym or unresolved name for a G1G2 species
#coloradosps.specimens was intermediate to get rid of none specimen records
coloradosps.g1g2 <- unique(rbind(coloradosps[rowstomatch[[1]],],coloradosps[rowstomatch[[2]],]))
nrow(coloradosps.g1g2)
hist(as.numeric(as.character(coloradosps.g1g2$decimalLatitude)))
#write.table(sort(unique(coloradosps.g1g2$scientificName)), "clipboard", sep="\t", row.names=FALSE)
plot(as.numeric(as.character(coloradosps.g1g2$decimalLongitude)),
     as.numeric(as.character(coloradosps.g1g2$decimalLatitude)),
     xlab="Longitude",ylab="Latitude",pch=16,cex=0.5)
```
How many are within 0.01 decimals of each other, round to two decimals - per Soltis 
```{r}
coloradosps.g1g2$decimalLatitude <- 
  as.numeric(as.character(coloradosps.g1g2$decimalLatitude))
coloradosps.g1g2$decimalLongitude <- 
  as.numeric(as.character(coloradosps.g1g2$decimalLongitude))
# ctrl+shift+c to comment/uncomment
# coloradosps.g1g2[which(coloradosps.g1g2$decimalLatitude <37),]
# coloradosps.g1g2[which(coloradosps.g1g2$decimalLongitude < -109.2),]

coloradosps.g1g2$lon2 <- round(coloradosps.g1g2$decimalLongitude,2) # about 1.1 km 
coloradosps.g1g2$lat2 <- round(coloradosps.g1g2$decimalLatitude,2)

coloradosps.g1g2$lon3 <- round(coloradosps.g1g2$decimalLongitude,3) # about 110 m
coloradosps.g1g2$lat3 <- round(coloradosps.g1g2$decimalLatitude,3)

#convert points from WGS84 to NAD83
projected <- coloradosps.g1g2[complete.cases(coloradosps.g1g2[,c("lat2","lon2")]),]
coordinates(projected) <- ~ lon2+lat2
proj4string(projected) <- CRS("+proj=longlat +datum=WGS84") 
projected <- spTransform(projected, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# write.csv(coloradosps.g1g2, "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/VisTrails/FieldData_EORG1G2_WGS84.csv")
```


There seems to be lots of points in Denver but no EOs!    
gray60 or 80, black dots for publication   , col="midnightblue"
```{r}

plot(colocounties.UTM,border="gray50") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="gray60", add=TRUE, lwd=5) #
plot(projected, add=TRUE, pch=20, cex=0.5)


polys_46 <- l1G1G2[l1G1G2$GNAME %in% as.character(mapply(FUN=function(x) unique(x$GNAME)[1],pca_herb_SAE)),]
plot(polys)
plot(polys_46, border="pink", add=TRUE)
str(as.character(mapply(FUN=function(x) unique(x$GNAME)[1],pca_herb_SAE)))

jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/HerbPntsEORs_CO.jpg",
     width=250, height=175,units='mm', res=300)

plot(colocounties.UTM,border="gray70") #colocounties is in WGS84, not in same datum as l1G1G2
plot(polys_46, border="gray60", add=TRUE, lwd=4) #
plot(projected, add=TRUE, pch=20, cex=0.5)
ScaleBar(colocounties.UTM,
             10, .25, .05, .85, .10, lwd=1)

dev.off()
```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/EORs.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="goldenrod", add=TRUE, lwd=5) #☻
#plot(projected, add=TRUE, pch=20, cex=0.5)

dev.off()

```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/herbs.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
#plot(l1G1G2, border="blue", add=TRUE, lwd=5) #☻
plot(projected, col = "blue", add=TRUE, pch=20, cex=0.5)

dev.off()

```

```{r}
jpeg("Q:/Research/Presentations/External Conference presentations/Natural Areas Conference 2018/both.jpg",
     width=170, height=150,units='mm', res=300)

plot(colocounties.UTM,border="gray30") #colocounties is in WGS84, not in same datum as l1G1G2
plot(l1G1G2, border="goldenrod", add=TRUE, lwd=5) 
plot(projected, col="blue", add=TRUE, pch=20, cex=0.5)

dev.off()

```
       [1] WGS84                     WGS 84                    WGS 1984                      
       [4] WGS96                     WGS 72                    WGS-84                     
       [7] WGS 1984.                 WGS83                     WGS84 (UTM Datum: NAD83)   
      [10] WGS84  (UTM Datum: NAD83) WGS84, Google Earth       WGS 1983                   
      [13] WGS 1984,                 WGS98                     WGS85                       
      [16] WGS86                     WGS89                     WGS95                       
      [19] WGS87                     WGS88                     WGS97                       
      [22] WGS92                     WGS 83                    WGS93                       
      [25] WGS94                     WGS84/NAD83               WGS90                        
      [28] WGS 19                    WGS99                     WGS91      
      
      "The latest revision is WGS 84 (also known as WGS 1984, EPSG:4326), established in 1984 and last revised in 2004.[1] Earlier schemes included WGS 72, WGS 66, and WGS 60. WGS 84 is the reference coordinate system used by the Global Positioning System."


Trying to reduce memory before running 
        > rm(colocounties)
        > rm(colocounties.UTM)
        > rm(coloradosps.g1g2)
        > rm(coloradosps.specimens)
        > rm(familyxyear)
        > rm(herbariumxyear)
        > rm(projected)

#Already loaded! Skip
```{r}
# oh no, want only after 1980 collections

distXsp_v3 <- lapply(1:60, function(i){
  gc()
  g1g2now <- coloradosps.g1g2[coloradosps.g1g2$scientificName %in%
                                c(g1g2names$Taxon[i],g1g2names$AcceptedName[i])&
                                !is.na(coloradosps.g1g2$decimalLatitude),]
  
  g1g2now <- g1g2now[as.numeric(as.character(g1g2now$year))>1980,]
  g1g2now <- g1g2now[g1g2now$decimalLatitude>0,]
  g1g2now <- g1g2now[grepl("Specimen",g1g2now$basisOfRecord),]
  g1g2now <- g1g2now[,c(1:2,14:20,30:38,41,43:44,53:62,68:70,83:86)]
  gc()
  
  
  if(nrow(g1g2now)>0){
  # put grid points across colorado, sample from it for each polygons
    polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                        g1g2names$Taxon[i]),] 
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")

    
    totalarea <- sum(area(disaggregate(polys)))
  # area of each polygon
    areanow <- sapply(slot(polys,"polygons"), slot, "area")
    
  # Proportional area to the whole area number to thin
    EORpnts <- do.call(rbind, lapply(1:length(polys), function(f){
      outline <- polys@polygons[[f]]@Polygons[[1]]@coords
      grid <- makegrid(polys, cellsize = 50) # cellsize in map units!
      names(grid) <- c("x","y")
      gridout <- grid[inout(grid,outline), ]
      if(nrow(gridout)>1){
        gridthin <- thin.max(gridout,c("x","y"),round(nrow(g1g2now)*(areanow[[f]]/totalarea),0))
        gc()
        } else {
          if(nrow(gridout)>0) {
            gridthin <- gridout 
          } else {
            gridthin <- NULL
          }
          }
      gridthin
      }))
    gc()
    
  #turn into spatialpointsdataframe for individual species
    coordinates(g1g2now) <- ~decimalLongitude+decimalLatitude
    proj4string(g1g2now) <- CRS("+init=epsg:4326")  # CRS("+proj=longlat +datum=WGS84")
    g1g2now <- spTransform(g1g2now, CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
  # The minimum distance from each point to the nearest polygon
    distnow <- apply(gDistance(g1g2now,polys, byid=TRUE),2,min)
    gc()
    
    out <- list(g1g2now,distnow, areanow, EORpnts)
    gc()
    out
    }
  })

save(distXsp_v3, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_v3.Rda") 

```




# testing background points   
either use KDE with kernelUD or Minimum convex polygon (MCP)
```{r}
library(adehabitatHR)
library(ks)
class(distXsp_v3[[1]][[1]])

xy <- data.frame(distXsp_v3[[1]][[1]]@coords)
names(xy) <- c("X","Y")
# coordinates(xy) <- ~ X+Y
xy <- SpatialPoints(xy)

# If added in points alnog roads then could make the contour even closer to parts where likely surveyed. 
# 95% home range contour
ud <- kernelUD(xy, extent=0.5, grid=150)
ver <- getverticeshr(ud, 95)

# kde.foo <- kde(xy, xmin=c(-5,-5), xmax=c(5,5), bgridsize=c(151,151))
# just put random points for background points witin the kernelUD 
bgpts <- spsample(ver, 300, "stratified")

verMCP <- mcp(xy, percent=95)

plot(ver)
# plot(verMCP, col="blue", add=TRUE)
points(bgpts, col="red")
points(xy, col="green")
points( spsample(ver, 300, "nonaligned"), col="black", pch=16)
points( spsample(ver, 300, "random"), col="orange", pch=3)

# If you want to introduce bias:
mm <- ud@coords[order(ud@coords[,1], ud@coords[,2]),]
mm
```



```{r}

distXsp1 <- mapply('[[', distXsp, 1)
distXsp1 <- do.call(rbind,distXsp1[!is.na(distXsp1)])
plot(dist~as.factor(scientificName), distXsp1[distXsp1$dist<500000,], las=2)

 # distXsp1_tpl <- TPL(distXsp1$scientificName)
# save(distXsp1_tpl, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp1_tpl.Rda")
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp1_tpl.Rda")

nrow(distXsp1);nrow(distXsp1_tpl)
distXsp1 <- data.frame(distXsp1,distXsp1_tpl)

plot(dist~as.factor(Family), distXsp1[distXsp1$dist<500000,], las=2,
     xlab="",ylab="")

#Write table for VisTrails field data
#nrow(seinet_TPL)
# seinet_TPL.cc <- seinet_TPL[seinet_TPL$decimalLongitude > -110,]
# seinet_TPL.cc <- seinet_TPL.cc[seinet_TPL.cc$decimalLatitude > 35, ]
# 
# plot(seinet_TPL.cc$decimalLongitude,seinet_TPL.cc$decimalLatitude)
# 
# write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2.csv")
# 
# write.csv(seinet_TPL.cc, "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Alpine_Phenology_2/Species_SEINet/FieldData_EORG1G2.csv")

```



Get the coordiates from each saved to use as csvs in VisTrails. 
```{r}

#distXsp[[i]][[1]] are the herbarium points, SpatialPointsDataFrame
want <- which(!is.na(mapply('[[', distXsp, 1)))
wantEOR <- which(!is.na(mapply('[[', distXsp, 4)))


#points in the EORs
EORsxsp <- mapply('[[', distXsp, 4)

# ctrl+shift+c to uncomment out
# for(i in wantEOR){
#   write.csv((EORsxsp[[i]]@coords),
#             paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORG1G2",g1g2names$AcceptedName[i],".csv", sep=""))
# }
# 
# 
# # make them match the number of points that the herbarium specimens have
# for(i in wantEOR){
#   if(nrow(EORsxsp[[i]]@coords)>herbspecimennumxsp[i]){
#   write.csv(EORsxsp[[i]]@coords[sample(1:nrow(EORsxsp[[i]]@coords), herbspecimennumxsp[i]),],
#             paste("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/FieldData_EORsamplesizematch",g1g2names$AcceptedName[i],".csv", sep=""))
#   }
# }

# spatial thinning instead of just matching the points that the herbarium specimens have. Spatially thin the herbarium specimens as well. 

```

    

#Instead of PRISM, use the elevation, slope, and aspect   
```{r}
rasterstack_2 <- stack(list(coElev, coAspect, coSlope))

rscrs_2 <- rasterstack_2@crs@projargs # "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

```


  

Test a PCoA for all grid EOR points and all Herb points. Then limit by year, can we figure out which EOR are historic points? 
```{r}

options(prism.path = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Prism_Climate_Data/30Year_Normal") 

#get_prism_normals(type="tmax", resolution = "800m", keepZip = FALSE, mon = 1:12)
#get_prism_normals(type="tmin", resolution = "800m", keepZip = FALSE, mon = 1:12)
#get_prism_normals(type="ppt", resolution = "800m", keepZip = FALSE, mon = 1:12)

# stack rasters
rasterstack <- ls_prism_data() %>% prism_stack(.) # many skipping the first 6 that are provisional
#get the projection from the raster stack
rscrs <- rasterstack@crs@projargs # "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
```


# Already loaded, skip!
```{r, eval=FALSE}

pca_eorandherb <- lapply(1:nrow(g1g2names), function(sp){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[sp],
                                         g1g2names$Taxon[sp]),] 
      #area of each polygon
      areanow <- sapply(slot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                             g1g2names$Taxon[i]),], "polygons"), 
                        slot, "area")

     cellsize <- 100
     grid <- lapply(1:length(polys), function(i){
       if(sqrt((polys[i,]@bbox[3]-polys[i,]@bbox[1])*(polys[i,]@bbox[4]-polys[i,]@bbox[2]))/cellsize < 1){
         grid <- makegrid(polys[i,], cellsize = 10, pretty=FALSE) #0.1km
         gridout <- SpatialPointsDataFrame(grid, data.frame(id=1:nrow(grid),polyid=i,
                                                            polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
           gridout
         } else {
           grid <- makegrid(polys[i,], cellsize = cellsize, pretty=FALSE) #1km 
           gridout <- SpatialPointsDataFrame(grid, data.frame(id=1:nrow(grid),polyid=i,
                                                              polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
           gridout
           }
       })
     

     ov <- lapply(1:length(polys), function(i){
         out <- grid[[i]][polys[i,],]
         out
         })

     eor_data <- lapply(1:length(ov), function(x){
       if(nrow(ov[[x]]@data)==0){
         center <- gCentroid(polys[i,])
         centerout <- SpatialPointsDataFrame(center, data.frame(id=1,polyid=x,
                                                                polys@data$PolyID@data[i,]),
                                             proj4string = CRS(proj4string(polys[i,])))
         out <- spTransform(centerout, CRS(paste(rscrs)))
         out
       } else {
         out <- spTransform(ov[[x]], CRS(paste(rscrs)))
         out
       }
     })
     
     mergedEORs <- do.call(rbind,eor_data)
     data_eor <- data.frame(coordinates(mergedEORs),id=mergedEORs$id,polyid=mergedEORs$polyid,
                            Bestcrs=mergedEORs$BESTSRC, firstobs=mergedEORs$FIRSTOBS, 
                            GRANK=mergedEORs$GRANK,
                            GNAME=mergedEORs$GNAME,ENDEMIC=mergedEORs$ENDEMIC, 
                            Lastobs=mergedEORs$LASTOBS,dist=NA,area=sum(areanow),
                            extract(rasterstack, mergedEORs))
     data_eor
})

 save(pca_eorandherb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorandherb.Rda") 

```


Get rid of data from polygons only seen before 1981
get rid of repeted values for all the climate things, do only one row for each cell
Column 4 is each polygon's id so there's only one point for each raster grid over a polygon but at least one point per polygon.   
#Already loaded, skip!
```{r}

pca_eorandherb[[1]][!duplicated(pca_eorandherb[[1]][,grep("PRISM",colnames(pca_eorandherb[[1]]))]),]

pca_eorsinglecell <- lapply(pca_eorandherb, function(x){
  x[!duplicated(x[,grep("PRISM",colnames(x))]),]
})

pca_eorpost1980single <- lapply(pca_eorandherb, function(x){
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  out <- x[!duplicated(x[,c(4,11:length(x))]),]
  out
})

pca_eorpost1980 <- lapply(pca_eorandherb, function(x){
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  x
})

save(pca_eorsinglecell, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorsinglecell.Rda")
save(pca_eorandherb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorandherb.Rda")
save(pca_eorpost1980, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorpost1980.Rda")
save(pca_eorpost1980single, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_eorpost1980single.Rda")


```


```{r}

#make into spatialpointsdataframes, make in UTM like EORs
eorsinglecell <- lapply(pca_eorsinglecell, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})

eorlotsofcell <- lapply(pca_eorandherb, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})

eorpost1980 <- lapply(pca_eorpost1980, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS(paste(rscrs))
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})


polys <- lapply(1:nrow(g1g2names), function(i){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),]
     polys
     })
```

Visual checks of data
```{r}
polys <- lapply(1:nrow(g1g2names), function(i){
  #put grid points across colorado, sample from it for each polygons
     polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                         g1g2names$Taxon[i]),]
     polys
     })

eorpost1980 <- lapply(pca_eorpost1980, function(x){
  coordinates(x) <- ~ x1+x2
  proj4string(x) <- CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
  projectedx <- spTransform(x, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
})

for(j in 10:13){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue",lwd=3, main=polys[[j]][i,]@data$LASTOBS)
    plot(eorpost1980[[j]], pch=21, col="red",add=TRUE)
    plot(polys[[j]], border="yellow", add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .05, .20, .95, .17, lwd=1) #first is round to nearest km
  }
}


for(j in 3:5){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue", main=polys[[j]][i,]@data$LASTOBS)
    plot(eorlotsofcell[[j]], pch=21, col="red",add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
  }
}


for(j in 3:5){
  for(i in 1:length(polys[[j]])){
    plot(polys[[j]][i,], border="blue", main=polys[[j]][i,]@data$LASTOBS)
    plot(eorpost1980[[j]], pch=21, col="red",add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
  }
}

for(i in 10:23){
  plot(polys[[i]], border="goldenrod", lwd=10, main=polys[[j]][i,]@data$LASTOBS)
  plot(eorsinglecell[[i]], add=TRUE)
    ScaleBar(polys[[j]][i,],
             0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
}

for(i in 1:length(polys)){
  plot(polys[[i]], border="blue")
  plot(grid[i], pch=20, cex=0.5, add=TRUE)
  ScaleBar(polys[[i]],
           0.01, .25, .20, .15, .17, lwd=2) #first is round to nearest km
}

```

#Already loaded, skip
```{r}
#The first in all these are the herbarium records
(distXsp[[1]][[1]]) #+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0   Need to transform

distXsp1_pca <- do.call(c,mapply('[[', distXsp, 1))
distXsp1_pca <- distXsp1_pca[!is.na(distXsp1_pca)]

pca_herb <- lapply(distXsp1_pca, function(x){
#  transdf <- data.frame(distXsp[[x]][[1]],dist=distXsp[[x]][[2]],area=distXsp[[x]][[3]])
  out <- spTransform(x, CRS(paste(rscrs)))
  pca_herbout <- data.frame(coordinates(out),id=out$id,polyid=NA,
                            Bestcrs=out$institutionCode, firstobs=out$eventDate, 
                            GRANK=NA,
                            GNAME=out$scientificName,ENDEMIC=NA, 
                            Lastobs=out$eventDate,dist=out$dist,area=out$area,
                            extract(rasterstack, out))
  pca_herbout
})


save(pca_herb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herb.Rda")

# since sampling bias is known issue, only one point per climate raster value (columns 11:length) are kept; don't know if sampling density is due to bias or higher area of suitability. hummm. 
pca_herbpost1980single <- lapply(pca_herb, function(x){
  colnames(x) <- c("x1","x2",colnames(x)[3:length(x)])
  x <- x[(which(nchar(as.character(x$Lastobs)) == 10)),]
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  out <- x[!duplicated(x[,c(11:length(x))]),]
  out
})

pca_herbpost1980 <- lapply(pca_herb, function(x){
  colnames(x) <- c("x1","x2",colnames(x)[3:length(x)])
  x <- x[(which(nchar(as.character(x$Lastobs)) == 10)),]
  x <- x[!grepl("-99",x$Lastobs),]
  x <- x[(as.Date(x$Lastobs)>"1980-12-31"),]
  x
})

save(pca_herbpost1980, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herbpost1980.Rda")

save(pca_herbpost1980single, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_herbpost1980single.Rda")

g1g2names[g1g2names$Family=="Plantaginaceae",]#Add the Family for Gutierrezia: Asteraceae  and Oonopsis: Asteraceae
g1g2names$Family[g1g2names$Family==""] <- "Asteraceae"
g1g2names$Family[g1g2names$Family=="Compositae"] <- "Asteraceae"
g1g2names$Family[g1g2names$Family=="Leguminosae"] <- "Fabaceae"
# 
# want <- which(!is.na(mapply('[[', distXsp, 1)))
# wantEOR <- which(!is.na(mapply('[[', distXsp, 4)))
# 
# # Not going to work for the ones missing Herbarium records
# data.frame(distXsp[[4]][[1]],g1g2names[4,])
# 
# x_Herbs= mapply('[[', distXsp, 1)
# x_EORs <- mapply('[[', distXsp, 4)
# 
# x_Herbs[[1]]@coords
# 
# ToVisTrails_G1G2Herb <- do.call(rbind,
#                                 lapply(want, function(x) data.frame(x_Herbs[[x]],g1g2names[x,])))
#                                 
# # ToVisTrails_G1G2EOR <- data.frame(matrix(ncol = length(ToVisTrails_G1G2Herb), nrow = 0))
# # colnames(ToVisTrails_G1G2EOR) <- colnames(ToVisTrails_G1G2Herb)
# 
# EOR_G1G2tomerge <- do.call(rbind,
#                                 lapply(wantEOR, function(x) data.frame(x_EORs[[x]],g1g2names[x,])))
# 
# 
# names(ToVisTrails_G1G2Herb)
# names(EOR_G1G2tomerge)
# 
# head(ToVisTrails_G1G2Herb[,c(89:90,92:117)])
# head(EOR_G1G2tomerge)
# 
# write.csv()

```

select background points that are closer to road so pick more background points around roads and most where buffer around population and roads overlap.     
Or like Wunder's student's bat paper, just use random points from the circles made earlier, just a buffer, cite Elith   
Or de-bias presence records for areas near roads. Phillips et al. 2009: design selection of background data to reflect same sample selection bias as occurrence data.   
```{r}
coroads <- shapefile("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Roads/Local Roads/Local Roads SHP/LROADS.shp")

#Planer
proj4string(coroads) #"+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
r <- raster(extent(coroads))
proj4string(r) <- proj4string(coroads)
r.smaller <- disaggregate(r, fact=2)

dd <- gDistance(coroads, as(r, "SpatialPoints"), byid=TRUE)
r[] <- apply(dd,1,min)

dd.smaller <- gDistance(coroads, as(r.smaller, "SpatialPoints"), byid=TRUE)
r.smaller[] <- apply(dd.smaller,1,min)

toraster <- raster(extent(r.smaller))
proj4string(toraster) <- rscrs

r.longlat <- projectRaster(r.smaller,crs = rscrs)

plot(r)
plot(r.smaller)

plot(r.longlat)
plot(circleHerb[[2]], add=TRUE)

plot(rasterstack[[1]], xlim=c(-109,-103), ylim=c(37,41))

plot(r.smaller[r.smaller<8000])
plot(coroads, add=TRUE)
```

Either background points are selected with more frequency closer to roads or 'background' is a buffer around roads and around points and selected randomly within that overlap - all need to match the number of presence points. 

Without quality presence–absence data, discrimination metrics such as TSS can be misleading measures of model performance
Boris Leroy  Robin Delsol  Bernard Hugueny  Christine N. Meynard  Chéïma Barhoumi Morgane Barbet‐Massin  Céline Bellard
First published: 02 July 2018    
   
1. Add up all the distances as proportions or chance of getting to these, act as contributions like fecundity but in reverse.   
2. OR, know the number of grid cells within a few of the polygon, more likely to have surveyed closer to a road and closer to the point or line of the EOR polygon.   
3. will want proportionally close number of presence and background points. so use Binomial distribution to figure out probability of placing random background points in a cell and at what frequency.. over cells? randomly across background area? Use Bernolli to select which cells to include? http://users.stat.ufl.edu/~abhisheksaha/sta4321/lect12.pdf       
4. use makegrid with n = the probability? 




 From VisTrails   
Need to test overlap amount.   
Calculate the amount of overlap between rasters
```{r}
DrEx_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

DrEx <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata/Maxent_1/maxent_bin_map.tif")

#DrExEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Draba exunguiculata_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1
#EOR <- DrEx_EOR

plot(DrEx_EOR)
plot(DrEx)
#plot(DrExEORmatchss)

AsMi_EOR <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EOR/Maxent_EORvHerb_1/maxent_bin_map.tif")

AsMi <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus/Maxent_1/maxent_bin_map.tif")

#Following my code from Elevation_nichemodels_Vistrails.Rmd, make overlap be 3, 2 be EOR, and Herbarium be 1

AsMiEORmatchss <- raster("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/NicheModels/Astragalus microcymbus_EORsamplesizematch/Maxent_EORvHerb_1/maxent_bin_map.tif")


plot(AsMi_EOR,main="EOR")
plot(colocounties, add=TRUE)
plot(AsMi, main="Herb")
plot(colocounties, add=TRUE)
plot(AsMiEORmatchss, main="EOR sample size match herb")
plot(colocounties, add=TRUE)
points(projected[grep("micro", projected$scientificName),])


#Make a copy of each to compare overlap O1 and O2
OAsMiEormatch <- AsMiEORmatchss
Oasmi <- AsMi 

#Where there is overlap between AsMiEormatchss and AsMi assign a 2 instead of 1 to AsMiEormatchss
OAsMiEormatch[Oasmi==1] <- 2 #change to 2 where overlap
out <- AsMi+OAsMiEormatch    #When added together will be 1 if only in one, 2 if only in AsMiEormatchss, 3 if in both
outcat <- ratify(out)        #To add ID for each
levelplot(outcat, col.regions =  c("white","green","darkred"), att="ID")

#Overlap of the other
Oasmi[AsMiEORmatchss==1] <- 2
out2 <- Oasmi+AsMiEORmatchss
outcat2 <- ratify(out2)
levelplot(outcat2, col.regions =  c("white","green","darkred"), att="ID")
#to get area, easier to be in meters, UTM so convert to proj4string(colocounties.UTM)
class(outcat)
# outcat <- projectRaster(outcat, crs = (proj4string(colocounties.UTM)))
# outcat2 <- projectRaster(outcat2, crs = (proj4string(colocounties.UTM))) 

areaoutcat <- outcat
areaoutcat[areaoutcat!=3] <- NA
sum(raster::area(areaoutcat, na.rm=TRUE))

areaoutcat2 <- outcat2
areaoutcat2[areaoutcat2!=3] <- NA
#tryagain<- projectRaster(outcat, crs = "+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0", method="ngb")
raster::area(areaoutcat2, na.rm=TRUE)

areaoutcat2raster <- rasterToPolygons(areaoutcat2)
out2proj <- spTransform(areaoutcat2raster, CRS(proj4string(colocounties.UTM)))
sum(gArea(out2proj, byid=TRUE))/(1000^2) #divided by 1km squared: 1043.532 square km

#Sum rasters that are overlap ==3 and multiply by cell size, but this is in decimal seconds at this latitude so about 1km is 1 and this is 0.00083 or whatevers
sum(outcat[] == 3)*res(outcat)[1]^2
sum(outcat2[] == 3)*res(outcat2)[1]^2

```
     1. Could use the comparison of more points (appropriate density?) within EORs vs. the sparse data because of biases and practices of herbarium collections to compare overlap of niche models for herb, eor, and sample size same eor. how many background?    
     2. Can't I get the background points from VisTrails? Yes! Background points were always set to 300 for my first crack at it. Too many? Maybe, maybe fine. Or just understand the bias it creats for AUC, sensitivity, specificity and such. 



layout plot     
Plot the Herb and the EOR and then the overlap
```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/EOR_Herb_EORthin_AsMi.jpg",
     width=700, height=170,units='mm', res=300)

nf <- layout(matrix(c(1,2,3), 1,3, byrow=FALSE),
             width=c(6,6,6), height=c(1))
layout.show(nf)

op <- par(no.readonly=TRUE)
par(mai=c(1.5,2,3.5,3)/4)

#EOR
plot(AsMi_EOR)
plot(colocounties, add=TRUE)
mtext("a)", side=3, cex=2, line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))
points(distXsp[[4]][[4]],pch=20, cex=0.5)

#Herbarium collections
plot(AsMi)
plot(colocounties, add=TRUE)
mtext("b)", side=3,cex=2,  line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))

#Thinned EOR points to match Herbarium points
plot(AsMiEORmatchss)
plot(colocounties, add=TRUE)
mtext("c)", side=3,cex=2,  line=0.5, at=par("usr")[1]+0.05*diff(par("usr")[1:2]))

dev.off()

```


http://www.earthskysea.org/!ecology/sdmShortCourseKState2012/grinnellExercise/exercise-tuning-maxent-using-beta-and-aic.html  
https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html    
Use AICc with Maxent to find best model  
```{r}



```


create a raster in R as distance from road
```{r}
localroads <- readOGR(dsn = "Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_GENERAL_non-species_files/All_General_Background_Layers/Colorado/Roads/Local Roads/Local Roads SHP", layer="LROADS")


bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)

#Create a raster to hold distance from road in the same size and extent of the bioclim layers
r <- raster()
extent(r) <- extent(bioclim.colorado[[1]])
projection(r) <- projection(bioclim.colorado[[1]])
res(r) <- res(bioclim.colorado[[1]])

localroads.latlon <- spTransform(localroads, CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))


dd <- gDistance(localroads.latlon, as(r, "SpatialPoints"), byid=TRUE)

```


Geographic distance from herbarium points to EORs  
```{r}
hist(distXsp1$dist[distXsp1$dist<700000]/1000, main = "Distance (km) from points to nearest EOR",
     xlab="km", breaks=50)#, xlim=c(1,612))

plot(density(distXsp1$dist[distXsp1$dist<700000]/1000))

vals <- density(distXsp1$dist)
pStar <- vals$x[which(vals$y==max(vals$y))]

plot(density(distXsp1$dist), xlim=c(0,50000))
abline(v=pStar, col="red")
abline(h=max(vals$y)-qlnorm(0.95))

length(distXsp1$dist[distXsp1$dist>100])/ #816
length(distXsp1$dist)

length(distXsp1$dist[distXsp1$dist<=100])/ #275
length(distXsp1$dist)

length(distXsp1$dist[distXsp1$dist>1000])/ #556
length(distXsp1$dist)

median(distXsp1$dist) #1083.777

reps <- 50000
ssize <- 500
rand.diff <- unlist(lapply(1:reps, function(x) median(sample(distXsp1$dist,ssize))))
ctrl95CI <- sd(rand.diff)/sqrt(ssize)*qt(0.975,df=ssize) 
ctrl95CI.l <- sd(rand.diff)/sqrt(ssize)*qt(0.025,df=ssize)


hist(rand.diff, main="Median distance from points to nearest EOR",
     xlab="meters")
abline(v=mean(rand.diff)+ctrl95CI, col="red")
abline(v=mean(rand.diff)+ctrl95CI.l, col="red")

```

```{r}
for(i in want){
  plot(distXsp[[i]][[1]], pch=16, cex=0.5, col=rgb(0,0,0,0.75), main=g1g2names$AcceptedName[i])
  plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),],
       border = rgb(1,0,0,0.5), lwd=3, add=TRUE)
  ScaleBar(raster(l1G1G2[l1G1G2$GNAME %in%
                           c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),]), #reference raster
           0.1, .5, .15, .15, .05, lwd=2) #round to nearest km, width, % y from bottom, % x from left, y text
}
```


```{r}
for(i in want){
  plot(l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),],
       border = rgb(1,0,0,0.5),  main=g1g2names$AcceptedName[i], lwd=3,
       xlim=c(min(distXsp[[i]][[1]]@coords[,1])-100,max(distXsp[[i]][[1]]@coords[,1]))+100,
       ylim=c(min(distXsp[[i]][[1]]@coords[,2]-100),max(distXsp[[i]][[1]]@coords[,2]))+100)
  points(distXsp[[i]][[1]], pch=16, cex=0.5, col=rgb(0,0,0,0.75))
  ScaleBar(raster(l1G1G2[l1G1G2$GNAME %in%
                           c(g1g2names$AcceptedName[i],g1g2names$Taxon[i]),]), #reference raster
           0.1, .25, .15, .15, .10, lwd=2) #round to nearest km, width, % y from bottom, % x from left, y text
}
```

```{r}
i <- sapply(colo.all, is.factor)
colo.all[i]<-lapply(colo.all[i], as.character)

colo.all$year <- as.numeric(colo.all$year)
colo.all$decimalLatitude <- as.numeric(colo.all$decimalLatitude)
colo.all$decimalLongitude <- as.numeric(colo.all$decimalLongitude)
colo.all$minimumElevationInMeters <- as.numeric(colo.all$minimumElevationInMeters)

colo.all[colo.all$recordNumber == 9088,] #year is 1999, not recorded at Fort Lewis
colo.all[colo.all$recordNumber == 1596,] #another repeat, should be 1997 not 1007
## Ok to get rid of duplicates that resulted in wrong years
colo.all <- colo.all[colo.all$year > 1800,]

colo.all$coordinateUncertaintyInMeters <-
  as.numeric(colo.all$coordinateUncertaintyInMeters)
#How many specimens give an uncertainty in meters (square meters?)
length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$coordinateUncertaintyInMeters) & !is.na(colo.all$decimalLatitude)])/
  length(colo.all$coordinateUncertaintyInMeters[!is.na(colo.all$decimalLatitude)])

colo.all[is.na(colo.all$coordinateUncertaintyInMeters),]

colo.all[colo.all$dist>50000 & !is.na(colo.all$dist),c(56,57,83,61,63,66,67)] #611551meters is width of colorado about

#Can't have missing data
colo.all.ll <- colo.all[!is.na(colo.all$decimalLatitude)&
                       !is.na(colo.all$decimalLongitude),]
coordinates(colo.all.ll) <-~decimalLongitude+decimalLatitude
proj4string(colo.all.ll) <- CRS("+proj=longlat +datum=WGS84") 
colo.all.ll <- spTransform(colo.all.ll, CRS("+proj=utm +zone=13 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

all.county <- over(colo.all.ll, colocounties.UTM[,"COUNTY"])
colo.all.ll$InCounty <- all.county[,1]


table(colo.all.ll$county[colo.all.ll$county != colo.all.ll$InCounty])
colo.all.ll@data[colo.all.ll$county != colo.all.ll$InCounty &
              !is.na(colo.all.ll$county),c("scientificName","institutionCode","county","InCounty","georeferencedBy")]

#Which county are the points really in
colocounties.UTM.df <- as.data.frame(colocounties.UTM)
colocounties.UTM.df$id <- as.numeric(rownames(colocounties.UTM.df))

plot(colocounties.UTM, border="grey80")
plot(colo.all.ll, add=TRUE, pch=16, cex=0.5)

#Make a Levenshtein distance matric between names (like that paper!)
countymatch<- adist(colo.all.ll@data$county[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    colo.all.ll@data$InCounty[!is.na(colo.all.ll@data$county)&
                                              colo.all.ll@data$county!=""],
                    partial=TRUE,ignore.case = TRUE)
#Single linkage method
#hcfar <- hclust(as.dist(countymatch), method='single')


#errors so try clean up county column
colo.all.ll$countyclean <- gsub("Co.*","",colo.all.ll$county)
#trim trailing white space
colo.all.ll$countyclean <- sub("\\s+$", "", colo.all.ll$countyclean)
colo.all.ll$countyclean <- toupper(colo.all.ll$countyclean)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]

nrow(colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),])/
       nrow(colo.all.ll)

colo.all.ll@data[!(colo.all.ll$countyclean %in%colo.all.ll$InCounty),]


```

Total area of EOR polygons per spcies for the the 46 species with mapped herbarium records 
```{r}
#n <- length(x); sort(x,partial=n-1)[n-1]   for the second to largest value
plotDistxRange <- ggplot(distXsp1[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area),], aes(sqrt(area), dist/1000))+
  geom_point()+
  theme_bw()+
  ggtitle("a)")+
  stat_smooth(method="lm", colour="black", level=0)+
  ylab("Distance (km) to nearest EOR")+
  xlab(expression(paste("Species range (km"^"2",")")))

ggplot(distXsp1[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area),], aes(area, log(dist/1000)))+
  geom_point()+
  theme_bw()+
  stat_smooth(method="lm", colour="black", level=0)+
  ylab("Distance (km) to nearest EOR")+
  xlab(expression(paste("Species range (km"^"2",")")))

#to do year next need to get as many years out as possible from character strings
distXsp1$year <- (as.numeric(gsub("([0-9]+).*$", "\\1", distXsp1$year)))

```



```{r}
#get rid of the three that are out of the state issues
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/distanceXrange.jpg",
     width=100, height=75,units='mm', res=300)

plotDistxRange

dev.off()

#get rid of the three that are out of the state issues
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/distanceXyear.jpg",
     width=200, height=75,units='mm', res=300)

layout(matrix(c(1,2), 1,2, byrow=FALSE),
             width=c(6,6), height=c(1))

par(mar=c(2,4.5,4.5,0.5))

plotDistxRange

ggplot(distXsp1[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area),], aes(year, dist/1000))+
  geom_point(pch=16)+
  theme_bw()+
  stat_smooth(method="lm",colour="black", level=0)+
  ylab("")+
  ggtitle("b)")+
  # ylab("Distance (km) to nearest EOR")+
  xlab("Year")# +
  # theme(axis.text.x = element_text(angle = 90, hjust = 1))


dev.off()
```

```{r}

#get rid of the three that are out of the state issues
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure2.jpg",
     width=275, height=125,units='mm', res=300)

layout(matrix(c(1,2), 1,2, byrow=FALSE),
             width=c(6,6), height=c(1))

par(mar=c(4,4,1,0.5))

plot(sqrt(distXsp1$area[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)]),
     (distXsp1$dist[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)])/1000,
     pch=16, cex=0.5, 
     xlab=expression(paste("Species range (km"^"2",")")),
     ylab="Distance (km) to nearest EOR")
abline(lm((distXsp1$dist[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)])/1000~
            sqrt(distXsp1$area[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)])))
mtext("a)", side=3, line=0, at=par("usr")[1]+0.01*diff(par("usr")[1:2]),
             cex=1.2)

plot(distXsp1$year[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)],
     (distXsp1$dist[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)])/1000,
     pch=16, cex=0.5, 
     ylab="",
     xlab="Year")
abline(lm((distXsp1$dist[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area)])/1000~
            distXsp1$year[distXsp1$dist<sort(distXsp1$dist,
                                             partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                            distXsp1$area<=max(distXsp1$area)]))
mtext("b)", side=3, line=0, at=par("usr")[1]+0.01*diff(par("usr")[1:2]),
             cex=1.2)

dev.off()
```


```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/distanceXSpeciesRange.jpg",
     width=150, height=100,units='mm', res=300)



ggplot(distXsp1[distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area),], aes(area/1000000, dist/1000))+
  geom_point(pch=16)+
  theme_bw()+
  stat_smooth(method="lm")+
  ylab("Distance (km) to nearest EOR")+
  xlab(expression(paste("Species range (km"^"2",")")))


dev.off()
```

```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/distanceXYear_SpeciesRange.jpg",
     width=275, height=200,units='mm', res=300)


ggplot(distXsp1[which(distXsp1$dist<sort(distXsp1$dist,partial=(nrow(distXsp1))-2)[(nrow(distXsp1))-2]&
                distXsp1$area<=max(distXsp1$area) & !is.na(distXsp1$New.Genus)),], 
       aes(year, dist/1000, color=as.numeric(as.factor(New.Species))))+
  geom_point()+
  theme_bw()+
  stat_smooth(method="lm")+
  facet_wrap(~New.Genus)+
  ylab("Distance (km) to nearest EOR")+
  xlab("Year")+ 
  scale_color_gradientn(colours = rainbow(length( table(distXsp1$Taxon))))+
  theme(legend.position="none")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

dev.off()
```


Is there anything inherintly interesting about the genus? Maybe some genera are more likely to shift in range than others... so yes, keep in.  

```{r}
# Don't need grid points for EOR but do want all records and see difference in years between herb and closest EOR

coloradosps.g1g2 <- read.csv("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/VisTrails/FieldData_EORG1G2_WGS84.csv")

load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/g1g2names.Rda")
l1eor <- readShapePoly("Q:/Research/All_Projects_by_Species/aa_Shapefiles_Maps/aa_ArcMap_Projects/Link_EO_Herbarium_Records/L1shp")

#Each polygon might be made of multiple disconnected polygons. Need to separate and label
l1G1G2 <- disaggregate(l1eor[l1eor$GRANK %in% c("G1","G2"),])
l1G1G2$PolyID <- do.call(rbind, lapply(split(l1G1G2,l1G1G2$OBJECTID),
                                       function(x){
              x$POlyID <- paste(x$EO_ID, LETTERS[seq(from=1, to= nrow(x))],sep="")
              x
              }))

distXsp_noEORgrid <- lapply(1:60, function(i){
  gc()
  g1g2now <- coloradosps.g1g2[coloradosps.g1g2$scientificName %in%
                                c(g1g2names$Taxon[i],g1g2names$AcceptedName[i])&
                                !is.na(coloradosps.g1g2$decimalLatitude),]
  
  g1g2now <- g1g2now[as.numeric(as.character(g1g2now$year))>1800,]
  # remove points not in Colorado
  g1g2now <- g1g2now[g1g2now$decimalLatitude>35,]
  g1g2now <- g1g2now[g1g2now$decimalLongitude>(-113),]
  g1g2now <- g1g2now[grepl("Specimen",g1g2now$basisOfRecord),]
  g1g2now <- g1g2now[,c(1:2,14:20,30:38,41,43:44,53:62,68:70,83:86)]
  gc()
  
  if(nrow(g1g2now)>0){
  # put grid points across colorado, sample from it for each polygons
    polys <- l1G1G2[l1G1G2$GNAME %in% c(g1g2names$AcceptedName[i],
                                        g1g2names$Taxon[i]),] 
    proj4string(polys) <- CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84") # CRS("+init=epsg:26913")
    
        
    years <- as.numeric(substring(polys$LASTOBS,1,4))
    yearsnotNA <- c()
    for(l in 1:length(years)){
      yearsnotNA[l] <- if(years[l]<9999){ 
                                      l } else {
                                        NA
                                      }
    }
    yearsnotNA <- yearsnotNA[!is.na(yearsnotNA)] 
    polys <- polys[yearsnotNA,]
    
    totalarea <- sum(area(disaggregate(polys)))
  # area of each polygon
    areanow <- sapply(slot(polys,"polygons"), slot, "area")
    
  #turn into spatialpointsdataframe for individual species
    coordinates(g1g2now) <- ~decimalLongitude+decimalLatitude
    proj4string(g1g2now) <- CRS("+init=epsg:4326")  # CRS("+proj=longlat +datum=WGS84")
    g1g2now <- spTransform(g1g2now, CRS("+proj=utm +zone=13 ellps=NAD83 +ellps=WGS84"))
  # The minimum distance from each point to the nearest polygon
    distnow <- apply(gDistance(g1g2now,polys, byid=TRUE),2,min)
    gc()
   
  out <- data.frame(Species = g1g2names$AcceptedName[i], g1g2now,Dist = distnow, Area = totalarea, 
                    EORdate = polys$LASTOBS[apply(gDistance(g1g2now,polys, byid=TRUE),2,which.min)],
                    Yeardiff = g1g2now$year- as.numeric(substring(as.character(polys$LASTOBS[apply(gDistance(g1g2now,polys, byid=TRUE),2,which.min)]),1,4)))
    gc()
    out
    }
  })
save(distXsp_noEORgrid, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_noEORgrid.Rda") 
```

Yeardiff is the herbarium colleciton year - the polygon mapped year (LastOBS) so positive values are herbarium later than mapped, negative numbers are the herbarium colleciton before the mapped time
```{r}

distXspall <- do.call(rbind, distXsp_noEORgrid)
distXspall$New.Genus <- gsub("([A-Za-z]+).*", "\\1", distXspall$Species)
table(distXspall$Yeardiff) # 9999 for unknown year of EOR mapping --> removed now
distXspall[distXspall$Yeardiff< -100,c("EORdate","year")]
names(distXspall)
```
But isn't the whole point that I want to vary over lots of genera?   
Models included (1) species range, (2) collection year, (3) Genus, (4) years between collection and mapping, (5) collection year + species range, (6) collection year * species range, (7) Genus * Year, (8) Genus * collection year * species range, (9) Genus * years between collection and mapping * species range, and (10) intercept only (null model).
```{r}
f1 <- lm(Dist ~ as.factor(New.Genus)*year + Area, data= distXspall)
f2 <- lm(Dist ~ year + Area, data= distXspall)
f3 <- lm(Dist ~ year, data= distXspall)
f4 <- lm(Dist ~ Area, data= distXspall) #x is the area of the species known range
f5 <- lm(Dist ~ 1, data= distXspall)
f6 <- lm(Dist ~ year*Area, data= distXspall) #year given the range
f7 <- lm(Dist ~ as.factor(New.Genus)*(year), data= distXspall)
f8 <- lm(Dist ~ Yeardiff, data= distXspall) # range shift
f9 <- lm(Dist ~ as.factor(New.Genus)*year*Area, data= distXspall)
f10 <- lm(Dist ~ as.factor(New.Genus)*Yeardiff*Area, data= distXspall)


(lmresults <- aictab(list(f1,f2,f3,f4,f5,f6,f7,f8,f9,f10),
       modnames=as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6,f7,f8,f9,f10),formula)))))

evidence(aictab(cand.set = list(f1,f2,f3,f4,f5,f6,f7,f8,f9,f10),
                modnames = as.character(unlist(lapply(list(f1,f2,f3,f4,f5,f6,f7,f8,f9,f10),formula)))))

sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})

(er1 <- exp(0.5*lmresults$Delta_AICc[2]))
(er2 <- exp(0.5*lmresults$Delta_AICc[3]))

```

When ignoring genus since those are so unevenly applied (lots of mustards)
```{r}

# Why is aictab adding an extra parameter
# maybe AICc

str(distXspall)
# f1 <- lm(Dist ~ as.factor(New.Genus)*year + Area, data= distXspall)
f2 <- lm(Dist ~ year + Area, data= distXspall)
f3 <- lm(Dist ~ year, data= distXspall)
f4 <- lm(Dist ~ Area, data= distXspall) #x is the area of the species known range
f5 <- lm(Dist ~ 1, data= distXspall)
f6 <- lm(Dist ~ year*Area, data= distXspall) #year given the range
# f7 <- lm(Dist ~ as.factor(New.Genus)*(year), data= distXspall)
f8 <- lm(Dist ~ Yeardiff, data= distXspall) # range shift
# f9 <- lm(Dist ~ as.factor(New.Genus)*year*Area, data= distXspall)
# f10 <- lm(Dist ~ as.factor(New.Genus)*Yeardiff*Area, data= distXspall)

f11 <- lm(Dist ~ Yeardiff*Area, data= distXspall) # range shift by specificity

f12 <- lm(Dist ~ Yeardiff+Area, data= distXspall) # range shift by specificity

aictab(list(f4))

AICc(f4, return.K = TRUE)
AIC
str(foo)

(lmresults <- aictab(list(f2,f3,f4,f5,f6,f8,f11,f12),
       modnames=as.character(unlist(lapply(list(f2,f3,f4,f5,f6,f8,f11,f12),formula)))))

evidence(aictab(cand.set = list(f2,f3,f4,f5,f6,f8,f11,f12),
                modnames = as.character(unlist(lapply(list(f2,f3,f4,f5,f6,f8,f11,f12),formula)))))

sapply(1:length(lmresults$Delta_AICc), function(i){
  exp(-0.5*lmresults$Delta_AICc[i])/sum(exp(-0.5*lmresults$Delta_AICc))
})

(er1 <- exp(0.5*lmresults$Delta_AICc[2]))
(er2 <- exp(0.5*lmresults$Delta_AICc[3]))
(er3 <- exp(0.5*lmresults$Delta_AICc[4]))
(er4 <- exp(0.5*lmresults$Delta_AICc[5]))
(er5 <- exp(0.5*lmresults$Delta_AICc[6]))

```

```{r}
distXspall$areabin <- 100
distXspall$areabin[distXspall$Area < max(distXspall$Area)] <- 95
distXspall$areabin[distXspall$Area < quantile(distXspall$Area, .75)] <- 75
distXspall$areabin[distXspall$Area < quantile(distXspall$Area, .5)] <- 50
distXspall$areabin[distXspall$Area < quantile(distXspall$Area, .25)] <- 25
distXspall$areabin[distXspall$Area == min(distXspall$Area)] <- 1
table(distXspall$areabin)

ggplot(distXspall, aes(Yeardiff, Dist/1000))+
  geom_jitter(position=position_jitter(1), aes(colour=Area/(1000^2)), size=2, alpha=0.5, pch=16)+  # geom_point(aes(colour=year))+
  stat_smooth(method="lm")+
  theme_bw()+
  # facet_wrap(~areabin, nrow=2)+
  xlab("Year (Herb - CNHP)")+
  ylab("Distance (km) to nearest EOR")+
  scale_colour_gradientn(colours = rev(c("#FF0000", "#FFA500", "#FFFF00", "#008000", "#9999ff", "#000066"))) # (low="black",high="lightgreen")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
ggplot(distXspall, aes(Area/1000000, Dist/1000))+
  geom_jitter(position=position_jitter(1.5), aes(colour=Yeardiff), size=2, alpha=0.5, pch=21)+  # geom_point(aes(colour=year))+
  stat_smooth(method="lm")+
  theme_bw()+
  xlab("Species Range")+
  ylab("Distance (km) to nearest EOR")+
  facet_wrap(~New.Genus)+
  scale_colour_gradientn(colours = rev(c("#FF0000", "#FFA500", "#FFFF00", "#008000", "#9999ff", "#000066"))) # (low="black",high="lightgreen")

```
#Convext hull or tiny circles around eors vs herbarium vs. both together, how much change?    

The Weber example of an out of state person mapping a collection to Gray's Peak from Chicago Lake that should be Mount Evans.     
- <https://conps.org/wp-content/uploads/2015/05/Mount_Evans_Summit_Lake_4-24-2011.pdf>    

"coloradosps" == SEINet collections from Colorado - downloaded 12/6/2017 from SEINet      
"coloG1G2" == species matching names in L1 EORs but without any synonym corrections     
"seinet" == Mo worked on this, made distances from nearest EOR and looked at posted informaiton to figure out what might have been used to pick point on map        
      
ITISlist is synonmy pulled from ITIS into SQL    
TPL is pulling from synonym from the plant list   "colo"   
      "seinetMo"is synonymy with TPL for Mo's list of 53 species (that are G1/G2s)   
      "colonames" is synonym with TPL for all colorado specimens from SEINet   

<https://rpubs.com/spoonerf/SDM4>   
        
#ENMeval vignette
<https://cran.r-project.org/web/packages/ENMeval/vignettes/ENMeval-vignette.html>   
```{r}
bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39) #at 0.5 minutes of a degree
##just get the datafiles lat and long from pca_eorandherb and extract the bioclim variables in addtion to the monthly precip, min, max temps

plot(bioclim.colorado$bio1_12)
plot(colocounties, add=TRUE)

# Use rasters in a RasterStack:
class(bioclim.colorado)

#Need back to latlon for bioclim layers
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[i]]][[1]], CRS(proj4string(bioclim.colorado)))
}

```

```{r}
jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/Figure2_v2.jpg",
     width=225, height=105,units='mm', res=300)

layout(matrix(c(1,2,3), 1,3))
plot(distXspall$Area,
     distXspall$Dist,
     xlab=expression(paste("Species Range km" ^2)),
     ylab=expression(paste("km")),
     pch=16, cex= .5, yaxt="n", xaxt="n")
     axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
     axis(1, at=c(0,10,20,30,40,50,60,70,80,90)*(1000^2), labels=c(0,10,20,30,40,50,60,70,80,90))
abline(lm(Dist~Area, data=distXspall))
mtext("a)", side=3, line=0, adj=0)
plot(jitter(distXspall$year ),
     distXspall$Dist,
     xlab="Year",
     ylab="",
     pch=16, cex= .25, yaxt="n")
     axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
abline(lm(Dist~year, data=distXspall))
mtext("b)", side=3, line=0, adj=0)
plot(jitter(distXspall$Yeardiff ),
     distXspall$Dist,
     xlab="Difference in collection year and mapped year",
     ylab="",
     pch=16, cex= .25, yaxt="n")
     axis(2, at=c(0,100,200,300,400)*1000, labels=c(0,100,200,300,400))
abline(lm(Dist~Yeardiff, data=distXspall))
mtext("c)", side=3, line=0, adj=0)
dev.off()
```

#Spatial thinning
<https://cran.r-project.org/web/packages/spThin/vignettes/spThin_vignette.html>     
Thin to create dataset in which all occurences are at least _thin.par_ distance apart. helps reduce effect of uneven, biased, species occurence collections on spatial model outcomes   
Got through 19 of them, go from 20 to length(wantEOR)   
SKIP, already loaded
```{r}
# The EOR grid points from distXsp is the forth item. Wants lat long, they are. 
#for(i in 1:length(wantEOR)){
#for(i in 20:length(wantEOR)){
#for(i in 21:length(wantEOR)){

EORgrid.thin <- list()
#for(i in c(1:18,21:47,49:length(wantEOR))){
for(i in c(20,48)){
  EORsp <- data.frame(distXsp[[wantEOR[i]]][[4]]@coords,
                      species=g1g2names$AcceptedName[wantEOR[i]])
    EORgrid.thin[[i]] <- thin(loc.data = EORsp,
                         lat.col = "x2", long.col = "x1",
                         spec.col="species",
                         thin.par = 1, reps=10,
                         locs.thinned.list.return = TRUE,
                         write.files = TRUE, max.files=5,
                         out.dir = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Thinned/",paste(out.base="EORsp_1km",i,"_",g1g2names$AcceptedName[wantEOR[i]],sep=""), write.log.file = TRUE,
                         log.file = "EORgridthinned_log.txt")
}

save(EORgrid.thin, file= "P:/hackathon/Simulations/EORgrid.thin.Rda")
```


```{r}
Herb.thin1000 <- list()
for(i in 1:length(want)){
  Herbsp <- data.frame(pointsXsp.latlon[[i]]@coords,
                       species=g1g2names$AcceptedName[want[i]])
  Herb.thin1000[[i]] <- thin(loc.data = Herbsp,
                     lat.col = "decimalLatitude", long.col = "decimalLongitude",
                     spec.col="species",
                     thin.par = 1, reps=1000,
                     locs.thinned.list.return = TRUE,
                     write.files = TRUE, max.files=5,
                     out.dir = "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Thinned/",paste(out.base="Herb.thin_1km_1000",i,"_",g1g2names$AcceptedName[wantEOR[i]],sep=""), write.log.file = TRUE,
                     log.file = "EORgridthinned1000_log.txt")
}

save(Herb.thin1000, file= "P:/hackathon/Simulations/Herb.thin1000.Rda")
```


### Background points  
Can either do bounding box (plus some buffer) or circles at some diameter   
    
    - there are 60 species L1/L2 species with data   
    - 59 species have EOR data    
    - 46 species have herbarium data avalaible through SEINet    
    - wantEOR are the ones from the 60 that have EOR data so wantEOR[i] in a loop would get the ones I want    
    - want[i] would get teh 46 that have herbarium data from the list of 60 species   

##SKIP!! you already loaded these
```{r}
#Circles around herbarium points
circleHerb <- list()
for(i in 1:length(want)){
  circleHerb[[i]] <- circles(pointsXsp.latlon[[i]]@coords, d=50000, lonlat=TRUE)
}

save(circleHerb, file= "P:/hackathon/Simulations/circleHerb.Rda")

#Circles around EOR points. 
circle <- list()
for(i in 1:length(wantEOR)){
  circle[[i]] <- circles(EORsxsp[[wantEOR[i]]]@coords, d=50000, lonlat=TRUE)
}

save(circle, file= "P:/hackathon/Simulations/circleEOR.Rda")
```


```{r}
plot(colocounties)
plot(circle[[2]], add=TRUE)
points(EORsxsp[[2]]@coords, pch=16)
plot(circle[[3]], add=TRUE, border="red")
points(EORsxsp[[3]]@coords, pch=16)
plot(circle[[4]], add=TRUE, border="blue")
points(EORsxsp[[4]]@coords, pch=16)
plot(circle[[5]], add=TRUE, border="orange")
points(EORsxsp[[5]]@coords, pch=16)
```

#SKIP!!! already loaded
```{r}
# Need stack of environmental layers  
#Raster stack of just the bioclim variables
#class(bioclim.colorado)

# Get backgrounds for each species
envs.backg <- list()
for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- crop(bioclim.colorado, circle[[i]]@polygons)
}

for(i in 1:length(wantEOR)){
  envs.backg[[i]] <- mask(envs.backg[[i]], circle[[i]]@polygons)
}


#Get backgrounds for each species according to the herbarium specimens
envs.backg.herb <- list()
for(i in 1:length(want)){
  envs.backg.herb[[i]] <- crop(bioclim.colorado, circleHerb[[i]]@polygons)
}

for(i in 1:length(want)){
  envs.backg.herb[[i]] <- mask(envs.backg.herb[[i]], circleHerb[[i]]@polygons)
}


save(envs.backg, file= "P:/hackathon/Simulations/envs.backgEOR.Rda")
save(envs.backg.herb, file= "P:/hackathon/Simulations/envs.backgHerb.Rda")
```


### NEED TO LOAD STUFF UP TOP TO DO THIS!

```{r}
#Plot presence points
plot(envs.backg[[wantEOR[19]]][[1]], main=g1g2names$AcceptedName[wantEOR[19]])
points(EORsxsp[[wantEOR[19]]], pch=16)

for(i in 1:length(envs.backg.herb)){
  plot(envs.backg.herb[[i]][[1]], main=g1g2names$AcceptedName[want[match(i,want)]] )
  plot(colocounties, add=TRUE)
  points(pointsXsp.latlon[[i]])
}

```


Background points for EORs
```{r}
# Background points from set area, select number of background points equal to the number of presence points
#Amend to 25% more background than presence. 

pca_eorandherb[[1]]

#bg <- list()
bg <- lapply(1:length(wantEOR), function(i){
  background <- as.data.frame(randomPoints(envs.backg[[i]][[1]], n=nrow(EORsxsp[[wantEOR[i]]]@coords)+
                                          0.25*(nrow(EORsxsp[[wantEOR[i]]]@coords))))
  out <- data.frame(x1 = background$x, x2 = background$y, id = NA, polyid=NA, Bestcrs=NA, firstobs=NA,
                           GRANK=NA, GNAME=g1g2names$AcceptedName[i], ENDEMIC=NA, Lastobs=NA,
                    dist=NA,area=NA,
                    raster::extract(rasterstack, background))
  out
})

save(bg, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/bg.Rda")


```


Background points for herbs
```{r}
# Background points from set area, select number of background points equal to the number of presence points
#Amend to 25% more background than presence. 

length(pointsXsp.latlon) #46

#bg <- list()
bg.herb <- lapply(1:length(want), function(i){
  background <- as.data.frame(randomPoints(envs.backg.herb[[i]][[1]], n=nrow(pointsXsp.latlon[[i]]@coords)+
                                          0.25*(nrow(pointsXsp.latlon[[i]]@coords))))
  out <- data.frame(x1 = background$x, x2 = background$y, id = NA, polyid=NA, Bestcrs=NA, firstobs=NA,
                           GRANK=NA, GNAME=g1g2names$AcceptedName[i],ENDEMIC=NA, Lastobs=NA,
                    dist=NA,area=NA,
                    raster::extract(rasterstack, background))
  out
})

save(bg.herb, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/bg.herb.Rda")
```



Background points given roads raster, point in each grid, extract values, run the binomial distribution function probably over it. 

```{r}
#Plot presence and background points
for(i in 1:length(wantEOR)){
  plot(envs.backg[[i]][[1]])
  plot(colocounties, add=TRUE)
  points(EORsxsp[[i]], pch=16, cex=0.5)
  points(bg[[i]], pch=21, cex=0.5, col="red")
  scalebar(100, type="line", divs = 4, below="kilometers", lonlat = TRUE)
}

#Try some thinning first, for some that's lots of points that are 

```

Partitioning occurrences for evalutaiton of models to reduce overinflated performace due to biased sampling/autocorrelation among points   
```{r}
#Block partitioning spatially by dividing the data into four bins based on their position in relation to a north/south and east/west partitioning that gets the points into nearly 4 equal size groups. 

#NEED to do this for the herbarium specemins too, not just the EORS!!!

blocks <- list()
for(i in 1:length(pointsXsp.latlon)){ #46 species with points
  blocks[[i]] <- get.block(pointsXsp.latlon[[i]]@coords, bg[[want[i]]])
  
  plot(envs.backg[[i]][[1]], col="gray", legend=FALSE)
  points(EORsxsp[[i]]@coords, pch=21, bg=blocks[[i]]$occ.grp) #In CRS("+proj=longlat +datum=WGS84"), grid points every 100 meters (map units which was UTM) within EOR polygons.
  points(pointsXsp.latlon[[i]])
  
}
```



```{r}
bkgrnd <- calc(r.smaller, fun= function(X) sampleRandom(r.smaller, size= x))

samplesize <- calc(r.smaller, fun=function(x) spsample)

hist(samplesize)

layerbackgroundpoints <- sampleRandom(r.smaller, size = ) 

```


Create the full dataframes for each species   
https://www.molecularecologist.com/2013/04/species-distribution-models-in-r/     

```{r}
#Create bckground points, give them a 0 and same column names with lat long. Give eors a 1 and herb a 2 for looking at groups but then do a clustering

sort(unique(coloradosps.g1g2$scientificName)) #68 names since a few synonyms 
sort(unique(g1g2names$AcceptedName)) #60 names, why no Astragalus anisus??  
sort(unique(g1g2names$Taxon)) #60 names, why no Astragalus anisus??   

```


#Skip already loaded
```{r}
pca_data <- lapply(1:length(want), function(x){
  eor <- pca_eorpost1980[[want[x]]]
  eor$pa <- 1
  herb <- pca_herbpost1980[[x]]
  herb$pa <- 2
  background.eor <- bg[[x]]
  background.eor$pa <- 3
  background.herb <- bg.herb[[x]]
  background.herb$pa <- 4 
  rbind(eor, herb, background.eor, background.herb)
})

pca_data[[1]]
```


```{r}
names(pca_data[[1]])

#Make seasonal precip and temps? 
# Winter: December-March; Summer: April-July; Fall: August-November
season <- pca_data[[1]]
climatenames <- do.call(c,lapply(names(season)[11:46], function(s){
                  out <- unlist(strsplit(s, split='_', fixed=TRUE))[c(2,6)]
                  paste(out[1],out[2],sep='_')
                  }))
colnames(season)[11:46] <- climatenames

pca_data2 <- lapply(pca_data, function(season){
  season$WinterPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(12,1:3)]])
  season$SummerPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(4:7)]])
  season$FallPrecip <- rowMeans(season[,grep("ppt", colnames(season))[c(8:11)]])
  
  season$WinterMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(12,1:3)]])
  season$SummerMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(4:7)]])
  season$FallMinTemp <- rowMeans(season[,grep("tmin", colnames(season))[c(8:11)]])
  
  season$WinterMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(12,1:3)]])
  season$SummerMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(4:7)]])
  season$FallMaxTemp <- rowMeans(season[,grep("tmax", colnames(season))[c(8:11)]])
  
  season
}) 

names(pca_data2[[1]])

save(pca_data, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data.Rda")

save(pca_data2, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/pca_data2.Rda")

```




Model Fitting, AIC, GLM   
```{r}
library(psych)

#predictor variables and eliminated one variable from each pair that was strongly correlated (Pearson or Spearman correlation, r > 0.70 — Elith et al., 2006). - Hayes et al 2015

cor.plot(cor(pca_data2[[1]][c(48, 54)]), numbers=TRUE) #after getting rid of them one by one... left with winter precip and winter max which both make sense, depending on how much winter precip and how hot so how long it might stay around and melt into the spring probably has largest impact. 
```

Can't seem to automate variable selection to get rid of one variable when more than cutoff
```{r}
cutoff <- 0.7
cornames <- colnames(pca_data2[[1]][48:56])
cor.table<-cor(pca_data2[[1]][,(48:56)])
data <- as.data.frame(as.table(cor.table))
#with how many other variables is one correlated? 
cortable <- table(data$Var1[data$Freq>=cutoff])
mostcor <- names(cortable[which(max(cortable)==cortable)])
cor.plot( cor(pca_data2[[1]][,c(grep(paste(mostcor[1:2], collapse="|"),
                            cornames, value=TRUE, invert=TRUE))]), numbers=TRUE)

```


```{r}
combins <- combn(colnames(cor.table),2,FUN=function(x) paste(x,collapse="_"))
cor.data <- data[data$Var1 != data$Var2, ]
data <- cor.data[paste(cor.data$Var1, cor.data$Var2, sep="_") %in% combins,]
data[data$Freq>=cutoff,]

#Which is better? Removing Var1 or Var2? 
sort(unique(data$Var1))
sort(unique(data$Var2))

#Even at 70, should be able to keep two varaibles! Something is off. 

model1 <- as.formula(paste("pa ~",
                           paste( grep(paste(unique(data$Var1), collapse="|"),
                                       names(pca_data2[[1]][48:56]), value=TRUE, invert=TRUE ))))

model2 <- as.formula(paste("pa ~",
                           paste( grep(paste(unique(data$Var2), collapse="|"),
                                       names(pca_data2[[1]][48:56]), value=TRUE, invert=TRUE ))))

```


```{r}
m1 <- glm(pa ~ WinterPrecip+SummerPrecip+FallPrecip+
            WinterMinTemp+SummerMinTemp+FallMinTemp+
            WinterMaxTemp+SummerMaxTemp+FallMaxTemp,  data=pca_data2[[1]])
summary(m1)

# If I want the variable with lower residual std. error from correlated pairs:

# Might be that winter precip is most important for all these and summer and winter temps
m2 <- glm(pa ~ WinterPrecip+
            WinterMinTemp+SummerMinTemp+
            WinterMaxTemp+SummerMaxTemp,  data=pca_data2[[1]])
summary(m2)

#Nothing important?
m3 <- glm(pa ~ 1,  data=pca_data2[[1]])
summary(m3)

m4 <- glm(pa ~ WinterPrecip+WinterMaxTemp,  data=pca_data2[[1]])

#not correlated for at least first species
m5 <- glm(pa ~ WinterPrecip+SummerPrecip,  data=pca_data2[[1]])

lm.list <- list(m1,m2, m3,m4,m5) #list(m4,m2, m1,m5,m3)
lm.names <- as.character(unlist(lapply(lm.list,formula)))
(lm.results <- aictab(lm.list, modnames=lm.names))

#evidence ratio 
for(i in 2:length(lm.list)){
  print(exp(0.5*lm.results$Delta_AICc[i]))
}

#Largest model most parsimonious. The uncorrelated is less parsimonious (Delta_AICc=63.86) (winterprecip and summerprecip), ~1 is worse
```

https://www.molecularecologist.com/2013/04/species-distribution-models-in-r/    
EOR presence is 1 and background is 3    
Herb presence is 2 and background is 4
```{r}
#dec-mar precip, max temp
winterPrecip <- stackApply(rasterstack[[c(12,1:3)]], 1, mean)
winterMaxtemp <- stackApply(rasterstack[[c(12,1:3)+12]], 1, mean)
winterMintemp <- stackApply(rasterstack[[c(12,1:3)+24]], 1, mean)
fallPrecip <- stackApply(rasterstack[[c(8:11)]], 1, mean)
fallMaxtemp <- stackApply(rasterstack[[c(8:11)+12]], 1, mean)
fallMintemp <- stackApply(rasterstack[[c(8:11)+24]], 1, mean)
summerPrecip <- stackApply(rasterstack[[c(4:7)]], 1, mean)
summerMaxtemp <- stackApply(rasterstack[[c(4:7)+12]], 1, mean)
summerMintemp <- stackApply(rasterstack[[c(4:7)+24]], 1, mean)

cor.plot(cor(pca_data2[[1]][c(48:56)]), numbers=TRUE)

#would lke to use scaled, colorado layers
us <- getData("GADM", country="USA", level=1)
# extract states (need to uppercase everything)
swstates <- c("Colorado", "Wyoming", "Utah", "New Mexico" ,"Arizona")

sw = us[match(toupper(swstates),toupper(us$NAME_1)),]
sw <- spTransform(sw, CRS(proj4string(winterPrecip)))

# now use the mask function
winPrecipclip <- mask(winterPrecip, sw)
winTmaxclip <- mask(winterMaxtemp, sw)

winterMintempclip <- mask(winterMintemp, sw)
fallPrecipclip <- mask(fallPrecip, sw)
fallTmaxclip <- mask(fallMaxtemp, sw)
fallMintempclip <- mask(fallMintemp,sw)
summerPrecipclip <- mask(summerPrecip,sw)
summerMaxtempclip <- mask(summerMaxtemp,sw) #error
summerMintempclip <- mask(summerMintemp, sw) #error



# winter temps and precip important to give moisture to plants in spring, melt, and to hit germination needs, fall temp is not coorelated and should impact most fruit produciton for most species, too hot and it'll all dry up too much. 
stackmini <- stack(winPrecipclip,winTmaxclip,winterMintempclip, 
                   #fallPrecipclip, 
                   fallTmaxclip) #, fallMintempclip)
                   #, summerPrecipclip)

save(stackmini, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/stackmini.Rda")
```

```{r}

jpeg("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Images/EnvironVariables.jpg",
     width=200, height=200, units='mm', res=300)

layout(matrix(c(1,2,3,4), 2,2, byrow=FALSE),
       width = c(5,5), height=c(5,5))

plot(winPrecipclip, xlim=c(-109,-103),ylim=c(35,42),
     xlab="lon", ylab="lat",
     main="Winter Precipitation")

plot(winTmaxclip, xlim=c(-109,-103),ylim=c(35,42),
     xlab="lon", ylab="lat",
     main="Maximum Winter Temperature")

plot(winterMintempclip, xlim=c(-109,-103),ylim=c(35,42),
     xlab="lon", ylab="lat",
     main="Minimum Winter Temperature")
     
plot(fallTmaxclip, xlim=c(-109,-103),ylim=c(35,42),
     xlab="lon", ylab="lat",
     main="Maximum Fall Temperature")

dev.off()

```

Reference stackmini to have same environmental variables in PCA as in the maxent models
```{r}

animatepca_data <- Map(data.frame, pca_data2, SpeciesID=as.list(seq(1,length(pca_data2),1)))

pca_all <- lapply(1:length(pca_data2), function(i){
  x <- animatepca_data[[i]]
  x.pca <- princomp(~WinterPrecip+SummerPrecip+WinterMinTemp+WinterMaxTemp+FallMaxTemp, 
                    data = x[,c("WinterPrecip","SummerPrecip","WinterMinTemp","WinterMaxTemp","FallMaxTemp")], 
                    na.action = na.omit, cor=TRUE)
  # x.load <- loadings(x.pca)
  x.pc1 <- predict(x.pca)
  x.pc <- data.frame(x.pc1,x[!is.na(x$WinterMaxTemp),]) #x$pa where 1=EOR presence, 3=EOR background; 2=Herb presence, 4=herb background
  x.PoV <- x.pca$sdev^2/sum(x.pca$sdev^2) #Percent of variance explained by the components
  out <- data.frame(x.pc,PoV1=x.PoV[1],PoV2=x.PoV[2])
  out
})

anim_all <- do.call(rbind, pca_all)
table(anim_all$PoV1)
```

```{r}
animatePCA <- list()
for(i in 1:46){
  animatePCA[[i]] <- ggplot(anim_all[anim_all$SpeciesID == i,], aes(Comp.1, Comp.2, 
                                 colour=as.factor(pa), 
                                 shape=as.factor(pa),
                                 frame=SpeciesID))+
    ggtitle(anim_all$GNAME[anim_all$SpeciesID == i][1])+
    geom_point()+
                    stat_ellipse()+
                    theme_bw()+
                    xlab(paste("PC1 (",round(anim_all$PoV1[anim_all$SpeciesID == i][1],2)*100,"%)",sep=""))+
                    ylab(paste("PC2 (",round(anim_all$PoV2[anim_all$SpeciesID == i][2],2)*100,"%)",sep=""))+
                    theme(plot.title=element_text(hjust=0))+
                    theme(legend.justification=c(0,1), legend.position=c(0,1),
                          legend.background = element_rect(fill=alpha('white', 0.1)))+
                    #labs(color = "Presence/Background", shape = "Presence/Background")+
    scale_shape_manual(name = "Presence/Background", 
                       labels=c("EOR presence",
                              "Herbarium presence",
                              "EOR background",
                              "Herbarium background"),
                       values=c(16,17,1,2)) +               
    scale_colour_manual(name = "Presence/Background",
                       labels=c("EOR presence",
                              "Herbarium presence",
                              "EOR background",
                              "Herbarium background"),
                       values=c("indianred4","hotpink3","lightslategray","lightsteelblue3")) #("darkolivegreen4","green4","lightgrey","ivory4"))
}

for(i in 1:46){
  print(animatePCA[[i]])
}
```


```{r}
# 
# cl <- makeCluster(6)
# registerDoParallel(cl)
# stopCluster(cl)
# 
# memory.limit()
# memory.limit(size=memory.limit()+1000)
# memory.limit(size=memory.limit()-1000) #take it back down a notch? oh, can't!?


#Loop through all the species
Maxent46species <- lapply(1:length(pca_data2), function(i){
  checkrecords <- sapply(split(pca_data2[[i]], pca_data2[[i]]$pa), function(x) nrow(x))
   
  # to keep at least 5 records per group
  if(sum(checkrecords<10)>0){
    NULL
  } else {

        kfoldnum <- 10
        speciestable <- pca_data2[[i]]
        speciestable$group_pa <- unlist(lapply(split(pca_data2[[i]], pca_data2[[i]]$pa), function(x){
          kfold(x$pa, kfoldnum)
          }))
    
      out <- list()
      for(l in 1:10){     #Are 10 repeats for 10 fold enough?
          #randomly select on of the test roups to sample as the training data, 10 fold
          test <- sample(1:10, 1)
          
          train_pEOR <- speciestable[speciestable$pa == 1 & speciestable$group_pa!=test,c("x1","x2")]
          train_pHerb <- speciestable[speciestable$pa == 2 & speciestable$group_pa!=test,c("x1","x2")]
          train_bEOR <- speciestable[speciestable$pa == 3 & speciestable$group_pa!=test,c("x1","x2")]
          train_bHerb <- speciestable[speciestable$pa == 4 & speciestable$group_pa!=test,c("x1","x2")]
          
          test_pEOR <- speciestable[speciestable$pa == 1 & speciestable$group_pa==test,c("x1","x2")]
          test_pHerb <- speciestable[speciestable$pa == 2 & speciestable$group_pa==test,c("x1","x2")]
          test_bEOR <- speciestable[speciestable$pa == 3 & speciestable$group_pa==test,c("x1","x2")]
          test_bHerb <- speciestable[speciestable$pa == 4 & speciestable$group_pa==test,c("x1","x2")]
          
          me <- maxent(stackmini, p=train_pEOR, a=train_bEOR)
          meHerb <- maxent(stackmini, p=train_pHerb, a=train_bHerb)
          
          #evalidate teh model
          e <- dismo::evaluate(test_pEOR, test_bEOR, me, stackmini)
          eHerb <- dismo::evaluate(test_pHerb, test_bHerb, meHerb, stackmini)
          gc()
          
          #Visulatize predictions
          writeRaster(dismo::predict(me, stackmini),paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/EOR",g1g2names$AcceptedName[i],l,".tif", sep=""),overwrite=TRUE)
          gc()
          writeRaster(dismo::predict(meHerb, stackmini),paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/Herb",g1g2names$AcceptedName[i],l,".tif", sep=""),overwrite=TRUE)
          gc()
          thres <- dismo::threshold(e) 
          thresHerb <- dismo::threshold(eHerb)
          # 1: which group to withhold, 2: maxent for EOR, 3: maxent for Herbarium, 4: evaluate EOR, 5: evaluate Herbarium, 6: thresholds for EOR, 7: thresholds for Herbarium, 8: species
          out[[l]] <- list(test,me,meHerb,e,eHerb,thres,thresHerb,g1g2names$AcceptedName[i])#,pme,pmeHerb)
          gc()
      }
      out
  }
})

save(Maxent46species, file= "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/Maxent46species.Rda")
```

1: which group to withhold, 2: maxent for EOR, 3: maxent for Herbarium, 4: evaluate EOR, 5: evaluate Herbarium, 6: thresholds for EOR, 7: thresholds for Herbarium, 8: species
```{r}
#Specificy=Sensitivity or Kappa??
#Make names match raster files
thres_all <- do.call(rbind,lapply(Maxent46species[lapply(Maxent46species, length)>0], function(x){
  species <- sapply(x, '[[', 8)
  EOR_th <- sapply(x, '[[', 6)
  Herb_th <- sapply(x, '[[', 7)
  EOR_ss <- unlist(EOR_th[5,]) #1:kappa, 2:spec_sens, 3:no_omission, 4:prevalence, 5:equal_sens_spec, 6:sensitivity
  HERB_ss <- unlist(Herb_th[5,])
  eorout <- data.frame(threshold=EOR_ss,Species=paste("EOR",gsub(" ","_",species),1:10,sep=""))
  herbout <- data.frame(threshold=HERB_ss,Species=paste("Herb",gsub(" ","_",species),1:10,sep=""))
  rbind(eorout,herbout)
}))

#20 species
maxentspecies <- Maxent46species[lapply(Maxent46species, length)>0]
spRasterEORvsHerb <- lapply(unlist(sapply(maxentspecies, '[[', 8)[8,]),
                            function(sp){
                              lapply(1:10, function(rep){
                                lapply(c("EOR","Herb"), function(comp){
        raster(paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/",
                     comp,sp,rep,".tif", sep=""))
                                })
                              })
                            })

  spRasterEORvsHerb[[1]][[1]][[1]]@file@name
  #Need name to match up threshold with EOR or Herb and species and which map
  spRasterEORvsHerb[[1]][[1]][[2]]@data@names   #!!! Yes!
  
  thres_all[[1]]$Species


auc_all <- sapply(Maxent46species, function(x){
  species <- sapply(x, '[[', 8)
  EOR_eval <- sapply(x, '[[', 4)
  Herb_eval <- sapply(x, '[[', 5)
  EOR_auc <- sapply(EOR_eval, function(y) y@auc)
  HERB_auc <- sapply(Herb_eval, function(y) y@auc)
  data.frame(AUC_EOR=EOR_auc, AUC_Herb=HERB_auc, Species=species)
})

#make raster bricks of EOR and Herbs for each species; just species that got done 
speciesused <- unique(gsub('[[:digit:]]+', '', thres_all$Species))
unlistspRaster <- unlist(spRasterEORvsHerb)
rasterbricks <- lapply(speciesused, function(sp){
  sp1 <- unlistspRaster[grep(sp,
                               sapply(unlistspRaster, function(x) x@data@names))]
  stack(sp1)
})

#Need to pick a way to make a consensus model and do same for both. 
#Should also compare niche models of both to actual distribution defined by CNHP
sapply(1:length(rasterbricks), function(j){
  #r <- raster() #to hold information
  filename <- paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/MeanRasterstack",rasterbricks[[j]]@layers[[1]]@data@names,".tif", sep="")
  x<-rasterbricks[[j]]
  out <- raster(x)
  bs <- blockSize(x)
  out <- writeStart(out, filename, overwrite=TRUE)

  # Loop over each block and do your calculation
  for (i in 1:bs$n) {
      v <- getValues(x, row=bs$row[i], nrows=bs$nrows[i] )
     # mean
        v <- rowMeans(v, na.rm=TRUE)
        out <- writeValues(out, v, bs$row[i])
     }
  out <- writeStop(out)
  return(out)
  gc()
})


averageRaster <- lapply(rasterbricks, function(x){
  gc()
  writeRaster(calc(x, mean), paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/MeanRaster",x@layers[[1]]@data@names,".tif", sep=""),overwrite=TRUE)
  gc()
})

rasterbricks[[1]]@layers[[1]]@data@names

#Go for threshold of each and then overlap 
thresholdrasters <- lapply(unlist(spRasterEORvsHerb), function(x){
  gc()
  writeRaster((x > thres_all$threshold[thres_all$Species==x@data@names]),
              paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEnt/Threshold",x@data@names,".tif", sep=""),overwrite=TRUE)
  })


#need to add number and EOR or Herb to threshold extract from raster name
plot(spRasterEORvsHerb[[1]][[1]][[1]] > 
       thres_all$threshold[thres_all$Species == spRasterEORvsHerb[[1]][[1]][[1]]@data@names], 
     1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", 
     main=paste("Predicted presence of",spRasterEORvsHerb[[1]][[1]][[1]]@data@names,sep=" "))
plot(sw, add=TRUE)

```



Testing the loop for all 46 species
```{r}
group_pa <- do.call(c,sapply(1:4, function(x) kfold(pca_data2[[1]][pca_data2[[1]]$pa==x,],10))) #nope, need seperate for presence and background

#randomly select on of the test roups to sample as the training data
test <- sample(1, 1:10)

train_pEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 1 & group_pa!=test,c("x1","x2")]
train_pHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 2 & group_pa!=test,c("x1","x2")]
train_bEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 3 & group_pa!=test,c("x1","x2")]
train_bHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 4 & group_pa!=test,c("x1","x2")]

test_pEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 1 & group_pa==test,c("x1","x2")]
test_pHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 2 & group_pa==test,c("x1","x2")]
test_bEOR <- pca_data2[[1]][pca_data2[[1]]$pa == 3 & group_pa==test,c("x1","x2")]
test_bHerb <- pca_data2[[1]][pca_data2[[1]]$pa == 4 & group_pa==test,c("x1","x2")]

me <- maxent(stackmini, p=train_pEOR, a=train_bEOR) #put maxent.jar into the java folder of dismo and changed  regedit.exe, navigate to HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft, right click and change permissions to full operation.


#validate teh model
e <- dismo::evaluate(test_pEOR, test_bEOR, me, stackmini)

#Visulatize predictions
pred_me <- dismo::predict(me, stackmini)
thres <- dismo::threshold(e)

plot(pred_me > thres$spec_sens, 1, cex=0.5, legend=T, mar=par("mar"), xaxt="n", yaxt="n", 
     main=paste("Predicted presence of",g1g2names[[1]][1],sep=" "))
plot(sw, add=TRUE)
```

Steven J. Phillips, Miroslav Dudík, Robert E. Schapire. [Internet] Maxent software for modeling species niches and distributions (Version 3.4.1). Available from url: http://biodiversityinformatics.amnh.org/open_source/maxent/. Accessed on 2018-10-7.

https://stackoverflow.com/questions/32915628/best-way-to-create-response-curves-for-a-glm-species-distribution-model-in-r 
```{r}
corout <- as.data.frame(as.table(cor(pca_data[[1]][,11:length(pca_data[[1]])])))
corout[corout$Freq<0.60,]


m1 <- glm()
```



#Random forest - classification and regression trees   
Either a formula, or (1) data.frame with predictor variables, (2) vector with response. categorical will classification, interval will do regression. 
```{r}

library(randomForest)

bioclim.colorado <- getData("worldclim", var="bio", res=0.5, lon=-106, lat = 39)


plot(bioclim.colorado$bio1_12)
plot(colocounties, add=TRUE)

#When loop, use "i in want" or lapply(want, function(i)) which are non-NA in the list for distXsp[[i]][[1]]@coords

#need back to lat long
pointsXsp.latlon <- list()
for(i in 1:length(want)){
  pointsXsp.latlon[[i]] <- spTransform(distXsp[[want[1]]][[1]], CRS("+proj=longlat +datum=WGS84"))
}

##COME BACK TO THIS AND SEE WHAT NEEDS TO BE Done to do random forest
presvals <- extract(bioclim.colorado, )


library(dismo)

kfoldnum <- 5

#Make a training and testing set, will want to iterative over all sets, keeping one out each time
#lapply(1:kfoldnum, function(x){})

group <- kfold(distXsp[[1]][[1]]@coords, kfoldnum)
pres_train <- distXsp[[1]][[1]]@coords[group != 1,]
pres_test <- distXsp[[1]][[1]]@coords[group == 1,]

ext <- extent(-109, -102, 36, 41)
backg <- randomPoints(bioclim.colorado, n=300, ext=ext, extf=1.25)
colnames(backg) <- dimnames(pres_test)[[2]]
groupbk <- kfold(backg, kfoldnum)
backg_train <- backg[groupbk != 1,]
backg_test <- backg[groupbk == 1,]






```

While Chapter1_SDM is running making the strips of probability maps     
Put together all the strips to make one probability map for each Herbarium species  
```{r}
load("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/Data/distXsp_v3.Rda")
notnull <- which(!unlist(lapply(mapply('[[', distXsp_v3, 1), is.null)))
atleast12 <- c()
for(l in notnull){
  atleast12[match(l,notnull)] <- if(nrow(distXsp_v3[[l]][[1]]@coords)>11){ 
    l } else {
      NA
    }
}
atleast12 <- atleast12[!is.na(atleast12)] 

pathstart <- "Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEntReplicates/"

# Just did Sp1, as of 3:22pm on Feb 20th Sp55 is in the middle of running (for Herbarium specimens); 3:24 after Sp55 done by 4:11pm; sp11kfold4 didn't finish, no _8 .grd so need to fix and then do atleast12[5] for kfold 4; atleast12[6:28]
lapply(atleast12[5], function(i){
  resultpath <- list.files(path = pathstart, pattern = paste("Predict",i,"kfold", sep=""), full.names = TRUE)
  lapply(1:4, function(k){
    x <- lapply(resultpath[grep(paste("kfold",k,sep=""),resultpath)], function(x){
      raster(x)
    })
    x$filename <- paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEntReplicates/ProbTiffSp",i,"Herbkfold",k,".tif", sep="")
    x$overwrite <- TRUE
    m <- do.call(merge, x)
  })
})




```
Average the 4 kfolds for each    
```{r}
library(snow)
ncores <- 2 # define the number of cores you want to use, 2 on laptop, 12 on desktop
# ,4:28
lapply(atleast12[1], function(x){
  resultpath <- list.files(path = pathstart, pattern = paste("ProbTiffSp",x,"Herbkfold", sep=""), full.names = TRUE)
  allrasts <- stack(resultpath)
  beginCluster(ncores)
  ras.mean <- clusterR(allrasts, calc, args=list(mean, na.rm=T))
  ras.SD <- clusterR(allrasts, calc, args=list(sd, na.rm=T))
  writeRaster(ras.mean, file=paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEntReplicates/ProbMEANSp",x,sep=""), format="GTiff")
  writeRaster(ras.SD, file=paste("Q:/Research/Projects/alpine-phenology/Uncertainty in niche modeling/RMaxEntReplicates/ProbSDSp",x,sep=""), format="GTiff")
  endCluster()
})

```


